Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Klein2017,
abstract = {Overwhelming scientific evidence demonstrates that experts' judgments can be highly accurate and reliable. As defined in the scientific literature,1 experts • employ more effective strategies than others, and do so with less effort; • perceive meaning in patterns that others do not notice; • form rich mental models of situations to support sensemaking and anticipatory thinking; • have extensive and highly organized domain knowledge; and • are intrinsically motivated to work on hard problems that stretch their capabilities.},
author = {Klein, Gary and Shneiderman, Ben and Hoffman, Robert R. and Ford, Kenneth M.},
doi = {10.1109/MIS.2017.4531230},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Klein et al. - 2017 - Why Expertise Matters A Response to the Challenges.pdf:pdf},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {nov},
number = {6},
pages = {67--73},
title = {{Why Expertise Matters: A Response to the Challenges}},
url = {http://ieeexplore.ieee.org/document/8268030/},
volume = {32},
year = {2017}
}
@article{Wu2018,
abstract = {In this paper, we discuss the concept of a smart city based on information and communication technology (ICT), analyze the objectives of smart city development in Taiwan, and explain the supporting technologies that make such development possible. Subsequently, we propose a hierarchical structure framework of smart city systems with levels of complexity ranging from low to high and interconnections and interactive relationships in five dimensions: the Internet of Things (IoT), cloud computing, Big Data, Mobile Network, and smart business. We integrate each key resource of the core operation systems of cities to promote the innovative operation of cities and further optimize city development. We then propose a Big Data platform data flow framework that uses information from ubiquitous sensor networks and information equipment to analyze the Big Data application process of smart cities and determine the resulting advantages and challenges. Additionally, we analyze the current state of development of smart cities in Taiwan. Finally, we discuss a new philosophy of smart city development and provide a practical blueprint for the formation, operation, and development of the smart cities with the aim of creating a bright future for the smart cities of Taiwan.},
author = {Wu, Shiann and Chen, Tsung-chun and Wu, Yenchun and Lytras, Miltiadis},
doi = {10.3390/su10010106},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2018 - Smart Cities in Taiwan A Perspective on Big Data Applications.pdf:pdf},
issn = {2071-1050},
journal = {Sustainability},
keywords = {Internet of Things,Taiwan,big data,information and communication technology,mobile network,smart cities},
month = {jan},
number = {2},
pages = {106},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Smart Cities in Taiwan: A Perspective on Big Data Applications}},
url = {http://www.mdpi.com/2071-1050/10/1/106},
volume = {10},
year = {2018}
}
@misc{Soga2016b,
abstract = {Increasingly, people are becoming less likely to have direct contact with nature (natural environments and their associated wildlife) in their everyday lives. Over 20 years ago, Robert M Pyle termed this ongoing alienation the "extinction of experience", but the phenomenon has continued to receive surprisingly limited attention. Here, we present current understanding of the extinction of experience, with particular emphasis on its causes and consequences, and suggest future research directions. Our review illustrates that the loss of interaction with nature not only diminishes a wide range of benefits relating to health and well-being, but also discourages positive emotions, attitudes, and behavior with regard to the environment, implying a cycle of disaffection toward nature. Such serious implications highlight the importance of reconnecting people with nature, as well as focusing research and public policy on addressing and improving awareness of the extinction of experience.},
author = {Soga, Masashi and Gaston, Kevin J.},
booktitle = {Frontiers in Ecology and the Environment},
doi = {10.1002/fee.1225},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Soga, Gaston - 2016 - Extinction of experience The loss of human-nature interactions.pdf:pdf},
issn = {15409309},
month = {mar},
number = {2},
pages = {94--101},
publisher = {Ecological Society of America},
title = {{Extinction of experience: The loss of human-nature interactions}},
volume = {14},
year = {2016}
}
@article{Griffin2009,
abstract = {ul q da is a L A T E X package for use in Qualitative Data Analysis research. It assists in the analysis of textual data such as interview transcripts and field notes. This document corresponds to ul q da v1.1, dated 2009/06/11.},
author = {Griffin, Ivan},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Griffin - 2009 - A T X package supporting Qualitative ul q da A L Data Analysis.pdf:pdf},
journal = {Analysis},
title = {{A T X package supporting Qualitative ul q da : A L Data Analysis}},
url = {http://www.latex-project.org/lppl.txt},
year = {2009}
}
@article{Narin1994,
abstract = {This article presents the theory behind modern evaluative bibliometric techniques at three levels. Policy applications, which characterizes the scientific and technological output of nations or regions; strategic analyses, which deals with articles and patents at the level of a university or company; and tactical analyses, which addresses questions concerning a single subject. The article explains the newer techniques that have been developed at each level, as well as the more important limitations.},
author = {Narin, Francis and Olivastro, Dominic and Stevens, Kimberly A.},
doi = {10.1177/0193841X9401800107},
issn = {15523926},
journal = {Evaluation Review},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {feb},
number = {1},
pages = {65--76},
title = {{Bibliometrics/Theory, Practice and Problems}},
url = {http://journals.sagepub.com/doi/10.1177/0193841X9401800107},
volume = {18},
year = {1994}
}
@misc{EU-COM-152015,
abstract = {Introduction The transition to a more circular economy, where the value of products, materials and resources is maintained in the economy for as long as possible, and the generation of waste minimised, is an essential contribution to the EU's efforts to develop a sustainable, low carbon, resource efficient and competitive economy. Such transition is the opportunity to transform our economy and generate new and sustainable competitive advantages for Europe. The circular economy will boost the EU's competitiveness by protecting businesses against scarcity of resources and volatile prices, helping to create new business opportunities and innovative, more efficient ways of producing and consuming. It will create local jobs at all skills levels and opportunities for social integration and cohesion. At the same time, it will save energy and help avoid the irreversible damages caused by using up resources at a rate that exceeds the Earth's capacity to renew them in terms of climate and biodiversity, air, soil and water pollution. A recent report also points at the wider benefits of the circular economy 1 , including in lowering current carbon dioxide emissions levels. Action on the circular economy therefore ties in closely with key EU priorities, including jobs and growth, the investment agenda, climate and energy, the social agenda and industrial innovation, and with global efforts on sustainable development. Economic actors, such as business and consumers, are key in driving this process. Local, regional and national authorities are enabling the transition, but the EU also has a fundamental role to play in supporting it. The aim is to ensure that the right regulatory framework is in place for the development of the circular economy in the single market, and to give clear signals to economic operators and society at large on the way forward with long term waste targets as well as a concrete, broad and ambitious set of actions, to be carried out before 2020. Action at EU level will drive investments and create a level playing field, remove obstacles stemming from European legislation or inadequate enforcement, deepen the single market, and ensure favourable conditions for innovation and the involvement of all stakeholders. The legislative proposals on waste, adopted together with this action plan, include long-term targets to reduce landfilling and to increase preparation for reuse and recycling of key waste streams such as municipal waste and packaging waste. The targets should lead Member States gradually to converge on bestpractice levels and encourage the requisite investment in waste management. Further measures are proposed to make implementation clear and simple, promote economic incentives and improve extended producer responsibility schemes. By stimulating sustainable activity in key sectors and new business opportunities, the plan will help to unlock the growth and jobs potential of the circular economy. It includes comprehensive commitments on ecodesign, the development of strategic approaches on plastics and chemicals, a major initiative to fund innovative projects under the umbrella of the EU's Horizon 2020 research programme, and targeted action in areas such as plastics, food waste, construction, critical raw materials, industrial and mining waste, consumption and public procurement. Other key legislative proposals on fertilisers and water reuse will follow. Finally, horizontal enabling measures in areas such as innovation and investment are included to stimulate the transition to a circular economy. The proposed actions support the circular economy in each step of the value chain – from production to consumption, repair and remanufacturing, waste management, and secondary raw materials that are fed back into the economy. The actions proposed will be taken forward in line with Better Regulation principles, and subject to appropriate consultation and impact assessment. The action plan focusses on action at EU level with high added value. Making the circular economy a reality will however require long-term involvement at all levels, from Member States, regions and cities, to businesses and citizens. Member States are invited to play their full part in EU action, integrating and complementing it with national action. The circular economy will also need to develop globally. Increased policy coherence in internal and external EU action in this field will be mutually reinforcing and essential for the implementation of global commitments taken by the Union and by EU Member States, notably the U.N. 2030 Agenda for Sustainable Development and the G7 Alliance on Resource Efficiency. This action plan will be instrumental in reaching the Sustainable Development Goals (SDGs) by 2030, in particular Goal 12 of ensuring sustainable consumption and production patterns.},
annote = {Document in Sustainability.

Note that there is planning information in this, towards the end.},
author = {EU-COM-15},
booktitle = {COM/2015/0614 },
editor = {Commission, European},
isbn = {52015DC0614},
keywords = {Governance,circular economy,competitive challenges,legal,networks,philosophy,product substitution,requirements,strategic planning,strategy},
title = {{Closing the loop - An EU action plan for the Circular Economy}},
url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52015DC0614},
volume = {COM/2015/0},
year = {2015}
}
@article{Buckland2011,
abstract = {The 2010 Biodiversity Target of the Convention on Biological Diversity (CBD), set in 2002, which stated that there should be ‘a significant reduction of the current rate of biodiversity loss' by 2010, highlighted the need for informative and tractable metrics that can be used to evaluate change in biological diversity. While the subsequent Aichi 2020 targets are more wide-ranging, they also seek to reduce the rate of biodiversity loss. The geometric mean of relative abundance indices, G, is increasingly being used to examine trends in biological diversity and to assess whether biodiversity targets are being met. Here, we explore the mathematical and statistical properties of G that make it useful for judging temporal change in biological diversity, and we discuss its advantages and limitations relative to other measures. We demonstrate that the index reflects trends in both abundance and evenness, and that it is not prone to bias when detectability of individuals varies by species. We note that it allows ...},
author = {Buckland, Stephen T. and Studeny, Angelika C. and Magurran, Anne E. and Illian, Janine B. and Newson, Stuart E.},
doi = {10.1890/es11-00186.1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Buckland et al. - 2011 - The geometric mean of relative abundance indices a biodiversity measure with a difference.pdf:pdf},
issn = {2150-8925},
journal = {Ecosphere},
month = {sep},
number = {9},
pages = {art100},
publisher = {Wiley},
title = {{The geometric mean of relative abundance indices: a biodiversity measure with a difference}},
volume = {2},
year = {2011}
}
@article{Oremus2016,
address = {Washington DC},
annote = {Full document below.

"It seems Facebook's human news editors weren't quite as expendable as the company thought.

On Monday, the social network's latest move to automate its “Trending” news section backfired when it promoted a false story by a dubious right-wing propaganda site. The story, which claimed that Fox News had fired anchor Megyn Kelly for being a “traitor,” racked up thousands of Facebook shares and was likely viewed by millions before Facebook removed it for inaccuracy.

The blunder came just three days after Facebook fired the entire New York–based team of contractors that had been curating and editing the trending news section, as Quartz first reported on Friday and Slate has confirmed. That same day, Facebook announced an “update” to its trending section—a feature that highlights news topics popular on the site—that would make it “more automated.”

Facebook's move away from human editors was supposed to extinguish the (farcically overblown) controversy over allegations of liberal bias in the trending news section. But in its haste to mollify conservatives, the company appears to have rolled out a new product that members of its own trending news team viewed as seriously flawed.

Three of the trending team members who were recently fired told Slate they understood from the start that Facebook's ultimate goal was to automate the process of selecting stories for the trending news section. Their team was clearly a stopgap. But all three said independently that they were shocked to have been let go so soon, because the software that was meant to supplant them was nowhere near ready. “It's half-baked quiche,” one told me.

Before we poke and prod that quiche, it's worth clearing up a popular misunderstanding. Facebook has not entirely eliminated humans from its trending news product. Rather, the company replaced the New York–based team of contractors, most of whom were professional journalists, with a new team of overseers. Apparently it was this new team that failed to realize the Kelly story was bogus when Facebook's trending algorithm suggested it. Here's how a company spokeswoman explained the mishap to me Monday afternoon:

The Trending review team accepted this topic over the weekend. Based on their review guidelines, the topic met the conditions for acceptance at the time because there was a sufficient number of relevant articles and posts. On re-review, the topic was deemed as inaccurate and does no longer appear in trending. We're working to make our detection of hoax and satirical stories more accurate as part of our continued effort to make the product better.
So: Blame the people, not the algorithm, which is apparently the same one Facebook was using before it fired the original trending team. Who are these new gatekeepers, and why can't they tell the difference between a reliable news source and Endingthefed.com, the publisher of the Kelly piece? Facebook wouldn't say, but it offered the following statement: “In this new version of Trending we no longer need to draft topic descriptions or summaries, and as a result we are shifting to a team with an emphasis on operations and technical skillsets, which helps us better support the new direction of the product.”

That helps clarify the blog post Facebook published Friday, in which it explained the move to simplify its trending section as part of a push to scale it globally and personalize it to each user. “This is something we always hoped to do but we are making these changes sooner given the feedback we got from the Facebook community earlier this year,” the company said.

That all made sense to the three former trending news contractors who spoke with Slate. (They spoke separately and on condition of anonymity, citing a nondisclosure agreement, but they agreed on multiple key points and details.) The former contractors said they weren't told much about their role or the future of the product they were working on, but the companies that hired them—one an Indiana-based consultancy called BCforward, the other a Texas firm called MMC—did indicate their jobs were not permanent. They also understood that the internal software that identified topics for trending news was meant to improve over time, so it could eventually take on more of the work itself.

The strange thing, they told me, was the algorithm didn't seem to be getting much better at selecting relevant stories or reliable news sources. “I didn't notice a change at all,” said one, who had worked on the team for close to a year. The system was constantly being refined, the former contractor added, by Facebook engineers with whom the trending contractors had no direct contact. But the improvements focused on the content management system and the curation guidelines the humans worked with. The feed of trending stories surfaced by the algorithm, meanwhile, was “not ready for human consumption—you really needed someone to sift through the junk.”

The second former contractor, who joined the team more recently, actually liked the idea of helping to train software to curate a personalized feed of trending news stories for readers around the world. “When I entered into it, I thought, ‘Well, the algorithm's basic right now, so that's not going to be [autonomous] for a couple years.' The volume of topics we would get, it would be hundreds and hundreds. It was just this raw feed,” full of clickbait headlines and topics that bore no relation to actual news stories. The contractor estimated that, for every topic surfaced by the algorithm that the team accepted and published, there were “four or five” that the curators rejected as spurious.

The third contractor, who agreed that the algorithm remained sorely in need of human editing and fact-checking, estimated that out of every 50 topics it suggested, about 20 corresponded to real, verifiable news events. But when the news was real, the top sources suggested by the algorithm often were not credible news outlets.

The contractors' perception that their jobs were secure, at least for the medium term, was reinforced when Facebook recently began testing a new trending news feature—a stripped-down version that replaced summaries of each topic with the number of Facebook users talking about it. This new version, two contractors believed, gave the human curators a greatly diminished role in story and source selection. Said one: “You'll get Endingthefed.com as your news source (suggested by the algorithm), and you won't be able to go out and say, ‘Oh, there's a CNN source, or there's a Fox News source, let's use that instead.' You just have a binary choice to approve it or not.”

The results were not pretty. “They were running these tests with subsets of users, and the feedback they got internally was overwhelmingly negative. People would say, ‘I don't understand why I'm looking at this. I don't see the context anymore.' There were spelling mistakes in the headlines. And the number of people talking about a topic would just be wildly off.” The negative feedback came from both Facebook employees participating in internal tests and external Facebook users randomly selected for small public tests.

The contractor assumed Facebook's engineers and product managers would go back to the drawing board. Instead, on Friday, the company dumped the journalists and released the new, poorly reviewed version of trending news to the public.

Why was Facebook so eager to make this move? The company may well have deemed journalists more trouble than they're worth after several of them set off a firestorm by criticizing the product in the press. Others complained about the “toxic” working conditions or dished dirt on Twitter after being let go. Journalists are a cantankerous lot, and in many ways a poor fit for Silicon Valley tech companies like Facebook that thrive on opacity and cultivate the perception of neutrality."},
author = {Oremus, W},
chapter = {30.08.2016},
journal = {Slate Magazine},
keywords = {Cyber-physical systems,agents,automation,case studies,consequences,decision-making,errors,ethics,expertise,federated control,organisational design,robots,situation awareness,socio-tech,software,staffing},
publisher = {Washington Post},
title = {{Trending Bad. How Facebook's foray into automated news went from messy to disastrous.}},
url = {http://www.slate.com/articles/technology/future{\_}tense/2016/08/how{\_}facebook{\_}s{\_}trending{\_}news{\_}feature{\_}went{\_}from{\_}messy{\_}to{\_}disastrous.html},
year = {2016}
}
@techreport{Hawley2017,
address = {Washington DC},
annote = {Document in Autonomy!!

Also attached is the article for The Ergonomist, 2017

Quotation:
““Decision makers, system developers, and users confidently assert that the requirement for positive human control is met even if that means not much more than having a warm body at the system's control station. One of the hard lessons of my 35 years of experience with Patriot is that an automated system in the hands of an inadequately trained crew is a de facto fully-automated system. Moreover, I'm no longer sure what the term “adequately trained” means within the context of supervisory control of near-autonomous operations. {\ldots} In many respects, calling for reliable supervisory control over a complex automated system is an unreasonable performance expectation.”

Concluding, practical, comment:
There are few hard and fast rules for achieving effective human-automation integration. Most such rules or design guidance are, in essence, design rules of thumb. Consequently, the degree to which acceptable human-automation integration has been achieved often must be determined empirically on a trial-and-error basis after system prototypes are available. Effective usability work of that kind requires real-time interactions with expert job performers, or as close as we can come to that. [But note time and cost implications of this recommendation]His listed failures in automatic operaton in Gulf War 2 (Operation Iraqi Freedom):
• They [US Army] had trusted the system in a naïve manner [because of Patriot successes in Gulf War 1, Operation Desert Storm]; 
• they had not adequately prepared their operators and crews for proper oversight of automated operations; 
• and they had been unwilling or unable to confront the fact that near-autonomous operations are qualitatively different from old-style manual control (i.e., “on-the-loop” versus “in-the-loop” control). 
• In short, they had failed to adapt to the complex new capability they possessed.

Commentary on shot-down F18:
One of the more interesting aspects of Patriot tactical operations after the first OIF fratricide incident (the British Tornado) was a decision to have
fire units drop their launchers to standby mode. That way, the system could remain in automatic engagement mode but not actually engage a track until one or more launchers were returned to ready status. Commanders apparently wanted a “second look” before permitting the system to engage. 

The second OIF fratricide (the Navy F-18) took place under this modified operating regimen. The system reported a false ballistic missile track later attributable to radar electromagnetic interference. The tactical director at the battalion command and control node gave the order, “Bring your launchers to ready.” That directive was tantamount to an order to engage. But that was not what the tactical director intended; he simply wanted to get ready to engage by bringing fire unit launchers to ready status. The subordinate battery fire units were in tactical ballistic missile automatic mode. The tactical director either did not know that, or he did not remember in the heat of impending action that returning launchers to ready status would result in an automatic engagement by the first available launcher. The F-18 was engaged and destroyed.

A later Army board of inquiry recommended that the tactical director be issued a general officer reprimand for not terminating the engagement. In an obvious example of hindsight bias, the board determined that there was sufficient evidence available at the time to have termi- nated the engagement after missile launch. I thought the reprimand was unwarranted. In both fratricide incidents, the Patriot crews did what they had been trained to do, which was reinforced by the prevailing command climate and widespread, but not generally accurate, beliefs about the system's engagement reliability. 

In retrospect, I have never believed that the launch crew knew for certain they had engaged the F-18. They shot at what the system initially determined was a tactical ballistic missile.

Training comments:
Training has been modified to include incidents similar to those encountered during OIF. Trainees are encouraged (and instructed on how) to query to the system to confirm or disconfirm its track classification and identification results. 

However, the length of institutional and collective (unit) training has not been increased substantially. Training times still fall short of what the literature on operator expertise suggests for jobs of Patriot's complexity. 

For the most part, the training changes that have been made are add-ons or modifications to older training curricula. New approaches to, and objectives for, operator and crew training have been recommended but have not been implemented.

Moreover, Patriot operators and crews still do not remain in hands-on air battle management roles long enough to become truly proficient in their jobs. Routine Army personnel practices interfere with the development of essential levels of individual and crew expertise. 

During operational tests of Patriot software upgrades, incidents of the sort that occurred during OIF still occur. This is particularly true when test events go off-script, and operators are presented with situations they have not previously seen or explicitly trained to address.

Problems in automated systems design
• Automated Systems Seldom Provide All Anticipated Benefits. Newly automated systems rarely live up to their initial billing. First-time users of automated systems must anticipate a debugging and calibration period during which the system's actual capabilities and limitations are determined. It is often necessary for field users to deter- mine how they should practically employ the system, as opposed to unquestioningly using it the way system devel- opers think it should be used.
• Increased System Monitoring Load. Automation may change the nature of an operator's job, but it does not always simplify it. Automated systems often are characterized by a proliferation of components brought on by increased system complexity. Under an automation regimen, operators often have less to
do moment-to-moment, but as a consequence of an increased number of components, they have more indications of system status to monitor. Vigilance can be a problem.
• False Sense of Security. Belief in the system's infallibility (i.e., it's always right) can lull operators into a false sense of security, with the result that they will not make checks that would otherwise be advisable. Long periods of time during which the system operates successfully have been observed to lead to complacency, and com- placency can result in reduced vigilance and a lessening of operational prudence.
• Automation Transforms Operators into System Monitors Rather than Active Controllers. Automation does not remove human operators from the system. Rather, it moves human operators from direct, in-the-loop control of system operations to higher-level supervisory control tasks (that is, on-the-loop control). Problems can arise when the automated control system has been developed because it presumably can do the job better than a human
and less hands-on experience. This situation has been identified as a significant problem for pilots who rely excessively on automated flight control systems. It also will be a problem for “drivers” of future self-driving cars. Any notions that such drivers will be able to rapidly and seam- lessly disengage from whatever they are doing and assume control from the vehicle's automation under unusual or ambiguous circumstances are not borne out by past expe- rience with automated systems. Such control transitions will be problematic.[How true, as Uber has found out in Arizona].
• Out-of-the-Loop Familiarity. When system operator tasks are replaced by automation, the operators' level of interaction or familiarity with the system is reduced. There is considerable evidence that when an abnormal situation does occur, operators will be slower to detect it and will take a longer time to jump back into the control loop and make the appropriate control actions. [They will have lost practice, competence, accuracy, and speed]
• Increased Training Requirements. One of the most common myths about automation is that as a system's auto- mation level increases, less human expertise is required. Contrary to this popular belief, automation does not always lessen operator training requirements. It frequently changes the nature of operator performance demands Increased Training Requirements. One of the most common myths about automation is that as a system's auto- mation level increases, less human expertise is required. Contrary to this popular belief, automation does not always lessen operator training requirements. It frequently changes the nature of operator performance demands},
author = {Hawley, J K},
keywords = {Consent (informed):,Cyber-physical systems,affordances,agents,allocation of functions,automation,consequences,control,dashboard/battlespace,decision-making,engineering,ergonomics,ethics,federated control,interface design,job design,network-centric warfare,philosophy,resilience,robots,safety,socio-tech,task analysis,training},
publisher = {Center for a New American Security},
title = {{Patriot wars: automation and the Patriot air and missile defense system}},
url = {https://www.cnas.org/publications/reports/patriot-wars},
volume = {Ethical Au},
year = {2017}
}
@techreport{EPSRC2014,
abstract = {Expectation I Research organisations will promote internal awareness of these principles and expectations and ensure that their researchers and research students have a general awareness of the regulatory environment and of the available exemptions which may be used, should the need arise, to justify the withholding of research data; Clarification This is expected to be an on‐going part of the normal business of organisation‐based training and awareness for staff and students in all research organisations funded by EPSRC. For example, research data management may be addressed as part of the normal induction process for new staff/research students, and awareness of the issues may be subsequently refreshed through regular online and/or facilitated training. Expectation II Published research papers should include a short statement describing how and on what terms any supporting research data may be accessed. Clarification},
author = {EPSRC},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/EPSRC - 2014 - Clarifications of EPSRC expectations on research data management .pdf:pdf},
institution = {EPSRC},
number = {October},
pages = {1--7},
title = {{Clarifications of EPSRC expectations on research data management .}},
url = {https://epsrc.ukri.org/files/aboutus/standards/clarificationsofexpectationsresearchdatamanagement/},
year = {2014}
}
@article{Kitchin2015,
abstract = {Academic knowledge building has progressed for the past few centuries using small data studies characterized by sampled data generated to answer specific questions. It is a strategy that has been remarkably successful, enabling the sciences, social sciences and humanities to advance in leaps and bounds. This approach is presently being challenged by the development of big data. Small data studies will however, we argue, continue to be popular and valuable in the future because of their utility in answering targeted queries. Importantly, however, small data will increasingly be made more big data-like through the development of new data infrastructures that pool, scale and link small data in order to create larger datasets, encourage sharing and reuse, and open them up to combination with big data and analysis using big data analytics. This paper examines the logic and value of small data studies, their relationship to emerging big data and data science, and the implications of scaling small data into data infrastructures, with a focus on spatial data examples.},
author = {Kitchin, Rob and Lauriault, Tracey P.},
doi = {10.1007/s10708-014-9601-7},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kitchin, Lauriault - 2015 - Small data in the era of big data.pdf:pdf},
issn = {03432521},
journal = {GeoJournal},
keywords = {Big data,Cyber-infrastructures,Data infrastructures,Epistemology,Ontology,Small data},
month = {aug},
number = {4},
pages = {463--475},
publisher = {Kluwer Academic Publishers},
title = {{Small data in the era of big data}},
volume = {80},
year = {2015}
}
@article{Allen2019,
abstract = {Key points • The structured Contributor Role Taxonomy (CRediT) taxonomy, introduced in 2014, is now used in over 120 journals and set to grow substantially in the next couple of years. • CRediT responds to calls for greater transparency and recognition of author con- tributions and is increasingly being used to investigate authorship. • Whilst initially implemented in the life sciences, identification of contributorship is increasingly being seen as important in all disciplines},
author = {Allen, Liz and O'Connell, Alison and Kiermer, Veronique},
doi = {10.1002/leap.1210},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Allen, O'Connell, Kiermer - 2019 - How can we ensure visibility and diversity in research contributions How the Contributor Role Taxon.pdf:pdf},
issn = {17414857},
journal = {Learned Publishing},
month = {jan},
number = {1},
pages = {71--74},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{How can we ensure visibility and diversity in research contributions? How the Contributor Role Taxonomy (CRediT) is helping the shift from authorship to contributorship}},
url = {http://doi.wiley.com/10.1002/leap.1210},
volume = {32},
year = {2019}
}
@article{Broadus1987a,
author = {Broadus, Robert N.},
doi = {10.2307/40323625},
issn = {0748-5786},
journal = {Journal of Education for Library and Information Science},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {2},
pages = {152--153 ST -- Some Notes on Research in Bibliometr},
title = {{Some Notes on Research in Bibliometrics}},
url = {http://www.jstor.org/stable/10.2307/40323625?origin=crossref},
volume = {28},
year = {1987}
}
@techreport{Danzig2018,
abstract = {This report recognizes the imperatives that inspire the U.S. military's pursuit of technological superiority over all potential adversaries. These pages emphasize, however, that superiority is not synonymous with security. Experience with nuclear weapons, aviation, and digital information systems should inform discussion about current efforts to control artificial intelligence (AI), synthetic biology, and autonomous systems. In this light, the most reasonable expectation is that the introduction of complex, opaque, novel, and interactive technologies will produce accidents, emergent effects, and sabotage. In sum, on a number of occasions and in a number of ways, the American national security establishment will lose control of what it creates.},
address = {Washington DC},
author = {Danzig, Richard},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Danzig - 2018 - Managing Loss of Control as Many Militaries Pursue.pdf:pdf},
institution = {Center for a New American Security},
number = {June},
pages = {40},
title = {{Managing Loss of Control as Many Militaries Pursue}},
year = {2018}
}
@article{Selman2000,
abstract = {In order to deal with the inherent combinatorial nature of many tasks in artificial intelligence, domain-specific knowledge has been used to control search and reasoning or to eliminate the need for general inference altogether. However, the process of acquiring domain knowledge is an important bottleneck in the use of such "knowledge-intensive" methods. Compute-intensive methods, on the other hand, use extensive search and reasoning strategies to limit the need for detailed domain-specific knowledge. The idea is to derive much of the needed information from a relatively compact formalization of the domain under consideration. Up until recently, such general reasoning strategies were much too expensive for use in applications of interesting size but recent advances in reasoning and search methods have shown that compute-intensive methods provide a promising alternative to knowledge-intensive methods.},
author = {Selman, Bart},
doi = {10.1023/A:1018943920174},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Selman - 2000 - Compute-intensive methods in artificial intelligence.pdf:pdf},
issn = {10122443},
journal = {Annals of Mathematics and Artificial Intelligence},
number = {1-4},
pages = {35--38},
title = {{Compute-intensive methods in artificial intelligence}},
volume = {28},
year = {2000}
}
@inproceedings{teh2000low-costpolymers,
author = {Teh, N J and Palmer, P J and Conway, P P},
booktitle = {ADVANCES IN MANUFACTURING TECHNOLOGY - XIV},
isbn = {1-86058-267-2},
pages = {375--379},
title = {{Low-cost automatic embedding of electronics within injection moulded polymers}},
url = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2{\&}SrcApp=PARTNER{\_}APP{\&}SrcAuth=LinksAMR{\&}KeyUT=WOS:000166192200061{\&}DestLinkType=FullRecord{\&}DestApp=ALL{\_}WOS{\&}UsrCustomerID=a2493cf9206fa1916527cf88227fb107},
year = {2000}
}
@inproceedings{Shneiderman1996,
abstract = {A useful starting point for designing advanced graphical user$\backslash$ninterfaces is the visual information seeking Mantra: overview first,$\backslash$nzoom and filter, then details on demand. But this is only a starting$\backslash$npoint in trying to understand the rich and varied set of information$\backslash$nvisualizations that have been proposed in recent years. The paper offers$\backslash$na task by data type taxonomy with seven data types (one, two, three$\backslash$ndimensional data, temporal and multi dimensional data, and tree and$\backslash$nnetwork data) and seven tasks (overview, zoom, filter,$\backslash$ndetails-on-demand, relate, history, and extracts)},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shneiderman, B.},
booktitle = {Proceedings 1996 IEEE Symposium on Visual Languages},
doi = {10.1109/VL.1996.545307},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shneiderman - 1996 - The eyes have it a task by data type taxonomy for information visualizations.pdf:pdf},
isbn = {0-8186-7508-X},
issn = {1049-2615},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {336--343},
pmid = {4986861},
title = {{The eyes have it: a task by data type taxonomy for information visualizations}},
url = {https://www.sciencedirect.com/science/article/pii/B9781558609150500469 http://ieeexplore.ieee.org/document/545307/},
year = {1996}
}
@phdthesis{Almeida2018,
abstract = {Organizations deal with an ever increasing amount of data. Globalization and new business models brought about the need for more complex information systems and for the integration of data from different repositories. This in turn has led to an increase of potential data quality problems (DQPs), with consequences at information and management decisions' levels. Current approaches tackling DQPs rely on the specification by an expert of the most appropriate Data Cleaning Operations (DCOs), i.e., for detecting and correcting DQPs. Most of these approaches, and the corresponding data cleaning tools (DCTs) are tied to a specific data model or a specific schema, which makes their DCOs difficult to reuse in different situations. The use of ontologies to represent the semantics of DCOs unties the link to specific schemas or data models. Furber {\&} Hepp built an ontology (Data Quality Management Ontology) and a vocabulary for DCOs. The use of these ontologies along with the establishment of correspondences (ontology matching) between ontologies helps to solve their semantic heterogeneity. These correspondences along with the improvement of their expressivity is a field of ongoing research},
author = {Almeida, RGSF},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Almeida - 2018 - Semi-automatic instantiation of data cleaning operations specified at conceptual level.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {182},
school = {Universidade de Tr{\'{a}}s-os-Montes e Alto Douro},
title = {{Semi-automatic instantiation of data cleaning operations specified at conceptual level}},
url = {http://repositorio.utad.pt/handle/10348/8247 http://hdl.handle.net/10348/8247},
year = {2018}
}
@article{Bui2016,
abstract = {Objectives Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. In this study, we developed a text summarization system aimed at enhancing productivity and reducing errors in the traditional data extraction process. Methods We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications. The summaries at the sentence and fragment levels were evaluated in finding common clinical SR data elements such as sample size, group size, and PICO values. We compared the computer-generated summaries with human written summaries (title and abstract) in terms of the presence of necessary information for the data extraction as presented in the Cochrane review's study characteristics tables. Results At the sentence level, the computer-generated summaries covered more information than humans do for systematic reviews (recall 91.2{\%} vs. 83.8{\%}, p {\textless} 0.001). They also had a better density of relevant sentences (precision 59{\%} vs. 39{\%}, p {\textless} 0.001). At the fragment level, the ensemble approach combining rule-based, concept mapping, and dictionary-based methods performed better than individual methods alone, achieving an 84.7{\%} F-measure. Conclusion Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system.},
author = {Bui, Duy Duc An and {Del Fiol}, Guilherme and Hurdle, John F. and Jonnalagadda, Siddhartha},
doi = {10.1016/j.jbi.2016.10.014},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bui et al. - 2016 - Extractive text summarization system to aid data extraction from full text in systematic review development.pdf:pdf},
isbn = {1532-0480 (Electronic) 1532-0464 (Linking)},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Data collection,Machine learning,Review literature as topic,Text classification,Text summarization},
month = {dec},
pages = {265--272},
pmid = {27989816},
publisher = {Academic Press},
title = {{Extractive text summarization system to aid data extraction from full text in systematic review development}},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416301514},
volume = {64},
year = {2016}
}
@incollection{Gitelman2013,
abstract = {This chapter contains sections titled: Introduction, Behind the Data Archive, Routines and Ritual: “We Go Out on Wednesdays”, Shifting Field Sites: Environment, Humans and Infrastructure over Time, Cascading Rituals: From Field to Lab, Conclusion, Acknowledgments},
author = {Ribes, David and Jackson, Steven J.},
booktitle = {"Raw Data" Is an Oxymoron},
chapter = {8},
doi = {10.7551/mitpress/9302.001.0001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ribes, Jackson - 2013 - Data Bite Man The Work of Sustaining a Long-Term Study.pdf:pdf},
isbn = {9780262312325},
month = {mar},
pages = {147--166},
publisher = {The MIT Press},
title = {{Data Bite Man: The Work of Sustaining a Long-Term Study}},
year = {2013}
}
@misc{Tjukanov2018,
abstract = {So might an average person working with GIS or data visualizations think. I must admit that databases aren't the sexiest thing in the world (sorry DBA's), but if you are claiming (or aiming) to do analytics or visualization with (spatial) data in a more serious manner, you definitely shouldn't ignore them. I hope this blog post can give you an idea what kind of benefits the efficient use of spatial databases could offer you. Hype terms come and go in IT and there was a big hype around big data still just a few years ago, but that is now slowly fading away. Well, data is still big and actually it's bigger than ever. File sizes grow and in “data science” and geosciences people must deal with data that can easily be in the range of gigabytes. The bigger the data, more attention we need to pay to the way we store and analyze it.},
author = {Tjukanov, Topi},
booktitle = {Medium},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{Why should you care about PostGIS? — A gentle introduction to spatial databases}},
url = {https://medium.com/@tjukanov/why-should-you-care-about-postgis-a-gentle-introduction-to-spatial-databases-9eccd26bc42b?{\_}hsenc=p2ANqtz-9wj31ILjhqD8gEPTrCmkn2LYwx491Ufll4cXCn3jU27dL6WHITPKupu4QPWuueYDKX9DejeX5zY6jcK0p{\_}4hGSEylEm-WqecZoHvXKCM{\_}NV261bdU{\&}{\_}hsmi=6},
urldate = {2018-06-12},
year = {2018}
}
@article{Shimadzu2013,
abstract = {BACKGROUND: Some of the most marked temporal fluctuations in species abundances are linked to seasons. In theory, multispecies assemblages can persist if species use shared resources at different times, thereby minimizing interspecific competition. However, there is scant empirical evidence supporting these predictions and, to the best of our knowledge, seasonal variation has never been explored in the context of fluctuation-mediated coexistence.$\backslash$n$\backslash$nRESULTS: Using an exceptionally well-documented estuarine fish assemblage, sampled monthly for over 30 years, we show that temporal shifts in species abundances underpin species coexistence. Species fall into distinct seasonal groups, within which spatial resource use is more heterogeneous than would be expected by chance at those times when competition for food is most intense. We also detect seasonal variation in the richness and evenness of the community, again linked to shifts in resource availability.$\backslash$n$\backslash$nCONCLUSIONS: These results reveal that spatiotemporal shifts in community composition minimize competitive interactions and help stabilize total abundance.},
author = {Shimadzu, Hideyasu and Dornelas, Maria and Henderson, Peter A and Magurran, Anne E},
doi = {10.1186/1741-7007-11-98},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shimadzu et al. - 2013 - Diversity is maintained by seasonal variation in species abundance.pdf:pdf},
isbn = {1741-7007},
issn = {17417007},
journal = {BMC Biology},
keywords = {Biodiversity,Fluctuation mediated coexistence,Species coexistence,Stability,Storage effect},
number = {1},
pages = {98},
pmid = {24007204},
title = {{Diversity is maintained by seasonal variation in species abundance}},
url = {http://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-11-98},
volume = {11},
year = {2013}
}
@book{Cook2007,
abstract = {Instructional Book for using R for programming},
annote = {From Duplicate 1 (Interactive and Dynamic Graphics for Data Analysis with R! and GGobi - Cook, Dianne; Swayne, Deborah F)

id: 1; isbn: print 978-0-387-71761-6; isbn: electronic 978-0-387-71762-3},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cook, Dianne and Swayne, Deborah F.},
doi = {10.1007/978-0-387-71762-3},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-71761-6},
issn = {9780387938363},
pmid = {22057480},
publisher = {Springer},
title = {{Interactive and Dynamic Graphics for Data Analysis with R! and GGobi}},
url = {http://dx.doi.org/10.1007/978-0-387-71762-3 http://link.springer.com/10.1007/978-0-387-71762-3},
year = {2007}
}
@techreport{Conklin1996,
abstract = {Knowledge management is an essential capability in the emerging knowledge economy. In particular, organizations have a valuable asset in the informal knowledge that is the daily currency of their knowledge workers, but this asset usually lives only in the collective human memory, and thus is poorly preserved and managed. There are significant technical and cultural barriers to capturing informal knowledge and making it explicit. Groupware tools such as E-mail and Lotus Notes{\&}trade; tend to make informal knowledge explicit, but they generally fail to create a coherent organizational memory. On the other hand, attempts to build organizational memory systems have generally failed because they required some additional documentation effort with no clear short term benefit, or, like groupware, they did not provide an effective index or structure to the mass of information collected in the system. This paper explores the design of an organizational memory system that overcomes the barriers to capturing informal knowledge. The key component of this design is the use of a display system which captures the key issues and ideas during meetings and creates shared understanding in a knowledge team. The paper briefly describes a display system, QuestMap{\&}trade;, which uses hypertext to capture the thinking and learning in large, complex projects. The paper ends with a few examples of this kind of organizational memory system in action.},
annote = {Full paper at web address above. Note his end-of-paper summary below:

"Workgroup computing ("groupware") tools take an important step in the direction of facilitating knowledge work, and their databases inherently create some degree of organizational memory. But such tools also can-and do-create mountains of incoherent rubbish. The problem is that, to avoid the attic-full-of-stuff syndrome, knowledge must be organized and indexed as it is being captured, without creating a burden to the people who create it. 

The story I am telling in this paper can be summarized as follows: 

1.The concept of organizational memory, and the possibility of an effective organizational memory system, has growing importance in the global knowledge economy, but many organizations are letting their most valuable asset-their informal knowledge-go "up the smokestack." 
2.Current implementations of organizational memory fail for a variety of reasons, including: (a) a broad cultural focus on work products over process, (b) lack of tools which make capture and reuse of knowledge transparent. 
3.The challenge is to design an organizational memory system which offers sufficient short term payoffs to knowledge workers that they will use the system, both to capture knowledge as they are creating it and to look for and reuse existing knowledge. 
4.The next step in the evolution of organizational memory is the use of a display system to (a) focus knowledge workers on improving shared understanding and coherence in their project meetings, and (b) capture the group's informal knowledge-in context-and link it with the project's formal products in an easy and natural way. 
5.The display system mediates access to the organizational memory. It must (a) be readily available for all team meetings, (b) be linked into the organization's computing network so that information can be easily stored and retrieved, and (c) structure informal knowledge in a way that enhances the process of creative teamwork. The IBIS structure is generally a good starting point. 
6.Such display systems are not mere theory-they exist and work (an example is QuestMap). Display system tools do not solve the whole organizational memory problem, but they reflect essential principles that must be embodied in the design of an effective organizational memory system. 

Once a team or organization has recognized the value in its informal knowledge, and has begun to capture and manage it appropriately, it has the key raw ingredients of organizational memory. GDSS has direct experience, through our clients, of the value of this memory for a team when they come back later and need to take up where they left off. Of course, as the size of the organization-and its memory-increases, new problems of scale emerge that are both technical and cultural in nature. The good news is that the short term pay off from using display systems can pay for the cost of implementing them, thus paying for the next step toward a complete organizational memory system."

He writes as a paid-up member of the computing science fraternity. His focus is on the need to capture process, not just documents. Latter are the output of the process, and fail to give the context of the decision-making process. Therefore, you have to capture the informal process, which is what he discusses. But only within the org; no mention of the supply chain. 

Probably the most important part of the paper is where he discusses previous aattempts at org. memory:

"There was a project at an aerospace company some years ago in which the team decided it would capture its project memory. They preserved official reports, design documents, presentations, memos, meeting minutes-virtually anything that they wrote down. At the end of the project they had indeed created a project memory: an office completely filled with stacks of paper, extending almost to the ceiling. Even if someone thought there might be valuable information stored in that room, no one ever wanted to go in there and try to find it. 

Other project teams have recognized the importance of preserving the informal knowledge involved in the project. The team leader-keen on the importance of capturing informal knowledge-instructs the team members to write down important ideas, decisions, notes, and communications, either on paper or in email. Everything goes along fine until a deadline or milestone approaches and the pressure increases-then the team quietly drops these extra documentation duties in favor of doing their "real" work-producing formal artifacts. Because of this, the ideas created and the decisions made during this highly productive crunch phase of the project are not captured. 

A few project teams have attempted to capture their thinking and learning by audio or videotaping their meetings. Inevitably these teams end up with a staggering volume of tape. The key bits of knowledge they need later on are in there somewhere, but who has the time to watch or listen to it all to find them? (Note 8). 

A few projects have had the luxury of a project historian or librarian-someone whose job is to capture and organize theknowledge created in the course of the project. Unfortunately, these jobs don't seem to last very long. Without a way to capture the informal knowledge as well, and without an organizational commitment to accessing and using this knowledge, it is just an expensive way of filling the attic. Moreover, historians and librarians tend to be regarded by project teams as outsiders, which can further complicate their job (Note 9). 

The concept of creating, archiving, and using learning histories, as part of the research and engineering function, is an old one. It is considered good engineering practice to create reports documenting "lessons learned" on a project. However, even in companies where this practice has become part of the "standard operating procedure," it is very difficult to find instances of the resulting document actually being referenced in the next project, or of it informing future projects. 

More recently, some organizations have attempted to use groupware tools such as Lotus Notes™ to create and manage organizational memory. Groupware is designed to be used for informal communications, and thus has the potential to become a repository for an organization's informal knowledge. Unfortunately, groupware messages and documents tend to lack any structure, so the repository that gets created is just an electronic version of the "attic full of stuff." Most groupware applications include some search capability so that users can search the database for particular keywords. However, if the groupware application has caught hold, and is thus heavily used, its database is usually is too jumbled and incoherent for retrieval to be very satisfying or successful. 

These experiments in capturing organizational memory paint a gloomy picture of the prospect of preserving the most precious asset of the knowledge organization. They illustrate that you can't create a useful memory store just by capturing lots of information, you must somehow organize it in ways that create and preserve coherence and "searchability." These experiments also illustrate a fundamental tension in the design of an organizational memory system. When does this information organizing, structuring, and indexing work get done? 

Most current implementations of organizational memory postpone this organizing effort as long as possible, or try to do it automatically in the background (i.e., using artificial intelligence techniques). Neither of these approaches can capture the critical informal knowledge which gives the information context and has it make sense. 

For all its potential, we have not yet found a way to tap the value in an organization's informal knowledge. My conclusion is that the creation and use of organizational memory cannot be a by-product, an extra bit of work hanging on the side of the knowledge organization's main production process. If we are to find ways of preserving the asset of informal knowledge, we must look within the practices of everyday teamwork and change them. Creating an effective organizational memory system entails creating new tools and new practices, making changes in technology as well as culture (see Eppel {\&} Conklin about the latter). "

Then, later, he tries to characterise the system solution in an example:

"Here is an example. A team is working on developing a new kind of valve for an oil recovery device that their company manufactures. The valve must be able to operate at very cold temperatures, and not get stuck even if the oil flowing through it turns very viscous. The specialists on the team (in mechanics, fluid dynamics, cryogenics, etc.) have standard resources they use to get the basic formal knowledge they need for the design. But, generally, they proceed as if they are working in a kind of vacuum. How do they know if someone in the company has tried to create such a valve in the past? Perhaps an earlier effort was mounted but failed; perhaps it was technically successful but put on the shelf; perhaps someone showed that it was a practical impossibility and abandoned the effort. Perhaps there is, somewhere else in the company (or in a partner company) a specialist who has thought about this problem, but not published any of her results. 

For a computer system to augment this team's memory, it should have the following features: 

• it makes it easy and natural to review similar cases and projects for information that would be helpful now; 
• it avoids false hits by paying attention to relevance and meaning (e.g. On the valve project, doing a search for "oil" and "valve" does not return an heap of documents about cholesterol and heart disease); 
• at the same time as avoiding the false hits, it finds items which are related in interesting ways (e.g. A search for "oil" and "valve" does return items dealing with "refrigeration plumbing"); 
• it preserves context, by providing that retrieved "fact-oids" come with the context in which they originally made sense;
• it captures whatever learning this team does and automatically adds it to the organizational memory; 
• as meaning evolves over time, the links and indexes in the system evolve correspondingly. 

The oil valve project team is not alone: the same problems confront knowledge teams who need to know who else in their enterprise has worked with a given client, or has struggled to comply with a given law or regulation, or has used a given piece of equipment or software, or has bought from a given vendor. 

Technically, there are exciting possibilities for the use of hypertext, groupware, intelligent agents, neural networks, advanced search techniques, genetic algorithms, and other computing technologies to provide "relevance retrieval" access in large databases-retrieval which respects the meaning relationships among the stored items. However, in terms of providing the features listed above, these technologies are still experimental. "

Mentions in passing the process of forced organisational amnesia - where the organisation deliberately destroys its documents to eliminate the chances of successful prosecution. Tobacco firms please note. I think his problem here is that he's talking about information, not knowledge. It shopuld be OK to destroy information, because it has only a limited life anyway. Whic h he then seems to accept, by saying we need to copy human memory, by having a short-term memory store, from which we can then extract LTM valuable knowledge, and then ditch the rest. So projects should generate STM contents; then abstract the important stuff and insert it into LTM. Then argues for computerised whiteboard, complete with hypertext facilities, which you can then use for discussions, etc. to capture easily informal knowledge. Needs a format for doing this too, so that you create efficient knowledge maps, and can link to previopus maps. A key advantage, he claims, is that it makes the discussions more productive per unit time.

To do this you ned IBIS (Conklin, Jeff, and Michael Begeman (1989). "gIBIS: A Tool for All Reasons," Journal of the American Society for Information
Science, 40, 200-213) and QuestMap (QuestMap™ is available from the Soft Bicycle Company, Inc. in Washington, DC. Current information is available at
the SBC Web site: http://www.softbicycle.com). IBIS = Issue-Based Information System, predicated around 4 elements - questions, ideas, pros, cons. QuestMap is a commercial tool that captures this.},
author = {Conklin, E J},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Conklin - 1996 - Designing organisational memory preserving intellectual assets in a knowledge economy.pdf:pdf},
isbn = {http://www.gdss.com/DOM.htm},
keywords = {CSCW,Work flow management,agility,business process re-engineering,communications,competencies,competitive challenges,core competences,culture,design process,holons,information systems,interface design,intranets,knowledge,knowledge configuration,knowledge nets,knowledge structures,learning organisation,mental models,organisational design,prototyping,quality,quotes,reference architectures,socio-tech,software,supply chain,teams,telecoms,usability,user needs},
publisher = {Group Decision Support Systems, Washington D.C.},
title = {{Designing organisational memory: preserving intellectual assets in a knowledge economy}},
year = {1996}
}
@article{Bayarri2016,
abstract = {h i g h l i g h t s • Use odds of a correct rejection of the null hypothesis to incorrect rejection. • Pre-experimentally, these odds are the power divided by the Type I error. • Post-experimentally, these odds are the Bayes factor. • The Bayes factor is shown to be a fully frequentist measure of evidence. • A useful bound on the Bayes factor is given which depends only on the p-value. a b s t r a c t Much of science is (rightly or wrongly) driven by hypothesis testing. Even in situations where the hypothesis testing paradigm is correct, the common practice of basing inferences solely on p-values has been under intense criticism for over 50 years. We propose, as an alternative, the use of the odds of a correct rejection of the null hypothesis to incorrect rejection. Both pre-experimental versions (involving the power and Type I error) and post-experimental versions (depending on the actual data) are considered. Implementations are provided that range from depending only on the p-value to consideration of full Bayesian analysis. A surprise is that all implementations-even the full Bayesian analysis-have complete frequentist justification. Versions of our proposal can be implemented that require only minor modifications to existing practices yet overcome some of their most severe shortcomings.},
author = {Bayarri, M J and Benjamin, Daniel J and Berger, James O and Sellke, Thomas M},
doi = {10.1016/j.jmp.2015.12.007},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bayarri et al. - 2016 - Rejection odds and rejection ratios A proposal for statistical practice in testing hypotheses ✩.pdf:pdf},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factors,Bayesian,Frequentist,Odds},
pages = {90--103},
title = {{Rejection odds and rejection ratios: A proposal for statistical practice in testing hypotheses ✩}},
url = {http://creativecommons.org/licenses/by/4.0/},
volume = {72},
year = {2016}
}
@article{Zhang2003,
abstract = {Data preparation is a fundamental stage of data analysis. While a lot of low-quality information is available in various data sources and on the Web, many organizations or companies are interested in how to transform the data into cleaned forms which can be used for high-profit purposes. This goal generates an urgent need for data analysis aimed at cleaning the raw data. In this paper, we first show the importance of data preparation in data analysis, then introduce some research achievements in the area of data preparation. Finally, we suggest some future directions of research and development.},
author = {Zhang, Shichao and Zhang, Chengqi and Yang, Qiang},
doi = {10.1080/713827180},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Zhang, Yang - 2003 - Data preparation for data mining.pdf:pdf},
isbn = {0883-9514},
issn = {10876545},
journal = {Applied Artificial Intelligence},
month = {may},
number = {5-6},
pages = {375--381},
pmid = {4047876},
title = {{Data preparation for data mining}},
url = {http://www.tandfonline.com/doi/abs/10.1080/713827180},
volume = {17},
year = {2003}
}
@inproceedings{Sintek2002,
abstract = {This paper presents TRIPLE, a layered and modular rule language for the Semantic Web 1. TRIPLE is based on Horn logic and borrows many basic features from F-Logic 11 but is especially designed for querying and transforming RDF models 20.TRIPLE can be viewed as a successor of SiLRI (Simple Logic-based RDF Interpreter 5). One of the most important differences to F-Logic and SiLRI is that TRIPLE does not have a fixed semantics for object-oriented features like classes and inheritance. Its layered architecture allows such features to be easily defined for different object-oriented and other data models like UML, Topic Maps, or RDF Schema 19. Description logics extensions of RDF (Schema) like OIL 17 and DAML+OIL 3 that cannot be fully handled by Horn logic are provided as modules that interact with a description logic classifier, e.g. FaCT 9, resulting in a hybrid rule language. This paper sketches syntax and semantics of TRIPLE.},
author = {Sintek, Michael and Decker, Stefan},
booktitle = {ISWC},
doi = {http://dx.doi.org/10.1007/3-540-48005-6_28},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sintek, Decker - 2002 - TRIPLE - A Query, Inference, and Transformation Language for the Semantic Web.pdf:pdf},
isbn = {3540437606},
issn = {03029743},
number = {2342},
pages = {364--378},
publisher = {Springer, Berlin, Heidelberg},
title = {{TRIPLE - A Query, Inference, and Transformation Language for the Semantic Web}},
url = {http://link.springer.com/10.1007/3-540-48005-6{\_}28 http://www.springerlink.com/index/K1C8DY8QC4BKNPX0.pdf},
volume = {2342},
year = {2002}
}
@misc{Fried2016,
abstract = {A problem we see in psychological network papers is that authors sometimes over-interpret the visualization of their data. This pertains especially to the layout and node placement of the graph, for i},
author = {Fried, Eiko},
booktitle = {Psych Networks - Organized Incoherence},
title = {{R tutorial: how to identify communities of items in networks}},
url = {https://psych-networks.com/r-tutorial-identify-communities-items-networks/},
urldate = {2019-10-29},
year = {2016}
}
@article{Lazer2014,
abstract = {In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?},
author = {Lazer, David and Kennedy, Ryan and King, Gary and Vespignani, Alessandro},
doi = {10.1126/science.1248506},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lazer et al. - 2014 - The parable of google flu Traps in big data analysis.pdf:pdf},
isbn = {0036-8075},
issn = {10959203},
journal = {Science},
month = {mar},
number = {6176},
pages = {1203--1205},
pmid = {24626916},
publisher = {American Association for the Advancement of Science},
title = {{The parable of google flu: Traps in big data analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24626916},
volume = {343},
year = {2014}
}
@article{Brown2016,
abstract = {To understand change in global biodiversity patterns requires large-scale, long-term monitoring. The ability to draw meaningful comparison across studies is severely hampered by extensive variation in the design of the sampling equipment and how it is used. Here, we present a meta-analysis and description highlighting this variation in a common, widely used entomological survey technique. We report a decline in the completeness of methodological reporting over a 20-year period, while there has been no clear reduction in the methodological variation between researchers using pitfall traps for arthropod sampling. There is a growing need for improved comparability between studies to facilitate the generation of large-scale, long-term biodiversity datasets. However, our results show that, counterproductive to this goal, over the last 20 years there has little progress in reducing the methodological variation. We propose a standardized pitfall trap design for the study of ground-active arthropods. In addition, we provide a table to promote a more standardized reporting of the key methodological variables. Widespread adoption of more standardized methods and reporting would facilitate more nuanced analysis of biodiversity change.},
author = {Brown, Grant R and Matthews, Iain M},
doi = {10.1002/ece3.2176},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Matthews - 2016 - A review of extensive variation in the design of pitfall traps and a proposal for a standard pitfall trap desig.pdf:pdf},
issn = {20457758},
journal = {Ecology and Evolution},
keywords = {Araneae,biodiversity sampling,carabidae,formicidae,pitfall trap,standard design},
month = {jun},
number = {12},
pages = {3953--3964},
pmid = {27247760},
publisher = {Wiley-Blackwell},
title = {{A review of extensive variation in the design of pitfall traps and a proposal for a standard pitfall trap design for monitoring ground-active arthropod biodiversity}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27247760 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4867678},
volume = {6},
year = {2016}
}
@article{Herndon2014,
abstract = {We replicate Reinhart and Rogoff (2010A and 2010B) and find that selective exclu- sion of available data, coding errors and inappropriate weighting of summary sta- tistics lead to serious miscalculations that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies. Over 1946– 2009, countries with public debt/GDP ratios above 90{\%} averaged 2.2{\%} real annual GDP growth, not −0.1{\%} as published. The published results for (i) median GDP growth rates for the 1946–2009 period and (ii) mean and median GDP growth figures over 1790–2009 are all distorted by similar methodological errors, although the magnitudes of the distortions are somewhat smaller than with the mean figures for 1946–2009. Contrary to Reinhart and Rogoff's broader contentions, both mean and median GDP growth when public debt levels exceed 90{\%} of GDP are not dramatically different from when the public debt/GDP ratios are lower. The rela- tionship between public debt and GDP growth varies significantly by period and country. Our overall evidence refutes RR's claim that public debt/GDP ratios above 90{\%} consistently reduce a country's GDP growth.},
author = {Herndon, T. and Ash, M. and Pollin, R.},
doi = {10.1093/cje/bet075},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Herndon, Ash, Pollin - 2014 - Does high public debt consistently stifle economic growth A critique of Reinhart and Rogoff.pdf:pdf},
issn = {0309-166X},
journal = {Cambridge Journal of Economics},
month = {mar},
number = {2},
pages = {257--279},
publisher = {Oxford University Press},
title = {{Does high public debt consistently stifle economic growth? A critique of Reinhart and Rogoff}},
url = {https://academic.oup.com/cje/article-lookup/doi/10.1093/cje/bet075},
volume = {38},
year = {2014}
}
@article{Bahill1998,
annote = {id: 1; issn: print 10946977; publication{\_}type: full{\_}text},
author = {Bahill, A T and Gissing, B},
doi = {10.1109/5326.725338},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bahill, Gissing - 1998 - Re-evaluating systems engineering concepts using systems thinking.pdf:pdf},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
number = {4},
pages = {516 {\textless}last{\_}page{\textgreater} 527},
title = {{Re-evaluating systems engineering concepts using systems thinking}},
url = {http://dx.doi.org/10.1109/5326.725338},
volume = {28},
year = {1998}
}
@article{Perer2006,
author = {Perer, Adam and Shneiderman, Ben},
doi = {10.1109/TVCG.2006.122},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Perer, Shneiderman - 2006 - Balancing Systematic and Flexible Exploration of Social Networks.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {sep},
number = {5},
pages = {693--700},
title = {{Balancing Systematic and Flexible Exploration of Social Networks}},
url = {http://ieeexplore.ieee.org/document/4015419/},
volume = {12},
year = {2006}
}
@article{Pierce2019,
abstract = {To promote effective sharing, we must create an enduring link between the people who generate data and its future uses, urge Heather H. Pierce and colleagues. {\textcopyright} 2019, Nature.},
author = {Pierce, Heather H. and Dev, Anurupa and Statham, Emily and Bierer, Barbara E.},
doi = {10.1038/d41586-019-01715-4},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pierce et al. - 2019 - Credit data generators for data reuse.pdf:pdf},
issn = {14764687},
journal = {Nature},
keywords = {Databases,Policy,Publishing,Research management},
month = {jun},
number = {7759},
pages = {30--32},
publisher = {Nature Publishing Group},
title = {{Credit data generators for data reuse}},
volume = {570},
year = {2019}
}
@article{Scheffer2015,
abstract = {Recent studies provide compelling evidence for the idea that creative thinking draws upon two kinds of processes linked to distinct physiological features, and stimulated under different conditions. In short, the fast system-I produces intuition whereas the slow and deliberate system-II produces reasoning. System-I can help see novel solutions and associations instantaneously, but is prone to error. System-II has other biases, but can help checking and modifying the system-I results. Although thinking is the core business of science, the accepted ways of doing our work focus almost entirely on facilitating system-II. We discuss the role of system-I thinking in past scientific breakthroughs, and argue that scientific progress may be catalyzed by creating conditions for such associative intuitive thinking in our academic lives and in education. Unstructured socializing time, education for daring exploration, and cooperation with the arts are among the potential elements. Because such activities may be looked upon as procrastination rather than work, deliberate effort is needed to counteract our systematic bias.},
author = {Scheffer, Marten and Bascompte, Jordi and Bjordam, Tone K. and Carpenter, Stephen R. and Clarke, Laurie B. and Folke, Carl and Marquet, Pablo and Mazzeo, Nestor and Meerhoff, Mariana and Sala, Osvaldo and Westley, Frances R.},
doi = {10.5751/ES-07434-200203},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Scheffer et al. - 2015 - Dual thinking for scientists.pdf:pdf},
issn = {17083087},
journal = {Ecology and Society},
month = {jun},
number = {2},
publisher = {Resilience Alliance},
title = {{Dual thinking for scientists}},
volume = {20},
year = {2015}
}
@article{OSBORNE1963,
abstract = {The structure of dichotomous keys is discussed, and some general theorems are proved concerning the number and arrangement of the taxa in any given key. The possibility is considered that the various questions in a key may occasionally be answered wrongly, the key then leading to a wrong determination. It is shown that if this possibility exists, then the probability of a right determination is almost always improved by changing the structure of the key to make it as ‘short'as possible. The possibility is also considered of adding alternative identification paths through the key (‘reticulations'), starting at points where a particularly difficult question is asked. It is shown that this practice usually improves the key in respect of the determination of certain taxa, but may often diminish the overall probability of correct determination averaged over all taxa. Copyright {\textcopyright} 1963, Wiley Blackwell. All rights reserved},
author = {OSBORNE, DONALD V.},
doi = {10.1111/j.1469-8137.1963.tb06322.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/OSBORNE - 1963 - SOME ASPECTS OF THE THEORY OF DICHOTOMOUS KEYS.pdf:pdf},
issn = {14698137},
journal = {New Phytologist},
month = {aug},
number = {2},
pages = {144--160},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{SOME ASPECTS OF THE THEORY OF DICHOTOMOUS KEYS}},
volume = {62},
year = {1963}
}
@article{Wirth1997,
abstract = {Knowledge Discovery in Databases (KDD) is currently a hot topic in industry and academia. Although KDD is now widely accepted as a complex process of many different phases, the focus of research behind most emerging products is on underlying algorithms and modelling techniques. The main bottleneck for KDD applications is not the lack of techniques. The challenge is to exploit and combine existing algorithms effectively, and help the user during all phases of the KDD process. In this paper, we describe the project CITRUS which addresses these practically relevant issues. Starting from a commercially available system, we develop a scaleable, extensible tool inherently based on the view of KDD as an interactive and iterative process. We sketch the main components of this system, namely an information manager for effective retrieval of data and results, an execution server for efficient execution, and a process support interface for guiding the user through the process.},
author = {Wirth, R{\"{u}}diger and Shearer, Colin and Grimmer, Udo and Reinartz, Thomas and Schlosser, Jorg and Breitner, Christoph and Engels, Robert and Lindner, Guido},
doi = {10.1007/3-540-63223-9_123},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wirth et al. - 1997 - Towards process-oriented tool support for knowledge discovery in databases.pdf:pdf},
isbn = {3-540-63223-9},
journal = {First European Symposium, PKDD '97 Trondheim, Norway, June 24–27, 1997 Proceedings},
pages = {243--253},
title = {{Towards process-oriented tool support for knowledge discovery in databases}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F3-540-63223-9{\_}123.pdf},
year = {1997}
}
@article{Jacobs2009,
abstract = {Communications of the ACM - A Blind Person's Interaction with Technology},
author = {Jacobs, Adam},
doi = {10.1145/1563821.1563874},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jacobs - 2009 - The Pathologies of Big Data.pdf:pdf},
isbn = {00010782},
issn = {15427730},
journal = {Queue},
number = {6},
pages = {10},
pmid = {43479959},
title = {{The Pathologies of Big Data}},
url = {http://delivery.acm.org/10.1145/1570000/1563874/p10-jacobs.pdf?ip=158.125.51.236{\&}id=1563874{\&}acc=OPEN{\&}key=BF07A2EE685417C5.F25E547D993D41C5.4D4702B0C3E38B35.6D218144511F3437{\&}{\_}{\_}acm{\_}{\_}=1531161127{\_}ec096682960b4740b7bf9d8b4308f477 http://portal.acm.org/citation},
volume = {7},
year = {2009}
}
@article{Boyd2012,
abstract = {The era of Big Data has begun. Computer scientists, physicists, economists, mathematicians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring for access to the massive quantities of information produced by and about people, things, and their interactions. Diverse groups argue about the potential benefits and costs of analyzing genetic sequences, social media interactions, health records, phone logs, government records, and other digital traces left by people. Significant questions emerge. Will large-scale search data help us create better tools, services, and public goods? Or will it usher in a new wave of privacy incursions and invasive marketing? Will data analytics help us understand online communities and political movements? Or will it be used to track protesters and suppress speech? Will it transform how we study human communication and culture, or narrow the palette of research options and alter what ‘research' means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it is necessary to critically interrogate its assumptions and biases. In this article, we offer six provocations to spark conversations about the issues of Big Data: a cultural, technological, and scholarly phenomenon that rests on the interplay of technology, analysis, and mythology that provokes extensive utopian and dystopian rhetoric.},
archivePrefix = {arXiv},
arxivId = {4},
author = {Boyd, Danah and Crawford, Kate},
doi = {10.1080/1369118X.2012.678878},
eprint = {4},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Boyd, Crawford - 2014 - CRITICAL QUESTIONS FOR BIG DATA(2).pdf:pdf},
isbn = {0010414010},
issn = {1369-118X},
journal = {Information, Communication {\&} Society},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jun},
number = {5},
pages = {662--679},
pmid = {199800104512001},
title = {{CRITICAL QUESTIONS FOR BIG DATA}},
url = {http://www.tandfonline.com/loi/rics20http://dx.doi.org/10.1080/1369118X.2012.678878 http://www.tandfonline.com/loi/rics20{\%}5Cnhttp://dx.doi.org/10.1080/1369118X.2012.678878 http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.678878},
volume = {15},
year = {2014}
}
@article{Ribeiro1999,
abstract = {An annotated checklist of the mosquitoes of Continental Portugal was published by Ribeiro et al. (1988), recording 40 species and subspecies belonging to 7 genera.},
author = {Ribeiro, Henrique and Ramos, Helena Cunha},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Ramos - 1999 - European Mosquito Bulletin.pdf:pdf},
journal = {Journal of the European Mosquito Control Association},
number = {3},
pages = {1--11},
title = {{European Mosquito Bulletin}},
url = {http://www.e-m-b.org/sites/e-m-b.org/files/European{\_}Mosquito{\_}Bulletin{\_}Publications811/EMB03/EMB03{\_}01.pdf},
volume = {3},
year = {1999}
}
@techreport{IntelITCenter2014,
abstract = {This report describes key findings from a survey of 200 IT professionals about big data analytics that can help you plan your own projects, as well as a perspective on what these results mean for the IT industry.},
author = {{Intel IT Center}},
booktitle = {Intel IT Center},
doi = {10.1007/978-3-319-10665-6},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Intel IT Center - 2014 - Intel's 2014 IT Manager Survey on How Organizations Are Using Big data.pdf:pdf},
institution = {Intel IT Center},
isbn = {9783319106649},
number = {August 2012},
pages = {27},
title = {{Intel's 2014 IT Manager Survey on How Organizations Are Using Big data}},
url = {https://www.intel.com/content/dam/www/public/us/en/documents/reports/data-insights-peer-research-report.pdf http://public.dhe.ibm.com/common/ssi/ecm/en/iml14293usen/IML14293USEN.PDF},
year = {2014}
}
@article{Smith,
abstract = {Up to this point, our analysis of point patterns has focused on single point patterns, such as the locations of redwood seedlings or lung cancer cases. But often the relevant questions of interest involve relationships between more than one pattern. For example if one considers a forest in which redwoods are found, there will invariably be other species competing with redwoods for nourishment and sunlight. Hence this competition between species may be of primary interest. In the case of lung cancers, recall from Section 1.2 that the lung cancer data for Lancashire was primarily of interest as a reference population for studying the smaller pattern of larynx cancers. We shall return to this example in Section 5.8 below. But for the moment we start with a simple forest example involving two species. 5.1 Forest Example The 600 foot square section of forest shown in Figure 5.1 below contains only two types of trees. The large dots represent the locations of oak trees, and the small dots represent locations of maple trees. Although this is a fairly small section of forest, it seems clear that the pattern of oaks is much more clustered than that of maples. This is not surprising, given the very different seed-dispersal patterns of these two types of trees. As shown in Figure 5.2, oaks produce largest acorns that fall directly from the tree, and are only partially dispersed by squirrels. Maples on the other hand produce seeds with individual "wings" that can transport each seed a considerable distance with even the slightest breeze. Hence there are clear biological reasons why the distribution of oaks might be more clustered than that of maples. So how might we test this hypothesis statistically? 0},
author = {Smith, Tony},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Smith - Unknown - NOTEBOOK FOR SPATIAL DATA ANALYSIS Part I. Spatial Point Pattern Analysis.pdf:pdf},
journal = {Analysis},
title = {{NOTEBOOK FOR SPATIAL DATA ANALYSIS : Part I. Spatial Point Pattern Analysis}},
url = {https://www.seas.upenn.edu/{~}ese502/NOTEBOOK/Part{\_}I/5{\_}Comparative{\_}Analyses.pdf}
}
@misc{RStudioTeam2016,
address = {Boston, MA},
author = {{RStudio Team}},
publisher = {RStudio, Inc.},
title = {{RStudio}},
url = {http://www.rstudio.com/},
year = {2016}
}
@article{Soga2016,
abstract = {Today's children have less direct contact with nature than ever before, resulting in an “extinction of experience”. Research has suggested that such loss of daily interactions decreases people's appreciation of the natural world, but this remains quantitatively unexplored. We conducted a questionnaire survey of undergraduate university students in Tokyo, Japan, and determined the effects of frequency of contact with nature on emotional connectedness to nature and perceptions of neighbourhood nature. A total of 255 students participated in the surveys. Students' perceptions of neighbourhood nature were measured by to what extent they valued cultural ecosystem services derived from neighbourhood natural environments, birds and butterflies. Results showed that students valued neighbourhood natural environments, birds and butterflies for many different reasons, such as relaxation, beauty of natural scenes, an indicator of seasonality, and opportunities for education. Linear mixed models revealed that both current and childhood frequencies of contact with nature were positively related not only to students' emotional connectedness to nature but also their perceptions of neighbourhood nature. Students' emotional connection to nature and perceptions of neighbourhood nature were positively associated with each other. Our results suggest that, given the rapid decrease in children's daily contact with nature, public appreciation of the value of the natural world is likely gradually also to decrease. This can be a major obstacle to reversing global environmental challenges. People should therefore be encouraged to experience neighbourhood natural environments and biodiversity, and city planners and policy makers will play a vital role in connecting people with nature.},
author = {Soga, Masashi and Gaston, Kevin J. and Koyanagi, Tomoyo F. and Kurisu, Kiyo and Hanaki, Keisuke},
doi = {10.1016/j.biocon.2016.09.020},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Soga et al. - 2016 - Urban residents' perceptions of neighbourhood nature Does the extinction of experience matter.pdf:pdf},
issn = {00063207},
journal = {Biological Conservation},
keywords = {Conservation psychology,Ecosystem services,Human health,Nature experiences,Psychological wellbeing,Urban ecology},
month = {nov},
pages = {143--150},
publisher = {Elsevier Ltd},
title = {{Urban residents' perceptions of neighbourhood nature: Does the extinction of experience matter?}},
volume = {203},
year = {2016}
}
@misc{Rathbone2017,
abstract = {Observation of the public interest in events that offer access to working archaeological digs.},
author = {Rathbone, Neil},
pages = {1},
title = {{Personal Communication}},
year = {2017}
}
@article{Fibiger2005,
abstract = {The higher classification of the Noctuoidea (Oenosandridae, Doidae, Notodontidae, Strepsimanidae, Nolidae, Lymantriidae, Arctiidae, Erebidae, Micronoctuidae, and Noctuidae) is reviewed from the perspective of the classification proposed by KITCHING and RAWLINS (1998). Several taxa are reinstated, described as new, synonymised, or redescribed. Some characters that have been inadequately described, poorly understood, or misinterpreted, are redescribed and discussed. One family, two subfamilies, four tribes, and three subtribes are proposed as new. Available family-group names of Noctuoidea are listed in an appendix.},
author = {Fibiger, Michael and Lafontaine, J Donald},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fibiger, Lafontaine - 2005 - A review of the higher classification of the Noctuoidea (Lepidoptera) with special reference to the Holarct.pdf:pdf},
isbn = {3-938249-01-3},
journal = {Esperiana Buchreihe zur Entomologie Bd},
pages = {29},
title = {{A review of the higher classification of the Noctuoidea (Lepidoptera) with special reference to the Holarctic fauna}},
volume = {11},
year = {2005}
}
@misc{Akiwatkar2017,
abstract = {I assume that 99.9{\%} of the people reading this article know data is — a collection of facts, statistics, and information of a given subject is known as data, Right? But what about data science? Data science is a “concept to unify statistics, data analysis and their related methods” in order to “understand and analyze actual phenomena” with data. It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information science, and computer science. This field is becoming more and more popular with the advent of Machine Learning all around us. In order to understand data science, we must know at least one of the programming languages. Lucky for you, that are many programming languages that are used for data science.},
author = {Akiwatkar, Rohit},
booktitle = {DZone},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Akiwatkar - 2017 - The Most Popular Languages for Data Science - DZone Big Data.pdf:pdf;:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Akiwatkar - 2017 - The Most Popular Languages for Data Science - DZone Big Data.jpeg:jpeg},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {2},
title = {{The Most Popular Languages for Data Science - DZone Big Data}},
url = {https://dzone.com/articles/which-are-the-popular-languages-for-data-science},
urldate = {2018-05-15},
year = {2017}
}
@article{Hart2008,
abstract = {We argue that a contract provides a reference point for a trading relationship: more precisely, for parties' feelings of entitlement. A party's ex post performance depends on whether he gets what he is entitled to relative to outcomes permitted by the contract. A party who is shortchanged shades on performance. A flexible contract allows parties to adjust their outcome to uncertainty, but causes inefficient shading. Our analysis provides a basis for long-term contracts in the absence of noncontractible investments, and elucidates why "employment" contracts, which fix wage in advance and allow the employer to choose the task, can be optimal.},
author = {Hart, O and {J. Moore}},
doi = {10.3386/w12706},
journal = {Quarterly Journal of Economics},
keywords = {contracts},
number = {1},
pages = {1--48},
title = {{ Contracts as reference points}},
volume = {123},
year = {2008}
}
@misc{Zanin2016,
abstract = {The increasing power of computer technology does not dispense with the need to extract meaningful information out of data sets of ever growing size, and indeed typically exacerbates the complexity of this task. To tackle this general problem, two methods have emerged, at chronologically different times, that are now commonly used in the scientific community: data mining and complex network theory. Not only do complex network analysis and data mining share the same general goal, that of extracting information from complex systems to ultimately create a new compact quantifiable representation, but they also often address similar problems too. In the face of that, a surprisingly low number of researchers turn out to resort to both methodologies. One may then be tempted to conclude that these two fields are either largely redundant or totally antithetic. The starting point of this review is that this state of affairs should be put down to contingent rather than conceptual differences, and that these two fields can in fact advantageously be used in a synergistic manner. An overview of both fields is first provided, some fundamental concepts of which are illustrated. A variety of contexts in which complex network theory and data mining have been used in a synergistic manner are then presented. Contexts in which the appropriate integration of complex network metrics can lead to improved classification rates with respect to classical data mining algorithms and, conversely, contexts in which data mining can be used to tackle important issues in complex network theory applications are illustrated. Finally, ways to achieve a tighter integration between complex networks and data mining, and open lines of research are discussed.},
archivePrefix = {arXiv},
arxivId = {1604.08816},
author = {Zanin, M. and Papo, D. and Sousa, P. A. and Menasalvas, E. and Nicchi, A. and Kubik, E. and Boccaletti, S.},
booktitle = {Physics Reports},
doi = {10.1016/j.physrep.2016.04.005},
eprint = {1604.08816},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zanin et al. - 2016 - Combining complex networks and data mining Why and how.pdf:pdf},
isbn = {0370-1573},
issn = {03701573},
keywords = {Big Data,Complex networks,Data mining},
month = {may},
pages = {1--44},
publisher = {North-Holland},
title = {{Combining complex networks and data mining: Why and how}},
volume = {635},
year = {2016}
}
@techreport{MayorofLondon2017,
abstract = {Part of the Greater London Authority family of organisations led by Mayor of London Sadiq Khan, we are the integrated transport authority responsible for delivering the Mayor's strategy and commitments on transport.},
address = {London, UK},
author = {{Mayor of London}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mayor of London - 2017 - Review of the TfL WiFi pilot Our findings.pdf:pdf},
institution = {Transport for London},
pages = {24},
title = {{Review of the TfL WiFi pilot: Our findings}},
url = {http://content.tfl.gov.uk/review-tfl-wifi-pilot.pdf},
year = {2017}
}
@article{Berlin1980,
abstract = {The importance of euparal as a permanent mounting medium for eggs of parasites and the process of making permanent slides are analyzed and discussed in detail. Studies show that helminth eggs mounted in euparal exhibit excellent optical and drying properties. Euparal also can be used to identify proglottids of Taenia species and to examine for the presence of Trichinella spiralis in suspicious muscle biopsies.},
author = {Berlin, O G.W. and Miller, M J},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Berlin, Miller - 1980 - Euparal as a permanent mounting medium for helminth eggs and proglottids.pdf:pdf},
issn = {00951137},
journal = {Journal of Clinical Microbiology},
month = {nov},
number = {5},
pages = {700--703},
pmid = {7196920},
publisher = {American Society for Microbiology Journals},
title = {{Euparal as a permanent mounting medium for helminth eggs and proglottids}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7196920 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC273675},
volume = {12},
year = {1980}
}
@inproceedings{Caruana2015,
abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
doi = {10.1145/2783258.2788613},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Caruana et al. - 2015 - Intelligible Models for HealthCare.pdf:pdf},
isbn = {9781450336642},
keywords = {I26 [Computing Methodologies]: Learning-Induction,additive models,classification,healthcare,interaction detection,logistic regression,risk prediction},
pages = {1721--1730},
title = {{Intelligible Models for HealthCare}},
url = {http://dx.doi.org/10.1145/2783258.2788613 http://dl.acm.org/citation.cfm?doid=2783258.2788613},
year = {2015}
}
@article{Katz1997,
abstract = {Advanced scientometric tools are moving from the realm of the privileged few with access to mainframe and minicomputers to the desktop of researchers equipped with personal computers. This shift is not only due to the decreasing cost and technological advances in PCs but the ready availability of a powerful multitasking operating system, a versatile text processing language and easy access to the Internet. Furthermore, the latest releases of PC software, such as Microsoft Excel, make it possible to develop graphical user interfaces into complex bibliometric data for a wide spectrum of researchers and policy analysts. Recent developments in digital communication, in particular, tools to access the Internet via the World Wide Web will provide even greater flexibility to those researchers wishing to make their scientometric data available to a diverse international audience. This paper examines how the BESST project developed a Desktop Scientometric environment using public domain, hardware independent software, prototyped a graphical user interface to provide easy access to UK sectoral level bibliometric data and gives a glimpse into future developments.},
author = {Katz, J. S. and Hicks, Diana},
doi = {10.1007/BF02461128},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Katz, Hicks - 1997 - Desktop scientometrics.pdf:pdf},
issn = {0138-9130},
journal = {Scientometrics},
keywords = {Desktop Techniques},
mendeley-tags = {Desktop Techniques},
month = {jan},
number = {1},
pages = {141--153},
publisher = {Kluwer Academic Publishers},
title = {{Desktop scientometrics}},
url = {http://link.springer.com/10.1007/BF02461128},
volume = {38},
year = {1997}
}
@misc{Stadler2014,
abstract = {Joining data tables with R!},
author = {Stadler, Ronald},
booktitle = {R-Studio},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Stadler - 2014 - JOINing data in R using data.table.pdf:pdf},
keywords = {R},
mendeley-tags = {R},
pages = {9},
title = {{JOINing data in R using data.table}},
url = {https://rstudio-pubs-static.s3.amazonaws.com/52230{\_}5ae0d25125b544caab32f75f0360e775.html},
urldate = {2019-02-21},
year = {2014}
}
@misc{Eppler,
author = {Eppler, Martin},
title = {{Visual Literacy}},
url = {https://www.visual-literacy.org/},
urldate = {2020-06-28},
year = {2020}
}
@article{Dias2017,
abstract = {Formal concept analysis (FCA) is a mathematical theory of data analysis with applications in many areas. The problem of obtaining a concept lattice of an appropriate size was identified in several applications as one of the most important problems of FCA. In order to deal with this problem several techniques with different characteristics were proposed for concept lattice reduction. However, there are currently no adequate methods to assess what types of knowledge transformations can result from a reduction. A methodology for analysis of concept lattice reduction is presented here. It is based on the use of sets of proper implications holding in the original and reduced formal contexts or concept lattices. Working with both sets of implications, the methodology is able to show what is preserved, eliminated, inserted or transformed by a reduction technique. Three classes of reduction techniques are analyzed from the standpoint of the methodology in order to highlight techniques of each class have in common with respect to the transformations performed. Such analysis is followed by specific examples in each class.},
author = {Dias, S{\'{e}}rgio M. and Vieira, Newton J.},
doi = {10.1016/j.ins.2017.02.037},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dias, Vieira - 2017 - A methodology for analysis of concept lattice reduction.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Formal concept analysis,Lattice reduction,Proper implications},
month = {aug},
pages = {202--217},
publisher = {Elsevier},
title = {{A methodology for analysis of concept lattice reduction}},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517305388},
volume = {396},
year = {2017}
}
@inproceedings{palmer2004aprocedure,
author = {Palmer, P J and Mason, L A and Dunn, M},
doi = {10.1115/ESDA2004-58349},
isbn = {0-7918-3741-6},
month = {jul},
organization = {Manchester, United Kingdom},
pages = {469--473},
title = {{A Case Study in Healthcare Quality Management: A Practical Methodology for Auditing Total Patient X-ray Dose During a Diagnostic Procedure'}},
volume = {3},
year = {2004}
}
@article{Huang1998,
abstract = {The k-means algorithm is well known for its efficiency in clustering large data sets. However, working only on numeric values prohibits it from being used to cluster real world data containing categorical values. In this paper we present two algorithms which extend the k-means algorithm to categorical domains and domains with mixed numeric and categorical values. The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function. With these extensions the k-modes algorithm enables the clustering of categorical data in a fashion similar to k-means. The k-prototypes algorithm, through the definition of a combined dissimilarity measure, further integrates the k-means and k-modes algorithms to allow for clustering objects described by mixed numeric and categorical attributes. We use the well known soybean disease and credit approval data sets to demonstrate the clustering performance of the two algorithms. Our experiments on two real world data sets with half a million objects each show that the two algorithms are efficient when clustering large data sets, which is critical to data mining applications.},
author = {Huang, Zhexue},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Huang - 1998 - Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values.pdf:pdf},
journal = {Data Mining and Knowledge Discovery},
keywords = {categorical data,cluster analysis,clustering algorithms,data mining},
pages = {283--304},
title = {{Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values}},
volume = {12},
year = {1998}
}
@article{Gurstein2002,
abstract = {Title from PDF title page (viewed Apr. 18, 2005). Some issues have a distinctive title.},
author = {Gurstein, Michael B and Day, Peter and Menou, Michel J and Taylor, Wallace},
doi = {10.1016/S0736-5853(00)00030-7},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gurstein et al. - 2002 - Community Informatics Enabling Communities with Information and Communication Technologies.pdf:pdf},
issn = {07365853},
journal = {Telematics and Informatics},
keywords = {Big Data,Data Literacy,Data visualization,Design,Journalism,Learning,Popular Education},
mendeley-tags = {Big Data},
number = {3},
pages = {61--62},
publisher = {Community Informatics Research Network},
title = {{Community Informatics: Enabling Communities with Information and Communication Technologies}},
url = {http://ci-journal.net/index.php/ciej/article/view/1294 http://linkinghub.elsevier.com/retrieve/pii/S0736585300000307},
volume = {19},
year = {2002}
}
@article{FOX2013,
abstract = {Abstract.  1. Population declines among insects are inadequately quantified, yet of vital importance to national and global biodiversity assessments and have significant implications for ecosystem services. 2. Substantial declines in abundance and distribution have been reported recently within a species-rich insect taxon, macro-moths, in Great Britain and other European countries. These declines are of concern because moths are important primary consumers and prey items for a wide range of other taxa, as well as contributing to ecosystem services such as pollination. 3. I summarise these declines and review potential drivers of change. Direct evidence for causes of moth declines is extremely limited, but correlative studies and extrapolation from closely related taxa suggest that habitat degradation (particularly because of agricultural intensification and changing silviculture) and climate change are likely to be major drivers. There is currently little evidence of negative population-level effects on moths caused by chemical or light pollution, non-native species or direct exploitation. 4. I make suggestions for future research with a focus on quantifying impacts of land management practices, light pollution and climate change on moth population dynamics and developing evidence-based measures that can be incorporated into agri-environment schemes and other policy initiatives to help reverse the widespread decline of moths in Great Britain and beyond.},
author = {FOX, RICHARD},
doi = {10.1111/j.1752-4598.2012.00186.x},
issn = {1752-4598},
journal = {Insect Conservation and Diversity},
keywords = {Biodiversity conservation,climate change,habitat degradation,light pollution,macro-moths,population trends},
number = {1},
pages = {5--19},
publisher = {Blackwell Publishing Ltd},
title = {{The decline of moths in Great Britain: a review of possible causes}},
url = {http://dx.doi.org/10.1111/j.1752-4598.2012.00186.x},
volume = {6},
year = {2013}
}
@techreport{Petrissans2012,
abstract = {EXECUTIVE SUMMARY The scope of this study is the evolution of embedded systems design (ESD)1 toward systems of systems (SoS), with a specific focus on industry viewpoints and emerging opportunities to improve European competitiveness. The study has a double focus: supporting the preparation of Horizon 2020 for the embedded and complex systems area, through identification of the main scientific and research challenges, and analyzing the emerging SoS market and relative scientific and research challenges.  The study analyzes emerging trends and drivers as well as the main technical problems in the evolution of the ESD and engineering field toward systems of systems. The proliferation of big data bases mainly driven through the Internet is considered one of the main drivers. Also, we will see the proliferation with ubiquitous connectivity of client devices, systems, and subsystems that are able to exchange information. By 2020 there will be 31 billion intelligent devices connected in one way or another through wireless and fixed broadband networks. At system level, innovation is expected through sophisticated functionality and novel cloud-based services that will lead to more heterogeneous and distributed architectures, and novel techniques for data aggregation and control, all of which call for novel methodology for future cloud-based dependable systems.  The study has identified four technology areas of importance:    System interaction with the Web. We can expect the emergence of ubiquitous and personalized access in real time to worldwide information from cars, phones, and homes. The trend in embedded systems toward the combination of systems with network and Internet services and global databases offers more and increasingly innovative functionality but considerably increases the complexity of system design.    New computing paradigms. Multicore technologies are becoming a pervasive technology in mobile devices, automotive, aerospace, etc. Innovation in embedded systems will increasingly demand high-performance computing in order to offer new functionality and services. New research challenges evolve through mixed-criticality systems from the integration of safety and time-critical functionalities with infotainment services.    New ways of interaction and cooperation with complex data through visualization and smart data analysis. SoS will drive new technologies and require more interoperability between humans and embedded systems. The role of human machine interface will be central.    Critical requirements. Security and safety are a high priority, and a world where all devices are interconnected must ensure confidentiality of data dependability of the system and trust in the connectivity chain. Driving with a Google car, the system autonomy, security, and safety become the top priority. A fundamentally new approach would be needed for the design of future cloud-based dependable systems.  The study suggests a new design methodology for systems of systems with possible emerging behavior and their control in respect to high availability, performance, safety, and security. A radical change from today's system design is expected through cloud-based challenges when combining bought-in, built-in, beamed-in functionality and services.  A future driven by societal challenges in mobility, energy and healthcare, for example, demands a new generation of systems with increasing scale and complexity towards systems-of-systems. Europe will have major opportunities through its existing and strong large equipment manufacturer base in the automotive and aerospace sectors, but will benefit from new opportunities in emerging markets such as electricity/gas/water utilities and smart energy. There is a great opportunity for Europe to become a leader in innovation and research in the area of engineering of systems of systems, mixed criticality systems, and smart user interfaces, not only in automotive and aerospace but also in other areas where robustness, availability, safety, and security are important requirements for embedded and complex system design.},
annote = {Report in T-AREA-SoS. 

Good document; useful. Prepared for the EU Commission, but no info regarding project number.

Embedded systems are those that are packaged as a complete application, which can be leased, bought, etc. and can be included in a system.

See fig 3 for a diagram of the Internet of Things.},
author = {P{\'{e}}trissans, A and Krawczyk, S and Veronesi, L and Cattaneo, G and Feeney, N and Meunier, C},
keywords = {Governance,agents,agility,architecture,automation,complexity,federated control,networks,resilience,robots,robustness,safety,security,software,strategy,supply chain},
title = {{Design of future Embedded Systems toward system of systems - trends and challenges}},
year = {2012}
}
@inproceedings{palmer1999aeffort,
author = {Palmer, P J and Williams, D J},
booktitle = {Proceedings of the Twenty-Fourth IEEE/CPMT International Electronics Manufacturing Technology (IEMT) Symposium},
issn = {1089-8190},
month = {oct},
organization = {Austin, Texas},
pages = {170--177},
publisher = {IEEE Catalog No 99CH36330},
title = {{A Model of the Distribution of Interconnectivity Through Multiple System Levels and the Impact of Different Design Stratagies and IP re-Use of Design Effort}},
year = {1999}
}
@misc{OEDOnline.2020,
abstract = {Data, n.},
address = {Oxford, UK},
author = {{OED Online.}},
publisher = {Oxford University Press (OUP)},
title = {{Oxford English Dictionary}},
url = {https://www.oed.com/view/Entry/296948},
urldate = {2020-09-08},
year = {2020}
}
@article{P.2016,
abstract = {Abstract—A huge repository of terabytes of data is generated each day from modern information systems and digital technolo - gies such as Internet of Things and cloud computing . Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making . Therefore , big data analysis is a current area of research and development . The basic objective of this paper is to explore the potential impact of big data challenges , open research issues , and various tools associated with it . As a result , this article provides a platform to explore big data at numerous stages . Additionally , it opens a new horizon for researchers to develop the solution , based on the challenges and open research issues .},
author = {P., D. and Ahmed, Kauser},
doi = {10.14569/IJACSA.2016.070267},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/P., Ahmed - 2016 - A Survey on Big Data Analytics Challenges, Open Research Issues and Tools.pdf:pdf},
isbn = {2158-107X},
issn = {21565570},
journal = {International Journal of Advanced Computer Science and Applications},
keywords = {Big data analytics,Hadoop,Massive data,Struc-tured data,Unstructured Data},
number = {2},
pages = {511--518},
title = {{A Survey on Big Data Analytics: Challenges, Open Research Issues and Tools}},
url = {https://www.researchgate.net/publication/296550027 http://thesai.org/Publications/ViewPaper?Volume=7{\&}Issue=2{\&}Code=ijacsa{\&}SerialNo=67},
volume = {7},
year = {2016}
}
@inproceedings{Khan2018,
abstract = {{\textcopyright} 2018 Association for Computing Machinery. In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.},
address = {New York, New York, USA},
author = {Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3206157.3206166},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Khan et al. - 2018 - The 10 Vs, issues and challenges of big data.pdf:pdf},
isbn = {9781450363587},
keywords = {Big Data,Data management},
pages = {52--56},
publisher = {ACM Press},
title = {{The 10 Vs, issues and challenges of big data}},
url = {http://dl.acm.org/citation.cfm?doid=3206157.3206166},
year = {2018}
}
@article{Yu2017a,
abstract = {—Complementary to the fancy big data applications, networking for big data is an indispensable supporting platform for these applications in practice. This emerging research branch has gained extensive attention from both academia and indus-try in recent years. In this new territory, researchers are facing many unprecedented theoretical and practical challenges. We are therefore motivated to solicit the latest works in this area, aiming to pave a comprehensive and solid starting ground for interested readers. We first clarify the definition of networking for big data based on the cross disciplinary nature and inte-grated needs of the domain. Second, we present the current understanding of big data from different levels, including its formation, networking features, mathematical representations, and the networking technologies. Third, we discuss the chal-lenges and opportunities from various perspectives in this hopeful field. We further summarize the lessons we learned based on the survey. We humbly hope this paper will shed light for forth-coming researchers to further explore the uncharted part of this promising land.},
author = {Yu, Shui and Liu, Meng and Dou, Wanchun and Liu, Xiting and Zhou, Sanming},
doi = {10.1109/COMST.2016.2610963},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2017 - Networking for Big Data A Survey.pdf:pdf},
isbn = {1482263491},
issn = {1553-877X},
journal = {IEEE Communications Surveys {\&} Tutorials},
number = {1},
pages = {531--549},
title = {{Networking for Big Data: A Survey}},
url = {http://ieeexplore.ieee.org/document/7571188/},
volume = {19},
year = {2017}
}
@article{Hashem2015,
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized. {\textcopyright} 2014 Elsevier Ltd.},
annote = {Really good source of follow on references.
Big Data Technology.},
author = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
doi = {10.1016/j.is.2014.07.006},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hashem et al. - 2015 - The rise of big data on cloud computing Review and open research issues.pdf:pdf},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Big data,Cloud computing,Hadoop},
month = {jan},
pages = {98--115},
pmid = {1476196123},
publisher = {Pergamon},
title = {{The rise of "big data" on cloud computing: Review and open research issues}},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001288?via{\%}3Dihub},
volume = {47},
year = {2015}
}
@misc{PalmerPaul;HenshawMichael;SinclairMurray;Siemieniuch2017,
abstract = {These diagrams were created using the Gephi network analysis tool. Each node represents and gap that needs addressing to enable the successful implementation of cyber physical systems. The nodes are classified and coloured by the NCOIC infrastructure scale with the highest level towards the top of the diagram. 72 EU projects addressing gaps are also included Each link or edge represents a dependency upon another gap or a gap addressed by a project. The diagram is produced by randomly arranging the nodes and then using an elastic force algorithm to rearrange the nodes. The final placement brings nodes with many links closer together. To interpret the diagram each node is highlighted in turn and its connected neighbours highlighted. This uncovers relationships between gaps and projects that are otherwise hard to see. The diagram is saved with the highlighted node number in the file name.},
author = {{Palmer, Paul; Henshaw, Michael; Sinclair, Murray; Siemieniuch}, Carys (2017)},
doi = {10.17028/RD.LBORO.5082067},
month = {jan},
title = {{Networks of Cyber Physical System (CPS) Gaps}},
year = {2017}
}
@article{Shimadzu2016,
abstract = {Every ecological data set is the result of sampling the biota at sampling locations. Such samples are rarely a census of the biota at the sampling locations and so will inherently contain biases. It is crucial to account for the bias induced by sampling if valid inference on biodiversity quantities is to be drawn from the observed data. The literature on accounting for sampling effects is large, but most are dedicated to the specific type of inference required, the type of analysis performed and the type of survey undertaken. There is no general and systematic approach to sampling. Here, we explore the unification of modelling approaches to account for sampling. We focus on individuals in ecological communities as the fundamental sampling element, and show that methods for accounting for sampling at the species level can be equated to individual sampling effects. Particular emphasis is given to the case where the probability of observing an individual, when it is present at the site sampled, is less than one. We call these situations ‘imperfect observations'. The proposed framework is easily implemented in standard software packages. We highlight some practical benefits of this formal framework: the ability of predicting the true number of individuals using an expectation that conditions on the observed data, and designing appropriate survey plans accounting for uncertainty due to sampling. The principles and methods are illustrated with marine survey data from tropical northern Australia.},
author = {Shimadzu, Hideyasu and Foster, Scott D. and Darnell, Ross},
doi = {10.1007/s10651-016-0342-2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shimadzu, Foster, Darnell - 2016 - Imperfect observations in ecological studies.pdf:pdf},
issn = {15733009},
journal = {Environmental and Ecological Statistics},
keywords = {Compound distributions,Detection probability,Ecological modelling,Marine surveys,Sampling,Species distribution models (SDMs)},
month = {sep},
number = {3},
pages = {337--358},
title = {{Imperfect observations in ecological studies}},
url = {http://link.springer.com/10.1007/s10651-016-0342-2},
volume = {23},
year = {2016}
}
@article{Bribiesca1997,
abstract = {A new perimeter for shapes composed of cells is defined. This perimeter is called the contact perimeter, which corresponds to the sum of the boundaries of neighboring cells of the shape. Also, a relation between the perimeter of the shape and the contact perimeter is presented. The contact perimeter corresponds to the measure of compactness proposed here called discrete compactness. In this case, the term compactness does not refer to point-set topology, but is related to intrinsic properties of objects.},
author = {Bribiesca, E.},
doi = {10.1016/S0898-1221(97)00082-5},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bribiesca - 1997 - Measuring 2-D shape compactness using the contact perimeter.pdf:pdf},
issn = {08981221},
journal = {Computers and Mathematics with Applications},
keywords = {Contact perimeter,Discrete compactness,Measure of compactness,Shape analysis,Shape classification,Shape properties},
number = {11},
pages = {1--9},
publisher = {Elsevier Ltd},
title = {{Measuring 2-D shape compactness using the contact perimeter}},
volume = {33},
year = {1997}
}
@article{Lewis2013,
abstract = {Massive datasets of communication are challenging traditional, human-driven approaches to content analysis. Computational methods present enticing solutions to these problems but in many cases are insufficient on their own. We argue that an approach blending computational and manual methods throughout the content analysis process may yield more fruitful results, and draw on a case study of news sourcing on Twitter to illustrate this hybrid approach in action. Careful combinations of computational and manual techniques can preserve the strengths of traditional content analysis, with its systematic rigor and contextual sensitivity, while also maximizing the large-scale capacity of Big Data and the algorithmic accuracy of computational methods. Massive datasets of communication are challenging traditional, human-driven approaches to content analysis. Computational methods present enticing solutions to these problems but in many cases are insufficient on their own. We argue that an approach blending computational and manual methods throughout the content analysis process may yield more fruitful results, and draw on a case study of news sourcing on Twitter to illustrate this hybrid approach in action. Careful combinations of computational and manual techniques can preserve the strengths of traditional content analysis, with its systematic rigor and contextual sensitivity, while also maximizing the large-scale capacity of Big Data and the algorithmic accuracy of computational methods.},
annote = {This paper describes a VULTURE like appraoch to the organisation and analysis of a Big Data problem.},
author = {Lewis, Seth C and Zamith, Rodrigo and Hermida, Alfred},
doi = {10.1080/08838151.2012.761702},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lewis, Zamith, Hermida - 2013 - Content Analysis in an Era of Big Data A Hybrid Approach to Computational and Manual Methods.pdf:pdf},
isbn = {08838151},
issn = {0883-8151},
journal = {Journal of Broadcasting {\&} Electronic Media},
number = {1},
pages = {34--52},
title = {{Content Analysis in an Era of Big Data: A Hybrid Approach to Computational and Manual Methods}},
url = {http://www.tandfonline.com/action/journalInformation?journalCode=hbem20 http://www.tandfonline.com/doi/abs/10.1080/08838151.2012.761702},
volume = {57},
year = {2013}
}
@article{Mutanen2005,
abstract = {Genital characteristics tend to vary greatly between Lepidoptera species, providing helpful features for species delimitation. The differences between species are usually remarkable and suspicions about species identity never arise. However, fairly often, and possibly increasingly, taxa are elevated to species rank on the basis of very slight morphological differences, often without quantitative support. Euxoa tritici (Linnaeus) is a typical example of a variable species split into severalmorphologically similar species. The present study tested whether the diagnostic genital characters of the current classification, based on nonquantitativemethodology, provide safe identi- fication of species. Both traditional distance morphometrics as well as modern geo- metric morphometrics, which also enables quantitative shape exploration, were used. Moreover, whether the study specimens can be unambiguously categorized into several species with visual comparisons was tested independently using four specialist entomologists.Genital types of several named species aswell as considerable variation in genitalia were found, but no support was found for the presence of several morphologically distinguishable species with quantitative morphometric analyses. Neither were study specimens categorized unambiguously by specialists. The results suggest that pure visual comparisons may lead to unsound taxonomic conclusions and that a quantitative approach in critical cases should be used more frequently},
author = {Mutanen, Marko},
doi = {10.1111/j.1365-3113.2005.00296.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mutanen - 2005 - Delimitation difficulties in species splits A morphometric case study on the Euxoa tritici complex (Lepidoptera, Noctui.pdf:pdf},
isbn = {1365-3113},
issn = {03076970},
journal = {Systematic Entomology},
month = {oct},
number = {4},
pages = {632--643},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Delimitation difficulties in species splits: A morphometric case study on the Euxoa tritici complex (Lepidoptera, Noctuidae)}},
url = {http://doi.wiley.com/10.1111/j.1365-3113.2005.00296.x},
volume = {30},
year = {2005}
}
@techreport{ExGpRRI132013,
abstract = {Executive summary Research and Innovation are a key pillar in the strategy of the European Union to create sus- tainable, inclusive growth and prosperity and address the societal challenges of Europe and the world. The need to gear the innovation process to societal needs is reflected in many high-level policy, strategy and programming documents, such as the Europe 2020 strategy (2010) and the Horizon 2020 framework programme proposal (2011). Furthermore, for ex- ample the Lund Declaration (2009) underlines the importance of addressing societal needs and ethical questions in research and development, as well as the Council conclusions on the Social Dimension of the European Research Area (2010). To achieve better alignment of research an and innovation with societal needs a number of initiatives have been undertaken by EU Member States and the European Commission. These initiatives have shown that there is a need for a comprehensive approach to achieve such an improved alignment. Responsible Research and Innovation (RRI) refers to the comprehensive approach of proceeding in research and innovation in ways that allow all stakeholders that are involved in the processes of research and innovation at an early stage (A) to obtain relevant knowledge on the consequences of the outcomes of their actions and on the range of options open to them and (B) to effectively evaluate both outcomes and op- tions in terms of societal needs and moral values and (C) to use these considerations (under A and B) as functional requirements for design and development of new research, products and services. The RRI approach has to be a key part of the research and innovation process and should be established as a collective, inclusive and system-wide approach. This expert group report identifies policy options for strengthening Responsible Research and Innovation. In chapter 1, the needs for action, and in particular European action is demonstrated. This chapter shows that there are many examples in which the outcomes of research has been contested in society, because societal impacts and ethical aspects have not adequately been taken into consideration in the development of innovation. In many cases, the related research funding was wasted. On the other hand, there are many cases in which the successful and early consideration of societal needs has brought up innovation which were particular successful also in economic terms. Hence, the early consideration of RRI aspects can contribute to developing lead markets for innovation serving societal needs. Furthermore, the chapter describes examples of unattended fields: these are fields of innovation which have a high economic and societal potential, but market forces in itself are insufficient to provide the necessary incentives for investment in these fields. Finally, the chapter identifies many different approaches among European and national actors in research and innovation policies to overcome the situation. Many different concepts and policies are in development to overcome the situation. There is a need to better coordinate these efforts in order to avoid failed investments in research and innovation, and in order to better exploit the potential of considering societal needs in innovation. This is very much in line with the aforementioned Europe 2020 strategy and the need for better alignment of research and innovation with societal challenges. In chapter 2, policy objectives for RRI are defined, with a view to the achievement of the Europe 2020 objectives, and in particular those of the Innovation Union and the European Research Area. The key objective of EU action should be to develop a coherent approach among the EU Member States that defines processes, instruments and criteria for RRI that on the one hand encourage researchers and innovative firms to consider ethical concerns and address societal needs. A framework for the operationalization of RRI entails (a) defin- ing criteria for RRI, (b) defining processes for a successful application of RRI, and (c) Defining instruments to encourage RRI. In chapter 3, four main policy scenarios are presented. Option 1, ‘business as usual' implies that the existing approaches to address RRI in EU funding programmes would continue to be the main tools to promote RRI at the EU level. There would neither be any additional ef- forts to address RRI in the upcoming funding programmes like Horizon 2020 nor any new funding opportunities to address RRI. Furthermore, actions on RRI in EU Member States and potentially also private businesses would continue but there would neither be any attempts to coordinate the different approaches towards RRI, nor to initiate a process for the devel- opment of a common European understanding of RRI. The standards for RRI would remain scattered. Option 2 is an ‘improved business as usual', with specific funding for RRI. This would en- tail several actions. One action would be to ‘mainstream' RRI in the existing funding pro- grammes. In this case no new funding opportunities on RRI would be allocated, but criteria for RRI would have to be applied across all EU funding programmes. This would not only raise awareness for RRI but also create greater transparency with regard to the provisions for taking into account societal needs and ethical aspects in the proposed research. A sec- ond action under option 2 would be an increased share of funding for inter- and transdis- ciplinary research, including funding options for stakeholder participation in the research process, so as to further encourage research that more directly takes societal needs into account. A third action would be a specific funding line for research on RRI. There is still a need for more information on the interactions of science, innovation and society, which needs further development of the theoretical approaches and the study of conditions for a successful application of RRI in practice. Option 3 entails ‘Improved coordination with the EU Member States without a legally binding initiative'. This option goes beyond strengthening RRI in the existing funding programmes of the EU. It includes directly addressing the Member States as well as business enterprises, research institutions and public and private research funders. However, option 3 and op- tion 2 are not mutually exclusive, but can be used as complementary tools to promote RRI. Option 3 is process-oriented, aiming at fostering the dialogue on RRI enhancement and improving the transparency with regard to RRI activities in the Member States. It creates a framework enabling policy makers, researchers and business enterprises to put a stronger emphasis on RRI by raising awareness for the issue, but also by changing education and setting incentives for applying RRI in both research and innovation. This option would have several action lines, one being to establish a framework for enhancing cooperation on RRI activities. This framework can be accompanied with a reporting scheme on RRI activities. The framework would involve for example trainings on RRI for researchers, and funding schemes for RRI within existing Member States funding programmes. The latter could also be linked to EU initiatives such as Joint Programming Initiatives (JPIs) and European Innova- tion Partnerships (EIP). The framework could also use public procurement as a policy instru- ment, to set incentives for business enterprises. Member States can thus stimulate RRI in innovation processes by using public procurement to start pre-commercial projects and to purchase innovative and sustainable products. Another action line would be to establish Codes of Conduct for RRI activities. Codes of conduct are a method of self-governance by which researchers and innovators can agree on norms or standards. A third action line would furthermore be to establish Standards on RRI that can be adopted voluntarily. This would imply the development of a common framework for RRI that is applicable to the design of research processes that can the shape of European standards and establish a benchmarking process for RRI. This process can be organised in a similar way as the formu- lation of the ISO norm 26000, which provides guidance on Corporate Social Responsibility (CSR) practice in business enterprises and other organisations. A 4th option can also be identified, which is ‘Improved coordination with the Member States with a legally binding initiative'. This option is similar to option 3, but follows a more top- down approach in which mandatory guidelines would be implemented via European Regula- tions or Directives. Chapter 4 presents an analysis of the impacts of the various policy options. Assessing the impacts is difficult as some of the options are designed to initiate or enhance development processes for RRI activities rather than suggesting concrete measures. Furthermore, at this early stage costs will be easier to measure than benefits and the latter have to be measured more implicitly. The impacts of the first option, “business as usual” have been described in the first chapter: currently, there is a lack of incentives for considering RRI aspects in R{\&}D processes, as well as a lack of awareness about RRI. This option does not entail direct additional cost, but it would imply that the potential of research funding is still not fully exploited. Indirectly, this leads to costs for business enterprises and research institutions that could have developed more successful products, for research funders who could have spent their funding more efficiently and for consumers who do not necessarily receive products that serve their inter- ests in the best possible way. Furthermore, without any initiative to coordinate current RRI initiatives in some of the Member States, the European landscape on RRI will be fragment- ed, which in turn can be counterproductive for the Innovation Union and the Single Market. Option 2, ‘improved business as},
address = {Brussels},
annote = {Document in Road2CPS. Note date; 2013. Later documents issued by EC: Rome Declaration 3920, Nanocode 3910,3922. Some RRI documnts in BS8611, too. There is a big EU document; google it.

"Contested Innovation:

There are countless examples of innovation that have been contested by societal actors be- cause of ethical concerns or because of their failure to meet societal needs. These examples of innovations have in common that
• their technical and economic feasibility has been demonstrated
• there is an apparent sound ethical reason to adopt or implement them,
• large investments have been undertaken,
• they have been contested and frustrated on the basis of security, social or ethical concerns.

For example, Carbon Capture and Storage (CCS) technologies are highly contested in many Member States (Eurobarometer 2011). Citizens raise safety concerns over storage facili- ties in their neighbourhood although this technology is regarded as potentially beneficial to fighting climate change. Smart energy meters are another example of a contested in- novation. For example, in the Netherlands the roll-out of smart metering has been delayed because of concerns regarding privacy issues. Similarly, electronic health records are highly contested because of privacy concerns although they are also regarded as potentially ben- eficial to simplify medical treatment or prevent medical errors. A more detailed description of these examples can be found in Annex II.

Other examples of contested technologies include the following (see also Eurobarometer 2005):

• Nanotechnologies
• Genetically modified organisms
• E-Mobility
• Stem Cell Research
• Online social networks
• Biotechnology
• Dual-use robotics
• Nuclear technology
• Military and security technologies

What is common to these examples is the fact that considerable investments have been made to develop these innovations. Still societal needs and ethical concerns regarding these innovations were not adequately identified at the outset and were insufficiently in- corporated in their design and development. While it is difficult to estimate the costs of failed innovations, these cases show that an early consideration of ethical aspects and societal needs contributes to a more efficient spending of resources for research, develop- ment and innovation. The risks, concerns and uncertainties of new technologies oftentimes are considered only at a late stage, often just before market introduction (e.g. in case of products based on nanoparticles (BAuA 2006)) and their implications are not made to bear upon the design and development of new research, products and services. Public policies and business can react with moratoriums to those innovations, which can ultimately lead to stranded investments that cannot be recovered."},
author = {ExGpRRI13},
doi = {10.2777/46253},
institution = {European Commission},
isbn = {EUR25766 EN},
keywords = {Consent (informed):,Cyber-physical systems,IoT,agents,certification:,competitive challenges,consequences,contracts,design process,engineering,ergonomics,ethics,green,job design,legal,legislation,org. trust,organisational design,philosophy,requirements,robots,safety,security,socio-tech},
number = {EUR25766 EN},
publisher = {Directorate-General for Research and Innovation Science in Society},
title = {{Options for Strengthening Responsible Research and Innovation}},
url = {http://ec.europa.eu/research/science-society/document{\_}library/pdf{\_}06/options-for-strengthening{\_}en.pdf},
year = {2013}
}
@inproceedings{Binamungu2018,
abstract = {In Behaviour-Driven Development (BDD), the behaviour of the software to be built is specified as a set of example interactions with the system, expressed using a 'Given-When-Then' structure. The examples are written using customer language, and are readable by end-users. They are also executable, and act as tests that determine whether the implementation matches the desired behaviour or not. This approach can be effective in building a common understanding of the requirements, but it can also face problems. When the suites of examples grow large, they can be difficult and expensive to change. Duplication can creep in, and can be challenging to detect manually. Current tools for detecting duplication in code are also not effective for BDD examples. Moreover, human concerns of readability and clarity can rise. We present an approach for detecting duplication in BDD suites that is based around dynamic tracing, and describe an evaluation based on three open source systems.},
author = {Binamungu, Leonard Peter and Embury, Suzanne M. and Konstantinou, Nikolaos},
booktitle = {2018 IEEE 2nd International Workshop on Validation, Analysis and Evolution of Software Tests, VST 2018 - Proceedings},
doi = {10.1109/VST.2018.8327149},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Binamungu, Embury, Konstantinou - 2018 - Detecting duplicate examples in behaviour driven development specifications.pdf:pdf},
isbn = {9781538664926},
keywords = {Behaviour-driven development,Duplication detection,Dynamic tracing},
month = {mar},
pages = {6--10},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Detecting duplicate examples in behaviour driven development specifications}},
volume = {2018-March},
year = {2018}
}
@misc{Wickham2018,
abstract = {R package version 1.4.0},
author = {Wickham, Hadley and Bryan, Jennifer},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Title Automate Package and Project Setup.pdf:pdf},
title = {{usethis: Automate Package and Project Setup}},
year = {2018}
}
@article{Simis2016,
abstract = {Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists' training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists' perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.},
author = {Simis, Molly J. and Madden, Haley and Cacciatore, Michael A. and Yeo, Sara K.},
doi = {10.1177/0963662516629749},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Simis et al. - 2016 - The lure of rationality Why does the deficit model persist in science communication.pdf:pdf},
issn = {13616609},
journal = {Public Understanding of Science},
keywords = {STEM,knowledge deficit model,policy,public,science communication,social sciences},
number = {4},
pages = {400--414},
publisher = {SAGE Publications Ltd},
title = {{The lure of rationality: Why does the deficit model persist in science communication?}},
volume = {25},
year = {2016}
}
@misc{Chang2018,
abstract = {Cambridge Analytica, the political consulting firm that did work for the Trump campaign and harvested raw data from up to 87 million Facebook profiles, is shutting down. There is a complicated web of relationships that explains how the Trump campaign, Cambridge Analytica, and Facebook are tied together, as my colleague Andrew Prokop explains in this excellent piece. But if you need a refresher on how all the pieces fit together, this diagram helps make sense of it all.},
author = {Chang, Alvin},
booktitle = {VOX},
title = {{The Facebook and Cambridge Analytica scandal, explained with a simple diagram}},
url = {https://www.vox.com/policy-and-politics/2018/3/23/17151916/facebook-cambridge-analytica-trump-diagram},
urldate = {2018-07-16},
year = {2018}
}
@article{Helfat2007,
abstract = {Stylized facts matter enormously in management inquiry because such inquiry ultimately deals with the real world. But management research places much less emphasis on empirical regularities than we should expect, and that is required, for scholarship that ultimately concerns itself with the real world. Imagine grounding the study of real organizations only in theory; we ought to be laughed out of the academy for that. Yet our journals appear to insist on theory testing. In a field that seeks to understand the real world, it makes little sense to always put theory before facts. We must understand at least the broad outlines of ‘what' a phenomenon consists of before we try to explain ‘why' it occurs. That is, we need research directed toward uncovering empirical regular- ities, otherwise known as ‘stylized facts'. Only then are we in a position to build theory that in turn can serve as the basis for more refined tests and extensions. Empirical research in management Suppose we require theory before pursuing any empirical work. We might miss important and interesting questions about what transpires in the real world, which is presumably what we care about in management in general, and in strategic organization in particular. Rather than insist that empirical research always test theory, as our journals often appear to require, we can and should use empirical research to investigate phenomena that we observe in the real world. We can seek to answer questions such as: What is the phenomenon exactly? How regularly does it occur? In what settings? In conjunction with what other phenomena, if any? As an example, suppose we observe that one industry has high profits but few firms and another industry has low profits and many firms, but we know nothing else that might inform us about this observed inverse relationship. 185},
author = {Helfat, Constance E.},
doi = {10.1177/1476127007077559},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Helfat - 2007 - Stylized facts, empirical research and theory development in management.pdf:pdf},
issn = {14761270},
journal = {Strategic Organization},
number = {2},
pages = {185--192},
title = {{Stylized facts, empirical research and theory development in management}},
volume = {5},
year = {2007}
}
@article{Leisch2009a,
abstract = {This tutorial gives a practical introduction to creating R packages. We discuss how object oriented programming and S formulas can be used to give R code the usual look and feel, how to start a package from a collection of R functions, and how to test the code once the package has been created. As running example we use functions for standard linear regression analysis which are developed from scratch.},
author = {Leisch, Friedrich},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Leisch - 2009 - Creating R Packages A Tutorial.pdf:pdf},
journal = {Reprint: Compstat 2008-Proceedings in Computational Statistics},
keywords = {R packages,open source,software,statistical computing},
title = {{Creating R Packages: A Tutorial}},
url = {https://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf},
year = {2009}
}
@article{Cai2018,
abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
archivePrefix = {arXiv},
arxivId = {1709.07604},
author = {Cai, Hong Yun and Zheng, Vincent W and Chang, Kevin},
doi = {10.1109/TKDE.2018.2807452},
eprint = {1709.07604},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Zheng, Chang - 2018 - A Comprehensive Survey of Graph Embedding Problems, Techniques and Applications.pdf:pdf},
isbn = {9780810858213},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Electronic mail,Graph embedding,Machine learning,Social network services,Task analysis,Taxonomy,Toy manufacturing industry,Two dimensional displays,graph analytics,graph embedding survey,network embedding},
number = {9},
pages = {22},
title = {{A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications}},
volume = {30},
year = {2018}
}
@inproceedings{Cooley,
abstract = {Application od data mining techbiques to the world wide web, referref to as web minin, has been the focus of several recent research projects and papers. However,there is no established vocabulary,leading to confusion when comparing research effots.},
author = {Cooley, R and Mobasher, B. and Srivastava, J.},
booktitle = {Proceedings Ninth IEEE International Conference on Tools with Artificial Intelligence},
doi = {10.1109/TAI.1997.632303},
isbn = {0-8186-8203-5},
issn = {1082-3409},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {558--567},
title = {{Web mining: information and pattern discovery on the World Wide Web}},
url = {http://ieeexplore.ieee.org/abstract/document/632303/ http://ieeexplore.ieee.org/document/632303/}
}
@article{Ward2013,
abstract = {The term big data has become ubiquitous. Owing to a shared origin between academia, industry and the media there is no single unified definition, and various stakeholders provide diverse and often contradictory definitions. The lack of a consistent definition introduces ambiguity and hampers discourse relating to big data. This short paper attempts to collate the various definitions which have gained some degree of traction and to furnish a clear and concise definition of an otherwise ambiguous term.},
archivePrefix = {arXiv},
arxivId = {1309.5821},
author = {Ward, Jonathan Stuart and Barker, Adam},
doi = {10.1145/2699414},
eprint = {1309.5821},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ward, Barker - 2013 - Undefined By Data A Survey of Big Data Definitions.pdf:pdf},
isbn = {9781612083957},
issn = {00010782},
journal = {arXiv},
month = {sep},
title = {{Undefined By Data: A Survey of Big Data Definitions}},
url = {http://arxiv.org/abs/1309.5821},
year = {2013}
}
@article{OHalloran2018,
abstract = {This article demonstrates how a digital environment offers new opportunities for transforming qualitative data into quantitative data in order to use data mining and information visualization for mixed methods research. The digital approach to mixed methods research is illustrated by a framework which combines qualitative methods of multimodal discourse analysis with quantitative methods of data mining and information visualization in a multilevel, contextual model that will result in an integrated, theoretically well-founded, and empirically evaluated technology for analyzing large data sets of multimodal texts. The framework is applicable to situations in which critical information needs to be extracted from geotagged public data: for example, in crisis informatics, where public reports of extreme events provide valuable data sources for disaster management.},
author = {O'Halloran, Kay L. and Tan, Sabine and Pham, Duc-Son and Bateman, John and {Vande Moere}, Andrew},
doi = {10.1177/1558689816651015},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/O'Halloran et al. - 2018 - A Digital Mixed Methods Research Design Integrating Multimodal Analysis With Data Mining and Information Vi.pdf:pdf},
issn = {1558-6898},
journal = {Journal of Mixed Methods Research},
keywords = {Big Data,data mining,digital mixed methods design,information visualization,multimodal discourse analysis,social semiotics},
mendeley-tags = {Big Data},
month = {jan},
number = {1},
pages = {11--30},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{A Digital Mixed Methods Research Design: Integrating Multimodal Analysis With Data Mining and Information Visualization for Big Data Analytics}},
url = {http://journals.sagepub.com/doi/10.1177/1558689816651015},
volume = {12},
year = {2018}
}
@inproceedings{Osterweil1987,
abstract = {Note: OCR errors may be found in this Reference. The major theme of this meeting is the exploration of the importance of .ul process as a vehicle for improving both the quality of software products and the the way in which we develop and evolve them. In beginning this exploration it seems important to spend at least a short time examining the nature of process and convincing ourselves that this is indeed a promising vehicle.},
address = {Monterey, California. USA},
annote = {cited over 1000 times.
Have to plan for software reuse.},
author = {Osterweil, Leon J.},
booktitle = {Proceedings of the 9th international conference on Software Engineering},
doi = {10.1109/ICSSP.2012.6225970},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Osterweil - 1987 - Software processes are software too.pdf:pdf},
isbn = {0897912160},
issn = {02705257},
pages = {2--13},
publisher = {IEEE Computer Society Press},
title = {{Software processes are software too}},
url = {https://dl.acm.org/citation.cfm?id=41766 http://portal.acm.org/citation.cfm?id=41765.41766{\%}5Cnhttp://dl.acm.org/citation.cfm?id=41766},
volume = {3},
year = {1987}
}
@article{Knight1970,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Knight, Kenneth L and Laffoon, Jean L},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Knight, Laffoon - 1970 - A Mosquito Taxonomic Glossary. II. Adult Head (Internal).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Mosquito Syst. Newsletter},
keywords = {MS02N02P069},
number = {2},
pages = {69--81},
pmid = {25246403},
title = {{A Mosquito Taxonomic Glossary. II. Adult Head (Internal).}},
url = {https://www.researchgate.net/publication/241425278},
volume = {2},
year = {1970}
}
@inproceedings{hyslop1999toolssystems,
author = {Hyslop, S M and Palmer, P J and Williams, D J},
booktitle = {Proceedings of the Twenty-Fourth IEEE/CPMT International Electronics Manufacturing Technology (IEMT) Symposium},
issn = {1089-8190},
month = {oct},
organization = {Austin, Texas},
pages = {395--401},
publisher = {IEEE Catalog No 99CH36330},
title = {{Tools for System Specific Packaging for Electro-Mechanical Systems}},
year = {1999}
}
@article{Havens2012,
abstract = {Very large (VL) data or big data are any data that you cannot load into your computer'sworking memory. This is not an objective definition, but a definition that is easy to understand and one that is practical, because there is a dataset too big for any computer youmight use; hence, this is VL data for you. Clustering is one of the primary tasks used in the pattern recognition and data mining communities to searchVLdatabases (includingVLimages) in various applications, andso, clustering algorithms that scale well to VL data are important and useful. This paper compares the ef- ficacy of three different implementations of techniques aimed to extend fuzzy c-means (FCM) clustering to VL data. Specifically,we compare methods that are based on 1) sampling followed by nonit- erative extension; 2) incremental techniques thatmake one sequen- tial pass through subsets of the data; and 3) kernelized versions of FCM that provide approximations based on sampling, including three proposed algorithms.We use both loadable and VL datasets to conduct the numerical experiments that facilitate comparisons based on time and space complexity, speed, quality of approxima- tions to batchFCM(for loadable data), and assessment ofmatches between partitions and ground truth. Empirical results show that random sampling plus extension FCM, bit-reduced FCM, and ap- proximate kernel FCM are good choices to approximate FCM for VL data. We conclude by demonstrating the VL algorithms on a dataset with 5 billion objects and presenting a set of recommenda- tions regarding the use of different VL FCM clustering schemes.},
author = {Havens, Timothy C. and Bezdek, James C. and Leckie, Christopher and Hall, Lawrence O. and Palaniswami, Marimuthu},
doi = {10.1109/TFUZZ.2012.2201485},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Havens et al. - 2012 - Fuzzy c-Means algorithms for very large data.pdf:pdf},
isbn = {10636706 (ISSN)},
issn = {10636706},
journal = {IEEE Transactions on Fuzzy Systems},
keywords = {Big data,fuzzy {\$}c{\$}-means (FCM),kernel methods,scalable clustering,very large (VL) data},
month = {dec},
number = {6},
pages = {1130--1146},
pmid = {20046893},
title = {{Fuzzy c-Means algorithms for very large data}},
url = {http://ieeexplore.ieee.org/document/6205366/},
volume = {20},
year = {2012}
}
@article{Theobald2015,
abstract = {The collective impact of humans on biodiversity rivals mass extinction events defining Earth's history, but does our large population also present opportunities to document and contend with this crisis? We provide the first quantitative review of biodiversity-related citizen science to determine whether data collected by these projects can be, and are currently being, effectively used in biodiversity research. We find strong evidence of the potential of citizen science: within projects we sampled (. n=. 388), {\~{}}1.3. million volunteers participate, contributing up to 2.5. billion in-kind annually. These projects exceed most federally-funded studies in spatial and temporal extent, and collectively they sample a breadth of taxonomic diversity. However, only 12{\%} of the 388 projects surveyed obviously provide data to peer-reviewed scientific articles, despite the fact that a third of these projects have verifiable, standardized data that are accessible online. Factors influencing publication included project spatial scale and longevity and having publically available data, as well as one measure of scientific rigor (taxonomic identification training). Because of the low rate at which citizen science data reach publication, the large and growing citizen science movement is likely only realizing a small portion of its potential impact on the scientific research community. Strengthening connections between professional and non-professional participants in the scientific process will enable this large data resource to be better harnessed to understand and address global change impacts on biodiversity.},
author = {Theobald, E. J. and Ettinger, A. K. and Burgess, H. K. and DeBey, L. B. and Schmidt, N. R. and Froehlich, H. E. and Wagner, C. and HilleRisLambers, J. and Tewksbury, J. and Harsch, M. A. and Parrish, J. K.},
doi = {10.1016/j.biocon.2014.10.021},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Theobald et al. - 2015 - Global change and local solutions Tapping the unrealized potential of citizen science for biodiversity research.pdf:pdf},
issn = {00063207},
journal = {Biological Conservation},
keywords = {Biodiversity,Citizen science,Climate change,Crowd sourcing,Global change,Volunteer monitoring},
month = {jan},
pages = {236--244},
publisher = {Elsevier Ltd},
title = {{Global change and local solutions: Tapping the unrealized potential of citizen science for biodiversity research}},
volume = {181},
year = {2015}
}
@article{Thelwall2008,
abstract = {Bibliometrics has changed out of all recognition since 1958; becoming established as a field, being taught widely in library and information science schools, and being at the core of a number of science evaluation research groups around the world. This was all made possible by the work of Eugene Garfield and his Science Citation Index. This article reviews the distance that bibliometrics has travelled since 1958 by comparing early bibliometrics with current practice, and by giving an overview of a range of recent developments, such as patent analysis, national research evaluation exercises, visualization techniques, new applications, online citation indexes, and the creation of digital libraries. Webometrics, a modern, fast-growing offshoot of bibliometrics, is reviewed in detail. Finally, future prospects are discussed with regard to both bibliometrics and webometrics.},
author = {Thelwall, Mike},
doi = {10.1177/0165551507087238},
isbn = {0165-5515},
issn = {01655515},
journal = {Journal of Information Science},
keywords = {Bibliometrics,Big Data,Scholarly publishing,Webometrics},
mendeley-tags = {Big Data},
month = {aug},
number = {4},
pages = {605--621},
title = {{Bibliometrics to webometrics}},
url = {http://journals.sagepub.com/doi/10.1177/0165551507087238},
volume = {34},
year = {2008}
}
@article{Jae-wookAhn2014,
abstract = {Visualization has proven to be a useful tool for understanding network structures. Yet the dynamic nature of social media networks requires powerful visualization techniques that go beyond static network diagrams. To provide strong temporal network visualization tools, designers need to understand what tasks the users have to accomplish. This paper describes a taxonomy of temporal network visualization tasks. We identify the 1) entities, 2) properties, and 3) temporal features, which were extracted by surveying 53 existing temporal network visualization systems. By building and examining the task taxonomy, we report which tasks are well covered by existing systems and make suggestions for designing future visualization tools. The feedback from 12 network analysts helped refine the taxonomy.},
author = {{Jae-wook Ahn} and Plaisant, Catherine and Shneiderman, Ben},
doi = {10.1109/TVCG.2013.238},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jae-wook Ahn, Plaisant, Shneiderman - 2014 - A Task Taxonomy for Network Evolution Analysis.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Big Data,Network visualization,design space,network evolution,task taxonomy,temporal analysis},
mendeley-tags = {Big Data},
month = {mar},
number = {3},
pages = {365--376},
title = {{A Task Taxonomy for Network Evolution Analysis}},
url = {http://ieeexplore.ieee.org/document/6620874/},
volume = {20},
year = {2014}
}
@article{Fiske2011a,
abstract = {Ecological research uses data collection techniques that are prone to substantial and unique types of measurement error to address scientic questions about species abundance and distribution. These data collection schemes include a number of survey methods in which unmarked individuals are counted, or determined to be present, at spatially- referenced sites. Examples include site occupancy sampling, repeated counts, distance sampling, removal sampling, and double observer sampling. To appropriately analyze these data, hierarchical models have been developed to separately model explanatory variables of both a latent abundance or occurrence process and a conditional detection process. Because these models have a straightforward interpretation paralleling mecha- nisms under which the data arose, they have recently gained immense popularity. The common hierarchical structure of these models is well-suited for a unied modeling in- terface. The R package unmarked provides such a unied modeling framework, including tools for data exploration, model tting, model criticism, post-hoc analysis, and model comparison.},
author = {Fiske, Ian J. and Chandler, Richard B.},
doi = {10.18637/jss.v043.i10},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fiske, Chandler - 2011 - Unmarked An R package for fitting hierarchical models of wildlife occurrence and abundance(2).pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Distance,Ecological,Hierarchical,Occupancy,Occurrence,Point count,Wildlife},
number = {10},
pages = {1--23},
publisher = {American Statistical Association},
title = {{Unmarked: An R package for fitting hierarchical models of wildlife occurrence and abundance}},
volume = {43},
year = {2011}
}
@misc{Microsoft,
abstract = {Microsoft Research provides a continuously refreshed collection of free datasets, tools and resources designed to advance the state of the art of academic research in many areas of computer science, such as natural language processing and computer vision.},
author = {Microsoft},
booktitle = {Microsoft},
title = {{Data Science for Research}},
url = {https://www.microsoft.com/en-us/research/academic-program/data-science-microsoft-research/{\#}!dataset-directory},
urldate = {2018-07-31},
year = {2018}
}
@techreport{Taylor2019,
abstract = {The history of using R in the SMI (Sheffield Methods Institute) is largely that we've not existed for that long - unlike most university departments or institutes (as I'm sure you well know) with long histories and accompanying existing practices, we've only existed since 2014, when a bunch of new people were hired. As a consequence of that we've been able to build something from the ground up, and those of us who teach quants (Quantitative Methods) all more-or-less agreed that we'd prefer to teach with R than anything else},
address = {Sheffield},
author = {Taylor, Mark},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Palmer - 2019 - Personal Communication. October 1, 2019.pdf:pdf},
institution = {Sheffield University},
keywords = {Quantitative Methods,R},
mendeley-tags = {Quantitative Methods,R},
pages = {2},
title = {{R Usage At Sheffield University}},
year = {2019}
}
@article{Williams2005,
abstract = {There has been much prescriptive work in project management, exemplified in various 'Bodies of Knowledge'.  However, experience shows some preojects overspending considerably.  recently, systemic modelling research into the behaviour of large projects explains project overshoots by 'systemic' effects and the (sometimes counterintuitive) effects of management actions.  However, while this work is becoming more widely knowwn, embedding the lessons in project management proactice is not straightforward.  The current prescriptive dominant discourse of project management contains implicit underlying assumptions with which the systemic modelling work clashes, indeed showing how conventional methods can exacerbate rather than alleviate porject problems. Exploration of this modelling suggest that for projects that are complex, uncertain, and time-linmited, conventional methods might be inappropriate, and aspects of newer methodologies in which the project 'emerges' rather than being fully pre-planned might be more appropriate.  Some of the current literature on project classification schemes also suggests similar parameters, without the rationale that systemic modelling provides, thus providing useful back-up to this analysis. The eventual aim of this line of work is to enable project managers to choose effective ways to manage projects based on understanding and model-based theory.},
annote = {Good, solid paper. 117 references, so it's a good source of knowledge. Taken a strongly model-based approach, without spending too much time on justifying the models. This is a weakness, but maybe he's done this in other papers. See also Grex 2006 for similar paper, but lacking the same depth.

Quotes several L. Koskela papers on OR aspects, so it seems this is what interests Lauri? Maybe Stuart Green knew this? 

Definitions of a project are:
• a temporary endeavour undertaken to create a unique product or service (Project management Institute, 2000)
• a unique venture with a beginning and an end, conducte by people to meet established goals with parameters of cost, schedule and quality (Buchanan {\&} Boddy, 1992)

There are two professional bodies representing project management; the Project Management Institute (PMI - 130,000 members) and the International Project Management Association (IPMA - smaller). Both have defined 'Bodies of Knowledge' (BoK) for project management.

Starts the argument by asserting the initial tranches of knowledge date back to 1960s (Polaris, space projects, etc.) Relatively little academic of management attention to this established practice, so it has not undergoner the usual transformation over time. In last decade this has changed; now a recognised part of the management literature. The scandinavians have played a big part in this resurgence. several references to this, including de meyer.

Says that the BoKs basically are predicated on sets of normative procedures based on a tasks approach to project management, and the implicit assumption that the project must be fully-planned from the outset. Basic assumptions, he says, are:
• Project management is rational and normative. Hence, one can generate the Project Lifecycle, and create a generic version of this. Behavioural consequence is that project planning is paramount; gives rise to the notion of the Project Life Cycle, and close control of people to make sure they adhere to it.
• They adopted a positivist approach; reality is out there, and truth exists. Furthermore, there's none of this nonsense about 'socially constructed reality'; we can observe reality directly, and in an unbiassed way, and establish facts. Behavioural consequence is that all perturbations from the plan are 'errors', and action needs to be taken to restore compliance with the plan.
• Project management is about managing scope, and this is best managed by decomposition and sequencing of tasks. This leads, he says, to Work Breakdown Structures and project networks (ref Koskela {\&} Howells!!). Implicit in this is that requirements do n ot change, hence preplanning captures all necessary work, and that defined tasks are independent, and can be managed as such. Behavioral consequence is that environmental effects are ignored, especially if they mean significant alterations to the Plan. The project thus is treated as an 'island of order' in the disorderly flux of organisational life.

This does not sit well with a people-oriented approach, nor with the cultural environment of the project, he says. Goes on to mentions Hofstede and Hampden-Turner {\&} Trompenaars in this. Then some stuff about 'masculine-feminine' approaches to project mgmt; better summed up with the feminine aspect being termed emotional intelligence.

Next criticism is that the BoKs do not take a systemic view; the WBS is hierarchical with little interaction; the CPA doesn't have interaction; risks are individual, not systemically related to each other (this is a good point; we haven't taken this approach either).

Then he sets out the counter-argument, quoting lots of studies rounding up the cases of project over-runs. Problem is that many of these go back to 1980s; a more recent collection, embracing modern mgmt techniques, would be good. Most recent analysis is 2002 (Flyvberg, Holm, Buhl), on transportation infrastructures (?civil engineering?); 90{\%} of projects over-spent.

Goes on to argue that you need to take into account feedback loops (hence systems dynamics models), and include the 'soft' aspects as well (these seem to include morale, schedule pressure, effects of overtime, overcrowding, and client effects such as scope changes, delays, interference, lack of trust, etc.). Bases this part of the argument on 'post-mortem analysis' of projects (e.g. the Channel Tunnel). This technique was pioneered by Ackermann, Eden {\&} Wlliams, 1997, q.v.; see also Williams, Ackermann {\&} Eden 2003. Method seems to be to build models starting with a 'clean sheet', then use documentary and quantitative data to build the project team's socially-constructed view of the reality of the project, embedded in the model. Apparently, they have a clear progression from cognitive maps to to cause maps to influence diagrams to systems dynamics models. This leads to explanations for project behaviour derived from systemic inter-related sets oif causal factors (Inot single causes). Says that the other group working in this area (Cooper) has identified 3 sources of complexity in project behaviour (with which Williams seems to agree):

• the rework cycle
• feedback effects on productivity and work quality
• knock-on effects from upstream events to dcown-stream events

His team have found that many of the feedback loops are set up and exacerbated by management response to project perturbations. typically, these are efforts to acceleratwe the project; these are in effect perturbations in themselves, but with less time available to accommodate their effects, so we have a vicious cycle.

What he doesn't dicuss are the models themselves; for instance, do they include:
• fixed feedback loops over the duration of the project, or not?
• fixed parameters during the run, or dynamic parameters?
• all the relevant 'soft' parameters, as in our CIMOSA inter-relations diagram.
• note all three of these imply a metamodel for controlling the model. In turn, if he does include all these, it must make interpretation of the model very difficult. there's a metrics issue here.

These systems dynamics models challenge all three standard assumptions; effects can show up much later; conventional steps to fix perturbations, such as acceleration, actually worsen the problem. Results are non-intutitive, which, he points out, really undoes the assumptions that standard methods will work, and that planning will work.

Eventually, after repeating his arguments with a little more detail (there are several attempts by others to produce classifications of projects; we may need to look at these vis-a-vis our IITKM cube), he gets onto alternatives to the Received View. Lists several:
• from software industry, there are Adaptive Software Development, Extreme Programming, and Scrum (Highsmith, 2002)
• from construction, there is Last Planner (Koskela {\&} Howell, 2002)

Finally, sets three research questions:

• what metrics to assess (a) structural complexity, (b) uncertainty, and (c) time limitations
• how to combine the twin philosophies of 'constant replanning' and the normative 'maintain the current plan'
• what is the best mix of project management approaches given the metrics

I think we need to split complexity into induced and inherent. other questions OK.},
author = {Williams, T M},
journal = {IEEE Transactions on Engineering Management},
keywords = {accounting practices,case studies,complexity,consequences,control,corporate governance,culture,evaluation,measurement,mental models,methods,metrics/metrication,projects,socio-tech,systems},
number = {4},
pages = {497--508},
title = {{Assessing and moving on from the dominant project management discourse in the light of project over-runs}},
volume = {52},
year = {2005}
}
@inproceedings{Yakout2013,
abstract = {Various computational procedures or constraint-based methods for data repairing have been proposed over the last decades to iden- tify errors and, when possible, correct them. However, these ap- proaches have several limitations including the scalability and qual- ity of the values to be used in replacement of the errors. In this paper, we propose a new data repairing approach that is based on maximizing the likelihood of replacement data given the data dis- tribution, which can be modeled using statistical machine learning techniques. This is a novel approach combining machine learning and likelihood methods for cleaning dirty databases by value mod- ification. We develop a quality measure of the repairing updates based on the likelihood benefit and the amount of changes applied to the database. We propose SCARE (SCalable Automatic REpair- ing), a systematic scalable framework that follows our approach. SCARE relies on a robust mechanism for horizontal data partition- ing and a combination of machine learning techniques to predict the set of possible updates. Due to data partitioning, several up- dates can be predicted for a single record based on local views on each data partition. Therefore, we propose a mechanism to com- bine the local predictions and obtain accurate final predictions. Fi- nally, we experimentally demonstrate the effectiveness, efficiency, and scalability of our approach on real-world datasets in compari- son},
address = {New York, New York, USA},
author = {Yakout, Mohamed and Berti-{\'{E}}quille, Laure and Elmagarmid, Ahmed K.},
booktitle = {Proceedings of the 2013 international conference on Management of data - SIGMOD '13},
doi = {10.1145/2463676.2463706},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yakout, Berti-{\'{E}}quille, Elmagarmid - 2013 - Don't be SCAREd.pdf:pdf},
isbn = {9781450320375},
keywords = {Big Data,data cleaning,inconsistent data},
mendeley-tags = {Big Data},
pages = {553--564},
publisher = {ACM Press},
title = {{Don't be SCAREd}},
url = {http://dl.acm.org/citation.cfm?doid=2463676.2463706},
year = {2013}
}
@techreport{COM3982014,
address = {Brussels},
annote = {Document in Sustainability, with its separated annex. Official word from EC on the Circular Economy.

See also {\#}3912 Moving towards a circular economy; a publicity blurb to summarise this document.


Resource productivity quote:
"Industry already recognises the strong business case for improving resource productivity. It is estimated that resource efficiency improvements all along the value chains could reduce material inputs needs by 17{\%}-24{\%} by 2030 (Macroeconomic modelling of sustainable development and the links between the economy and the environment.(2011), Meyer, B. et al.) and a better use of resources could represent an overall savings potential of €630 billion per year for European industry (Guide to resource efficiency in manufacturing: Experiences from improving resource efficiency in manufacturing companies (2012), Europe INNOVA). Business driven studies based on product-level modelling demonstrate significant material cost saving opportunities for EU industry from circular economy approaches and a potential to boost EU GDP by up to 3.9{\%} by creating new markets and new products and creating value for business (Towards the Circular Economy: Economic and business rationale for an accelerated transition (2012), Ellen MacArthur Foundation.). It is not surprising therefore that companies are continually working to improve resource management, but they are held back by a range of market barriers.

The high-level European Resource Efficiency Platform (http://ec.europa.eu/environment/resource{\_}efficiency/re{\_}platform/index{\_}en.htm), bringing together selected governments, businesses and civil society organisations, called for action to move to a more circular economy, which relies more on reuse and high-quality recycling and much less on primary raw materials."

See also the Environment Action Programme (7th EAP) OJ L 354, 28.12.2013, p. 171–200.

Characteristics of a circular economy:
• reducing the quantity of materials required to deliver a particular service (lightweighting);
• lengthening products' useful life (durability);
• reducing the use of energy and materials in production and use phases (efficiency);
• reducing the use of materials that are hazardous or difficult to recycle in products and production processes (substitution);
• creating markets for secondary raw materials (recyclates) (based on standards, public procurement, etc.);
• designing products that are easier to maintain, repair, upgrade, remanufacture or recycle (ecodesign);
• developing the necessary services for consumers in this regard (maintenance/repair services, etc.);
• incentivising and supporting waste reduction and high-quality separation by consumers;
• incentivising separation, collection systems that minimise the costs of recycling, and reuse;
• facilitating the clustering of activities to prevent by-products from becoming wastes (industrial symbiosis); and
• encouraging wider and better consumer choice through renting, lending or sharing services as an alternative to owning products, while safeguarding consumer interests (in terms of costs, protection, information, contract terms, insurance aspects, etc).

EC support for the Circular Economy:
"In order to support design and innovation for a more circular economy, the Commission will: 
- under the EU Research and Innovation Programme (Horizon 2020), demonstrate the opportunities for moving towards a circular economy at European level with large-scale innovation projects targeted at cooperation within and between value chains, fostering skills development and supporting the market application of innovative solutions; 
- establish a reinforced partnership to support research and innovative policies for the circular economy; 
- facilitate the development of more circular models for products and services, including through a more coherent product policy, and further develop the application of the Ecodesign Directive by paying further attention to resource efficiency criteria, including for the future priority product groups in the 2015-2017 Work Plan; and 
- encourage the cascading principle in the sustainable use of biomass, taking into account all biomass using sectors so that biomass can be utilised in a most resource efficient way. 

European Resource Efficiency Platform:
"The European Resource Efficiency Platform has identified significant opportunities for business at different stages in the ‘loop' feeding materials back into the production process or various segments of the supply chain of origin or in other supply chains (http://ec.europa.eu/environment/resource{\_}efficiency/documents/erep{\_}manifesto{\_}and{\_}policy{\_}recommendations{\_}31-03-2014.pdf). These are based on experience of successful initiatives that could be scaled up and applied more widely, and include: 
• in the production phase, sustainable sourcing standards, voluntary schemes led by industry and retailers, and industrial symbiosis to provide markets for by-products;
• in the distribution phase, improving information on the resources contained in products and how they can be repaired or recycled, referred to in the recommendations of the Platform as a ‘product passport'; and
• in the consumption phase, collaborative consumption models based on lending, swapping, bartering and renting products, and product service systems to get more value out of underutilised assets or resources (e.g. cars, tools, lodging)."

Waste management (see document for specifics):
"In 2012, the Commission developed a Waste Management Scoreboard and roadmaps with specific recommendations for the Member States with the weakest performance. It will continue to focus particular attention on the Member States with the largest distance to the targets, seeking to address, in partnership with them, implementation weaknesses at an early stage. 

Economic measures have proved instrumental in improving national waste management, in particular through landfill and incineration taxes, pay-as-you-throw and extended producer responsibility schemes, or incentives for local authorities to promote prevention, reuse and recycling. Landfill bans have also proved effective. Setting minimum requirements for producer responsibility schemes at EU level will help cut costs and eliminate barriers faced by producers having to respect several national schemes in the EU. "},
author = {COM398},
edition = {2nd},
institution = {European Commision},
keywords = {Cyber-physical systems,IoT,affordances,circular economy,green,legislation,networks,product cycles,quotes,standards,strategic planning,strategy,supply chain,sustainability},
number = {COM(2014) 398 final/2},
title = {{Towards a circular economy: a zero waste programme for Europe}},
year = {2014}
}
@book{Plant2008,
author = {Plant, Colin W.},
isbn = {9780952168577},
pages = {544},
publisher = {Hertfordshire Natural History Society},
title = {{The moths of Hertfordshire : the history, status, distribution and phenology of the micro- and macro-lepidoptera of a South-eastern English county}},
url = {http://www.hnhs.org/publication/moths-hertfordshire},
year = {2008}
}
@inproceedings{Cryer2001,
author = {Cryer, Jonathan D and Should, Good Graphs and Marks, Tick},
booktitle = {Joint Statistical Meetings},
title = {{Problems with using Microsoft Excel for statistics}},
year = {2001}
}
@misc{Geisberger2015,
address = {Munich},
annote = {Basically, the bible on CPS. Shorter version is the Position paper, q.v. Document in Road2CPS},
author = {Geisberger, E and Broy, M},
booktitle = {acatech STUDY},
keywords = {Cyber-physical systems},
publisher = {Herbert Utz Verlag GMBH},
title = {{Living in a networked world:  integrated research agenda Cyber Physical Systems (agendaCPS)}},
year = {2015}
}
@article{Fox2007,
abstract = {This article describes how the Rcmdr package can be extended by suitably designed “plug-in” packages. Embodied in the Rcmdr package, the R Commander was originally conceived as a basic-statistics graph- ical user interface (“GUI”) to R. The R Comman- der's capabilities have since been extended substan- tially beyond this original purpose, although it still accesses only a small fraction of the statistical and data-management capabilities of the standard R dis- tribution, not to mention those of the more than 1000 contributed packages now on CRAN.},
author = {Fox, John},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fox - 2007 - Extending the R Commander by “plug in” packages.pdf:pdf},
isbn = {1609-3631},
journal = {R News},
pages = {1--7},
title = {{Extending the R Commander by “plug in” packages}},
url = {https://tolstoy.newcastle.edu.au/R/e3/help/att-2806/wrapper.pdf http://biostat.cau.ac.kr/CRAN/doc/Rnews/Rnews{\_}2007-3.pdf{\#}page=46},
year = {2007}
}
@article{Konstantinou2019,
abstract = {Background: Data scientists spend considerable amounts of time preparing data for analysis. Data preparation is labour intensive because the data scientist typically takes fine grained control over each aspect of each step in the process, motivating the development of techniques that seek to reduce this burden. Results: This paper presents an architecture in which the data scientist need only describe the intended outcome of the data preparation process, leaving the software to determine how best to bring about the outcome. Key wrangling decisions on matching, mapping generation, mapping selection, format transformation and data repair are taken by the system, and the user need only provide: (i) the schema of the data target; (ii) partial representative instance data aligned with the target; (iii) criteria to be prioritised when populating the target; and (iv) feedback on candidate results. To support this, the proposed architecture dynamically orchestrates a collection of loosely coupled wrangling components, in which the orchestration is declaratively specified and includes self-tuning of component parameters. Conclusion: This paper describes a data preparation architecture that has been designed to reduce the cost of data preparation through the provision of a central role for automation. An empirical evaluation with deep web and open government data investigates the quality and suitability of the wrangling result, the cost-effectiveness of the approach, the impact of self-tuning, and scalability with respect to the numbers of sources.},
author = {Konstantinou, Nikolaos and Abel, Edward and Bellomarini, Luigi and Bogatu, Alex and Civili, Cristina and Irfanie, Endri and Koehler, Martin and Mazilu, Lacramioara and Sallinger, Emanuel and Fernandes, Alvaro A.A. and Gottlob, Georg and Keane, John A. and Paton, Norman W.},
doi = {10.1186/s40537-019-0237-9},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Konstantinou et al. - 2019 - VADA an architecture for end user informed data preparation.pdf:pdf},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Data integration,Data preparation,Data quality},
month = {dec},
number = {1},
pages = {32},
publisher = {SpringerOpen},
title = {{VADA: an architecture for end user informed data preparation}},
volume = {6},
year = {2019}
}
@inproceedings{webb2008aassemblies,
author = {Webb, D P and Hutt, D A and Whalley, D C and Palmer, P J},
booktitle = {Proceedings of the 2nd IEEE CPMT Electronic Systemintegration Technology Conference},
isbn = {978-1-4244-2813-7},
month = {sep},
organization = {Greenwich},
pages = {511--516},
title = {{A Substrateless Process for Sustainable Manufacture of Electronic Assemblies}},
volume = {1},
year = {2008}
}
@book{Tufte2001,
address = {Nuneaton, UK},
author = {Tufte, Edward R.},
edition = {2nd},
isbn = {978-1930824133},
pages = {200},
publisher = {Graphics Press},
title = {{The Visual Display of Quantitative Information}},
year = {2001}
}
@misc{PoliceOpenData,
abstract = {This is the site for open data about crime and policing in England, Wales and Northern Ireland. You can download street-level crime, outcome, and stop and search data in clear and simple CSV format and explore the API containing detailed crime data and information about individual police forces and neighbourhood teams. You can also download data on police activity, and a range of data collected under the police annual data requirement (ADR) including arrests and 101 call handling. All the data on this site is made available under the Open Government Licence v3.0.},
author = {{Police Open Data}},
booktitle = {2018},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{Home | data.police.uk}},
url = {https://data.police.uk/},
urldate = {2018-05-29}
}
@article{Alawamleh2012a,
abstract = {SMEs have to collaborate with other enterprises in a virtual organisation (VO) forms to cope with an increasingly dynamic and competitive environment. Despite the increased interest in the area of collaboration information is still lacking about the risk sources of VO. This paper aims to reinforce the proposal for an integrated methodology to classify, manage and assess network level risk sources in VO and discusses the advantages of AHP/ANP over the other multi-criteria decision making (MCDM) methods before discussing the analytical hierarchy process (AHP) and the analytical network process (ANP) methods and the advantages of ANP over the AHP. This will be followed by illustrations of how ANP can be used to assess VO risk sources as part of the framework to support the final decision of VO collaboration. ANP will be used to set up a panel of weights of risk sources to define which risks are more serious. Overall, insights from the research and the process suggested in this research will aid SMEs in making a less risky decision. Reference to this paper should be made as follows: Alawamleh, M. and Popplewell, K. (2012) 'Analysing virtual organisation risk sources: an analytical network process approach', Int.},
author = {Alawamleh, Mohammad and Popplewell, Keith},
doi = {10.1504/IJNVO.2012.045209},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Alawamleh, Popplewell - 2012 - Analysing virtual organisation risk sources An analytical network process approach EU Project SYNERGY-Sup.pdf:pdf},
journal = {Int. J. Networking and Virtual Organisations},
keywords = {ANP,VO,analytical network process,collaboration,multi-criteria decision-making methods,risk,virtual organisation},
number = {1},
pages = {18--39},
title = {{Analysing virtual organisation risk sources: An analytical network process approach EU Project SYNERGY-Supporting highly adaptive network enterprise collaboration through semantically enabled knowledge services View project Analysing virtual organisation }},
url = {https://www.researchgate.net/publication/262328316},
volume = {10},
year = {2012}
}
@article{Justice2016,
abstract = {ABSTRACT: The widespread use of electric lamps has created “ecological light pollution” and “artificial light ecology.” Given the important role of insects in ecosystems, how they are affected by light pollution deserves attention. Lamps designed for lighting small areas around residences are used in abundance, but studies specifically examining them are scarce. This study used a light trap to capture insects for 60 summer nights in a suburban town in Virginia, USA. During each night of trapping, one of five different light bulbs was used in the trap (incandescent, compact fluorescent, halogen, warm color temperature LED, or cool color temperature LED). The data suggest that fewer insects overall are attracted to bulbs using LED technology than bulbs using incandescent technology. This difference was also observed in the orders Lepidoptera and Diptera. These results support the use of LED bulbs to reduce the insect attraction and mortality caused by the use of artificial lights at night.},
author = {Justice, Michael J. and Justice, Teresa C.},
doi = {http://dx.doi.org/10.3157/021.125.0502},
issn = {0013-872X},
journal = {Entomological News},
keywords = {Artificial Light Pollution,CFL,Halogen,Incandescent,LED,Lamps,Light Trap,Urban Ecology},
number = {5},
pages = {315--326},
title = {{Attraction of Insects to Incandescent, Compact Fluorescent, Halogen, and Led Lamps in a Light Trap: Implications for Light Pollution and Urban Ecologies}},
url = {http://www.bioone.org/doi/full/10.3157/021.125.0502},
volume = {125},
year = {2016}
}
@techreport{Cinelli2018,
abstract = {In a recent communication [1], Breskin, Cole and Hudgens aimed to demonstrate "how single-world intervention graphs can supplement traditional causal diagrams". The example used in their demonstration involved selection bias due to attrition, namely, subjects dropping out from a randomized trial before the outcome is observed. Here we use the same example to demonstrate the opposite conclusion; the derivation presented by Breskin et al. is in fact longer and more complicated than the standard, three-step derivation facilitated by traditional causal diagrams. We further show that more natural solutions to attrition problems are obtained when viewed as missing-data problems encoded in causal diagrams. The trial example of Breskin et al. is shown in the causal diagram of Figure 1a. The task is to estimate the average causal effect E[Y |do(A = a)] in the general population, given complete data on A (vaccine assignment) and W (injection site pain), while data on Y (disease outcome) is available only for those subjects who did not drop out of the study (S = 0). U stands for unmeasured health status, and participants with poor health (U = 1) are assumed to be both more likely to experience pain and get the disease. The standard strategy of causal diagrams is to convert the query expression, E[Y |do(A = a)], into an equivalent expression that can be estimated from the available data [2, 3]. The derivation goes as follows: E[Y |do(A = a)] = E[Y |A = a] (1) = w E[Y |A = a, W = w]P (W = w|A = a) (2) = w E[Y |A = a, W = w, S = 0]P (W = w|A = a) (3) *},
archivePrefix = {arXiv},
arxivId = {1801.03583},
author = {Cinelli, Carlos and Pearl, Judea},
booktitle = {Journal of Epidemiology},
eprint = {1801.03583},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cinelli, Pearl - 2018 - On the utility of causal diagrams in modeling attrition a practical example.pdf:pdf},
title = {{On the utility of causal diagrams in modeling attrition: a practical example}},
year = {2018}
}
@techreport{Perianes-Rodriguez2017,
abstract = {The analysis of bibliometric networks, such as co-authorship, bibliographic coupling, and co-citation networks, has received a considerable amount of attention. Much less attention has been paid to the construction of these networks. We point out that different approaches can be taken to construct a bibliometric network. Normally the full counting approach is used, but we propose an alternative fractional counting approach. The basic idea of the fractional counting approach is that each action, such as co-authoring or citing a publication, should have equal weight, regardless of for instance the number of authors, citations, or references of a publication. We present two empirical analyses in which the full and fractional counting approaches yield very different results. These analyses deal with co-authorship networks of universities and bibliographic coupling networks of journals. Based on theoretical considerations and on the empirical analyses, we conclude that for many purposes the fractional counting approach is preferable over the full counting one.},
author = {Perianes-Rodriguez, Antonio and Waltman, Ludo and {Van Eck}, Nees Jan},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Perianes-Rodriguez, Waltman, Van Eck - 2017 - Constructing bibliometric networks A comparison between full and fractional counting.pdf:pdf},
pages = {38},
title = {{Constructing bibliometric networks: A comparison between full and fractional counting}},
url = {https://arxiv.org/pdf/1607.02452.pdf},
year = {2017}
}
@article{Bala2016,
abstract = {Due to their widespread use, Internet, Web 2.0 and digital sensors create data in non-traditional volumes at terabytes and petabytes scale. The big data characterized by the four V's has brought with it new challenges given the limited capabilities of traditional computing systems. This paper aims to provide solutions which can cope with very large data in Decision-Support Systems DSSs. In the data integration phase, specifically, the authors propose a conceptual modeling approach for parallel and distributed Extracting-Transforming-Loading ETL processes. Among the complexity dimensions of big data, this study focuses on the volume of data to ensure a good performance for ETL processes. The authors' approach allows anticipating on the parallelization/distribution issues at the early stage of Data Warehouse DW projects. They have implemented an ETL platform called Parallel-ETL P-ETL for short and conducted some experiments. Their performance analysis reveals that the proposed approach enables to speed up ETL processes by up to 33{\%} with the improvement rate being linear.},
author = {Bala, Mahfoud and Boussaid, Omar and Alimazighi, Zaia},
doi = {10.4018/IJDSST.2016100104},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bala, Boussaid, Alimazighi - 2016 - Extracting-Transforming-Loading Modeling Approach for Big Data Analytics.pdf:pdf},
issn = {1941-6296},
journal = {International Journal of Decision Support System Technology},
keywords = {big data,data warehousing,extracting-transforming-loading,hadoop,mapreduce,parallel and distributed},
month = {oct},
number = {4},
pages = {50--69},
title = {{Extracting-Transforming-Loading Modeling Approach for Big Data Analytics}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJDSST.2016100104},
volume = {8},
year = {2016}
}
@misc{Google2018,
abstract = {Google searchable list of public datasets.},
author = {Google},
booktitle = {Google},
title = {{Public Data}},
year = {2018}
}
@article{Kosmala2016,
abstract = {Ecological and environmental citizen-science projects have enormous potential to advance scientific knowledge, influence policy, and guide resource management by producing datasets that would otherwise be infeasible to generate. However, this potential can only be realized if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen-science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen-science dataset should therefore be judged individually, according to project design and application, and not assumed to be substandard simply because volunteers generated it.},
author = {Kosmala, Margaret and Wiggins, Andrea and Swanson, Alexandra and Simmons, Brooke},
doi = {10.1002/fee.1436},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kosmala et al. - 2016 - Assessing data quality in citizen science.pdf:pdf},
issn = {15409309},
journal = {Frontiers in Ecology and the Environment},
month = {dec},
number = {10},
pages = {551--560},
title = {{Assessing data quality in citizen science}},
url = {http://doi.wiley.com/10.1002/fee.1436},
volume = {14},
year = {2016}
}
@book{South1961,
address = {London, UK},
author = {South, Richard},
edition = {Series II},
pages = {379},
publisher = {Fredrick Warne {\&} Co.},
title = {{The Moths of the British Isles}},
year = {1961}
}
@article{LupLow2001a,
abstract = {Existing duplicate elimination methods for data cleaning work on the basis of computing the degree of similarity between nearby records in a sorted database. High recall can be achieved by accepting records with low degrees of similarity as duplicates, at the cost of lower precision. High precision can be achieved analogously at the cost of lower recall. This is the recall-precision dilemma. We develop a generic knowledge-based framework for effective data cleaning that can implement any existing data cleaning strategies and more. We propose a new method for computing transitive closure under uncertainty for dealing with the merging of groups of inexact duplicate records and explain why small changes to window sizes has little effect on the results of the sorted neighborhood method. Experimental study with two real-world datasets show that this approach can accurately identify duplicates and anomalies with high recall and precision, thus effectively resolving the recall-precision dilemma.},
author = {{Lup Low}, Wai and {Li Lee}, Mong and {Wang Ling}, Tok},
doi = {10.1016/S0306-4379(01)00041-2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lup Low, Li Lee, Wang Ling - 2001 - A knowledge-based approach for duplicate elimination in data cleaning(2).pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Data cleaning,Duplicate elimination,Knowledge-based system},
number = {8},
pages = {585--606},
publisher = {Elsevier Ltd},
title = {{A knowledge-based approach for duplicate elimination in data cleaning}},
volume = {26},
year = {2001}
}
@article{Yu2017,
abstract = {The ever-growing datasets in observational astronomy have challenged scientists in many aspects, including an efficient and interactive data exploration and visualization. Many tools have been developed to confront this challenge. However, they usually focus on displaying the actual images or focus on visualizing patterns within catalogs in a predefined way. In this paper we introduce Vizic, a Python visualization library that builds the connection between images and catalogs through an interactive map of the sky region. Vizic visualizes catalog data over a custom background canvas using the shape, size and orientation of each object in the catalog. The displayed objects in the map are highly interactive and customizable comparing to those in the observation images. These objects can be filtered by or colored by their property values, such as redshift and magnitude. They also can be sub-selected using a lasso-like tool for further analysis using standard Python functions and everything is done from inside a Jupyter notebook. Furthermore, Vizic allows custom overlays to be appended dynamically on top of the sky map. We have initially implemented several overlays, namely, Voronoi, Delaunay, Minimum Spanning Tree and HEALPix grid layer, which are helpful for visualizing large-scale structure. All these overlays can be generated, added or removed interactively with just one line of code. The catalog data is stored in a non-relational database, and the interfaces have been developed in JavaScript and Python to work within Jupyter Notebook, which allows to create customizable widgets, user generated scripts to analyze and plot the data selected/displayed in the interactive map. This unique design makes Vizic a very powerful and flexible interactive analysis tool. Vizic can be adopted in variety of exercises, for example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or just large scale visualizations.},
archivePrefix = {arXiv},
arxivId = {1701.01222},
author = {Yu, W. and {Carrasco Kind}, M. and Brunner, R. J.},
doi = {10.1016/j.ascom.2017.06.004},
eprint = {1701.01222},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Carrasco Kind, Brunner - 2017 - Vizic A Jupyter-based interactive visualization tool for astronomical catalogs.pdf:pdf},
issn = {22131337},
journal = {Astronomy and Computing},
keywords = {Catalogs,Jupyter,Large-scale structure of universe-methods,Numerical,Python,Visualization},
month = {jul},
pages = {128--139},
publisher = {Elsevier},
title = {{Vizic: A Jupyter-based interactive visualization tool for astronomical catalogs}},
url = {https://www.sciencedirect.com/science/article/pii/S2213133716301500?via{\%}3Dihub},
volume = {20},
year = {2017}
}
@article{Spearman1904,
abstract = {All knowledge-beyond that of bare isolated occurrence- deals with uniformities. Of the latter, some few have a claim to be considered absolute, such as mathematical implications and mechanical laws. But the vast majority are only partial; medicine does not teach that smallpox is inevitably escaped by vaccination, but that it is so generally; biology has not shown that all animals require organic food, but that nearly all do so; in daily life, a dark sky is no proof that it will rain, but merely a warning; even in morality, the sole categorical imperative alleged by Kant was the sinfulness of telling a lie, and few thinkers since have admitted so much as this to be valid uni- versally. In psychology, more perhaps than in any other science, it is hard to find absolutely inflexible coincidences; occasionally, indeed, there appear uniformities sufficiently reg- ular to be practically treated as laws, but infinitely the greater part of the observations hitherto recorded concern only more or less pronounced tendencies of one event or aftribute to accom- pany another.},
author = {Spearman, C},
doi = {10.2307/1422689},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Spearman - 1904 - The proof and measurement of association between two things.pdf:pdf},
issn = {00029556},
journal = {The American journal of psychology},
number = {14},
pages = {72--91},
pmid = {3322052},
title = {{The proof and measurement of association between two things}},
volume = {15},
year = {1904}
}
@techreport{Sulston2012,
abstract = {The 21st century is a critical period for people and the planet. The global population reached 7 billion during 2011 and the United Nations projections indicate that it will reach between 8 and 11 billion by 2050. Human impact on the Earth raises serious concerns, and in the richest parts of the world per capita material consumption is far above the level that can be sustained for everyone in a population of 7 billion or more. This is in stark contrast to the world's 1.3 billion poorest people, who need to consume more in order to be raised out of extreme poverty. The highest fertility rates are now seen primarily in the least developed countries while the lowest fertility rates are seen in the more developed countries, and increasingly in Asia and Latin America. Despite a decline in fertility almost everywhere, global population is still growing at about 80 million per year, because of the demographic momentum inherent in a large cohort of young people. The global rate of population growth is already declining, but the poorest countries are neither experiencing, nor benefiting from, this decline. Population and consumption are both important: the combination of increasing global population and increasing overall material consumption has implications for a finite planet. As both continue to rise, signs of unwanted impacts and feedback (eg climate change reducing crop yields in some areas) and of irreversible changes (eg the increased rate of species extinction) are growing alarmingly. The relationship between population, consumption and the environment is not straightforward, as the natural environment and human socioeconomic systems are complex in their own right. The Earth's capacity to meet human needs is finite, but how the limits are approached depends on lifestyle choices and associated consumption; these depend on what is used, and how, and what is regarded as essential for human wellbeing. Demographic change is driven by economic development, social and cultural factors as well as environmental change. A transition from high to low birth and death rates has occurred in various cultures, in widely different socio-economic settings, and at different rates. Countries such as Iran and South Korea have moved through the phases of this transition much more rapidly than Europe or North America. This has brought with it challenges different from those that were experienced by the more developed countries as they reached the late stages of the transition. Population is not only about the growing numbers of people: changes in age structure, migration, urbanisation and population decline present both opportunities and challenges to human health, wellbeing and the environment. Migrants often provide benefits to their countries of origin, through remittances, and to their host countries by helping to offset a workforce gap in ageing populations. Current and future migration will be affected by environmental change, although lack of resources may mean that the most vulnerable to these changes are the least able to migrate. Policy makers should prepare for international migration and its consequences, for integration of migrants and for protection of their human rights. Developing countries will be building the equivalent of a city of a million people every five days from now to 2050. The continuing and rapid growth of the urban population is having a marked bearing on lifestyle and behaviour: how and what they consume, how many children they have, the type of employment they undertake. Urban planning is essential to avoid the spread of slums, which are highly deleterious to the welfare of individuals and societies. The demographic changes and consumption patterns described above lead to three pressing challenges. First , the world's 1.3 billion poorest people need to be raised out of extreme poverty. This is critical to reducing global inequality, and to ensuring the wellbeing of all people. It will require increased per capita consumption for this group, allowing improved nutrition and healthcare, and reduction in family size in countries with high fertility rates. Second, in the most developed and the emerging economies unsustainable consumption must be urgently reduced. This will entail scaling back or radical transformation of damaging material consumption and emissions and the adoption of sustainable technologies, and is critical to ensuring a sustainable future for all. At present, consumption is closely linked to economic models based on growth. Improving the wellbeing of individuals so that humanity flourishes rather than survives requires moving from current economic measures to fully valuing natural capital. Decoupling economic activity from material and environmental throughputs is needed urgently for example by reusing equipment and recycling materials, reducing waste, obtaining energy from renewable sources, and by consumers paying for the wider costs of their consumption. Changes to the current socio-economic model and institutions are needed to allow both people and the planet to flourish by collaboration as well as competition during this and subsequent centuries. This requires farsighted political leadership concentrating on long term goals. Third, global population growth needs to be slowed and stabilised, but this should by no means be coercive. A large unmet need for contraception remains in both developing and developed countries. Voluntary family planning is a key part of continuing the downward trajectory in fertility rates, which brings benefits to the individual wellbeing of men and women around the world. In the long term a stabilised population is an essential prerequisite for individuals to flourish. Education will play an important role: well educated people tend to live longer healthier lives, are more able to choose the number of children they have and are more resilient to, and capable of, change. Education goals have been repeatedly agreed by the international community, but implementation is poor. Science and technology have a crucial role to play in meeting these three challenges by improving the understanding of causes and effects (such as stratospheric ozone depletion), and in developing ways to limit the most damaging trends (such as enhancing agricultural production with reduced environmental impact). However, attention must be paid to the socio-economic dimensions of technological deployment, as barriers will not be overcome solely by technology but in combination with changes in usage and governance. Demographic changes and their associated environmental impacts will vary across the globe, meaning that regional and national policy makers will need to adopt their own range of solutions to deal with their specific issues. At an international level, this year's Rio+20 Conference on Sustainable Development, the discussions at the UN General Assembly revisiting the International Conference on Population and Development (ICPD+20) scheduled for 2014/2015 and the review of the Millennium Development Goals in 2015 present opportunities to reframe the relationship between people and the planet. Successfully reframing this relationship will open up a prosperous and flourishing future, for present and future generations.},
address = {London},
annote = {See also Marqusee, also World Bank on rural poverty; McMichael, also Sherwood on heat stress


1.1 The evidence
The human population reached 7 billion in late 2011 (UNFPA 2011). Overall, humans are older and more urban than ever before. Fertility and mortality have declined markedly in many countries and life expectancy has increased (Cohen 2005), but differences in family size, life expectancy and income vary enormously around the globe and are greater than they have ever been. The World Bank estimates that 1.3 billion people in the developing world are trapped in absolute poverty living on less than US{\$}1.25 per day (World Bank 2012). Just under a fifth of the world's people have families averaging between 4 and 7 children and live in countries where 90{\%} of future population growth is expected to occur. In contrast the people who now have two children or fewer (42{\%} of the world's population), have the highest per capita incomes and consumption.

A greater number of consumers exist than ever before because of population growth. Economic development has meant that the material needs of societies have become more complex, reflecting aspirations as well as basic needs. Over the last sixty years total fish production has increased nearly fivefold (Swartz et al. 2010) and total world meat production fivefold (FAO 2006).

Patterns of consumption have led to big changes in the environment. The amount of carbon dioxide (CO2) in the atmosphere has increased steadily from pre-industrial times (Keeling 1960; Keeling and Whorf 2004a). Emissions of CO2 from energy use reached an all time high in 2010 (IEA 2011). The world has warmed by 0.8 ˚C from pre-industrial levels with a widespread melting of snow and ice. Since 1978 the annual average Arctic sea ice extent has shrunk by 2.7{\%} per decade (IPCC 2007a). With high levels of CO2 more of it is absorbed into water and the oceans are acidifying (Royal Society 2005; IAP 2009). The increase in food production has meant that more land is taken over for agriculture and fish stocks have dwindled rapidly. The loss of biodiversity has been dramatic (Convention on Biological Diversity 2010).


1.2 The challenges
The Royal Society perceives two critical issues that must be addressed quickly in order to establish a sustainable way of life for all people and avoid undermining the wellbeing of future generations. The first is the continuing expansion of the human population. The annual rate of global population growth has slowed from its peak at above 2.0{\%} in the mid 1960s; fertility rates have fallen so that in 2010 almost 48{\%} of world population had a total fertility of less than 2.1 children per woman (UN 2011a). However, rapid population growth continues in some parts of the world. The upward population trend will not reach its peak for another 40 years or more because present day children and the unborn have yet to have children themselves. 

The second major issue facing the planet is that, taken as a whole, per capita consumption is increasing. Total consumption will continue to increase as the population gets larger, as more people on the planet means more mouths to feed and more goods to satisfy their aspirations. People depend on their natural environment for meeting many of these needs and desires, but overconsumption of material resources is eroding this natural capital. Access to sufficient food, water and fuel for everybody is already a problem (UNDP 2011). The unprecedented growth in consumption of natural resources has provided real benefits for many. According to the 2010 UN Human Development Report (UNDP 2010), people around the world have become better educated, healthier and wealthier over the last 20 years. However, the economic benefits from development have not been equitably distributed. Many of the adverse impacts of environmental degradation have fallen disproportionately on the poor (UN 2011c). 

Reducing these differences in wealth is challenging. The problems posed by population size and overall consumption are interlinked. Lifting 1.3 billion people out of poverty will necessarily increase consumption in some regions and must be done in a way that will not totally alienate those who are already wealthy and whose consumption in certain areas needs to fall (World Bank 2012). While migration can contribute to development and to meeting the needs of ageing developed countries, it also impacts on local politics, and services
.

2.3.6 Urbanisation trends
Urbanisation is defined as an increasing proportion of a population living in urban areas, and is mostly due to the net movement of people from rural to urban areas (UN definition). Urbanisation should not be confused with urban growth (an absolute term, rather than a proportion) (Satterthwaite 2007), which is usually due to the fact that those living in (and migrating to) urban areas are of reproductive age. Urbanisation appears to be a global process as all countries of the world are becoming more urbanised, ie a greater proportion of the population are living in urban areas.

Globally, the proportion of people in urban areas rose from 29{\%} in 1950 to 49{\%} in 2005 and is expected to reach 69{\%} by 2050. In 2005, the total population of the Least, Less and More Developed Countries living in urban areas was 27{\%}, 46{\%} and 74{\%}, respectively. The Least Developed Countries are now seeing the fastest annual growth rate in the proportion of the total population that is urban, at 4{\%} between 2000 and 2005, compared to 2.7{\%} for the Less Developed Countries, excluding the Least Developed Countries, 0.7{\%} for the More Developed Countries and 2.2{\%} worldwide (UN 2010).


p.33 Fertility {\&} empowerment of women
What of the reverse effect of lower fertility and widespread contraceptive use on empowerment? Transition from a fertility rate of, say, six births per women to two represents a profound revolution in the lives of women and the ability to manage family size is central to the autonomy of women. Under the high fertility scenario it has been estimated that women spend 70{\%} of their lives bearing and rearing children whereas under a low fertility regime, the figure drops to 14{\%} (Lee 2003). When fertility is low, marriage and the family need no longer dominate women's lives; increased participation in paid work more usually follows than precedes the decline in childbearing (Mammen and Paxson 2000).

2.3.11 Drivers and barriers of urbanisation
Virtually all nations that have seen high levels of urbanisation over the last 50 – 60 years have had long periods of rapid economic growth, alongside large shifts in employment patterns from agricultural or pastoral activities to industrial, service and information activities. Nations with high proportions of urban populations and the largest cities are those that have the largest economies. In the 19th and 20th centuries industrialisation, urbanisation and modern economic growth occurred at the same time, whilst in recent decades urbanisation has occurred in settings in which sustained economic growth and industrialization are largely absent (Dyson 2011).
[Recall the argument that innovation is fertilised by urbanisation, and that there is possibly a minimum size for this to accelerate. Also note that innovation uptake rate (or absorption rate) in this scenario is the differential between slums and prosperity. Unfortunately, there doesn't seem to be a discussion of this, but see 'Innovation adoption rate', or Rogers, in Wikipedia.]p.39 Box 2.7 Urbanisation and growing urban poverty in Africa – the city of Nairobi [see table in report]
Despite being the least urbanised of the world's major regions, Africa's urban populations are growing at much higher rates than the other regions. Africa's high rates of urbanisation have not been accompanied by the broad socioeconomic transformations and economic growth that other regions experienced. Although urban areas have been the main drivers of the limited economic growth that Africa has experienced in the recent past (Kessides 2005), the inability of African economies to create enough economic opportunities for their rapidly growing populations has brought unique challenges in urban settings. A key outcome of rapidly growing urban populations amidst slow economic growth and poor urban planning has been the growth of informal settlements, commonly referred to as slums, in sub-Saharan Africa major cities. Estimates by UN Habitat indicate that about 62{\%} of urban residents in sub-Saharan Africa live in slum settlements, and that the number of slum dwellers almost doubled from 103 million to 200 million between 1990 and 2010. Slum settlements are characterized by poor housing conditions, poor social services, poor basic amenities, poor health outcomes, insecurity, and unstable incomes and livelihoods.

Nairobi, Kenya's economic hub, typifies the challenge of growing urban poverty that many countries in sub-Saharan Africa are facing. The city's population grew from 120,000 in 1948 to 3,140,000 in 2009. The city grew at an annual average rate of 5{\%} between 1989 and 1999, and 4{\%} during the period 1999 – 2009, despite the fall in employment opportunities associated with Kenya's economic downturn during this period. Between 40{\%} and 60{\%} of Nairobi residents live in slum settlements. Research carried out in the slums of Nairobi has documented the significant health and economic disadvantages that slum residents face, relative to other population sub-groups in Kenya. Although many of the slum residents are migrants from rural areas (only 15{\%} were born in the slum settlements), who come to Nairobi in order to get better livelihood opportunities, only 3.6{\%} of women and 22.3{\%} of men get salaried employment and most of them end up in unstable and low-paying jobs. Slum residents exhibit poorer health outcomes compared to other population sub-groups in Kenya due to a combination of three main characteristics of slum settlements. Firstly, slum dwellers live in congested locations, they reside in one-room houses, and lack access to safe drinking water, sanitation, and garbage disposal services. These poor environmental conditions increase their vulnerability to infectious diseases, including HIV/ AIDS (Ziraba et al. 2009). Secondly, slum residents have limited access to quality health services since health care is mostly provided by informal and unregulated health providers. Finally, slum dwellers do not access the quality services that are found in non-slum parts of the city because they lack money to pay for such services (Zulu et al. 2011). These urban disadvantages also extend to other social services like education. Children in slum settlements have relatively high dropout rates and uniquely low progression rates from primary to secondary school (Ngware et al. 2011), thereby leading to poor human capital among slum residents.

The rapid urban transition that Africa is undergoing presents an opportunity as well as a development challenge. As in developed regions and other emerging economies, urbanisation can be associated with, and help propel, economic prosperity. But rapid urbanisation can present enormous challenges unless the economies transform fast enough to generate enough jobs for the growing labour force. Additionally, good governance and planning are required to develop the urban infrastructure and social services that meet the needs of the growing population, including decent housing and amenities for low income people.

In 2011, the third most populous city in Kenya, after Nairobi and Mombasa, became a refugee camp called Dadaab with 465,000 refugees as of July 2011 from drought and from conflict in Somalia. It possible that many will never return to their homes. It is highly likely that such migration will be repeated many times in the future as rapid population growth and climate warming meet across the Sahel. To the problems of slum life outlined above are added violence and rape as women and children walk many miles in search of food and water.


p.59 Box 3.3 Technological lock-in
The consumption of resources per capita often continues to increase after advancements in technology and processes which would permit its reduction. The drivers of this phenomenon are diverse.

The rate of change in levels of consumption with the arrival of a new technology is a function of the inertia of the existing technology regime and the rate of uptake of the new technology. Technologies are embedded within wider social and economic systems: these include structures of markets, patterns of consumer demand, systems of regulation and infrastructure. For instance, the electricity-generating regime is founded upon rules and practices, relating to large centres of power generation connected by high-voltage grid networks. Consumption patterns and supportive institutional arrangements have built around and reinforced this system for at least eighty years in more developed economies. Any transition to an alternative (eg distributed) technological model would require a body of organisational change and consumer uptake before it can take effect.

Lock-in is the process by which self-reinforcing barriers to change, inhibit or prevent the uptake of new technologies, many of which might improve the sustainability of resource demand. Forces of institutional lock-in arise from associations formed to represent collective interests, and by the adherence to convention. In other cases, apparently inferior designs become fixed in use by a process in which strategy and historic circumstance are as important as the design itself, creating technological lock-in. Examples include the VHS video tape design, QWERTY keyboard and lightwater nuclear reactors. Once established, mass production, rapid development and conformity to the prevailing norm (eg for inter-operability) lockedin these technologies as the predominant designs in their respective markets. Such technologies yield increasing economies of scale, and implementation of new designs and processes in their place incurs additional “penetrational” structural and resource costs – even if long term consumption of resources, functionality, environmental impacts and cost might be superior. (See Unruh, 2000; Smith et al. 2005; Jackson and Papathanasopoulou, 2008).


p.57 Mining characteristics:
Considerable amounts of water and energy are needed to extract metals, which along with the pollution generated by the extraction processes, are the main constraints to greater sustainability of mining operations. Approximately 10{\%} of active mines and 20{\%} of exploratory sites are located in areas of high conservation value, while nearly 30{\%} of active mines are located in water-stressed areas. Although mining activities may only cover a few square kilometres, exploration activities tend to spread much more widely. As well as creating competition for already scarce water supplies, local populations nearby or downstream can be adversely affected by an impact on water quality.


p.64 Box 4.1 Ecosystem ServicesSupporting Services are those necessary for all other ecosystem services, and their impacts on people are either indirect or occur over a very long time period. Changes in the other categories have relatively direct and short-term impacts on people. Supporting services include primary production, production of atmospheric oxygen, soil formation and retention, nutrient cycling, water cycling, and pollination.

Provisioning Services are the products obtained from ecosystems, including food and fibre, fuel, fresh water, genetic resources, biochemicals, natural medicine and pharmaceuticals.

Regulating Services are the benefits obtained from the regulation of ecosystem processes, including air quality maintenance, climate regulation, water regulation, erosion control, water purification and waste treatment, regulation of disease, biological control, pollination, storm protection.

Cultural Services are the nonmaterial benefits people obtain from ecosystems through spiritual enrichment, cognitive development, reflection, recreation, and aesthetic experiences.

This classification is not absolute: some services can appear under more than one heading. For example, pollination is both a supporting service (through determining the structure and dynamics of natural plant communities,) and also a regulating service (through fertilisation of crop plants).

The UK National Ecosystem Assessment (NEA) distinguishes final ecosystem services that directly deliver welfare gains and/or losses to people through goods, from the general term ‘ecosystem services' that includes the whole pathway from ecological processes through final ecosystem services, goods and values to humans (UK National Ecosystem Assessment 2011). Goods are products that are used or consumed by people: most goods provided by ecosystems also have other inputs: for example, food is a good that is essential for humans, but typically the food that people eat has been harvested, transported and often processed before it is eaten. In this case the good may be a loaf of bread and the final ecosystem service is wheat grain, but these depend on a wide range of other ecosystem services, including soil formation, nutrient and water cycling and disease control.
p.68 Metrics for environmental degradationOne of the most well-known frameworks for describing the relationship between population, consumption and the environment is the IPAT equation (I=P*A*T). It relates environmental impact (I) to population size (P), affluence per capita (A) (a measure of consumption and production) and the level of environmentally damaging technology (T) (Ehrlich and Holdren 1971, 1972; Holdren and Ehrlich 1974).{\ldots}
In an attempt to capture more complexity and interactions between the IPAT variables, Dietz and Rosa (1994, 1997) developed Stochastic Impacts by Regression on Population, Affluence and Technology or STIRPAT. STIRPAT allows for the consideration of context when analysing populationenvironment relationships by adding control variables to the model. For example, the model can include demographic characteristics other than population size, such as age structure and urbanisation (


p.69 Population carrying capacity:
Most published estimates of Earth‘s human carrying capacity uncritically assume answers to these questions, though the answers are not necessarily made explicit. Cohen (1995) analysed over 60 estimates published from 1679 onward which in the past half century alone ranged from less than one billion to more than one trillion people. Van Den Bergh and Rietveld (2004) gave an updated survey of estimates of Earth‘s human carrying capacity. Cohen (1995) concluded that estimates are often highly political, intended either to demonstrate that there are too many humans already on Earth or that further growth presents no problems. Because no estimates of human carrying capacity have explicitly addressed the questions raised above, taking into account the diversity of answers, that vary across societies, cultures and times, it must be concluded that no reliable scientific estimates of sustainable human population size exist, and that such estimates would be provisional and technology dependent. Defining a pathway to a sustainable population, when there is no agreement of the final destination, is thus highly contentious.

p.70 4.5.2 Ecological footprints
The ecological footprint is an estimate of the amount of the Earth's renewable resources requisitioned by human activities. It has been developed over recent decades by the Global Footprint Network (www. footprintnetwork.org), recalculated and reported annually in the WWF Living Planet report (WWF 2010). It is an accounting method using international statistics on resource flows of natural and renewable resources that are consumed by people and subsequent wastes that are naturally sequestered. Based on people's use of resources and the land area estimated to be necessary to support their production, the footprint is calculated to reflect the area of land surface that the human population, at some point in time or place, uses for consumption and to disperse associated pollution and waste.

The measures used to calculate the footprint are standardised to a common unit, a global hectare. This encompasses the average productivity of all the biologically productive land and sea area in the world in a given year. The footprint can be calculated separately for different nations and development groups, and can be tracked over time. The aggregation and disaggregation of data can pose some difficulties for interpretation of the footprint, which cannot account for heterogeneity of different land uses, and is insensitive to by-products of production, impacts on biodiversity and natural ecosystems, and to environmental change. However, the metrics do indicate trends and can be useful for comparisons over time and across different countries and country groupings.

The Global Footprint Network uses a measure of the capacity of the Earth against which to compare the footprint. Using this ‘bio capacity' measure it is possible to estimate how much of the Earth (or how many planet Earths) it would take to support humanity if everybody continues current patterns of resource use, or under some other given lifestyle. This method suggests that the Earth's capacity was exceeded by people sometime in the 1980s and that by 2010 the estimate was that human activities were using about 1.5 times the area that would be av},
author = {Sulston, J},
institution = {The Royal Society},
isbn = {ISBN 978-0-85403-955-5},
publisher = {The Royal Society},
title = {{People and the planet}},
year = {2012}
}
@inproceedings{Pearl2020,
author = {Pearl, Judea},
booktitle = {2019 IEEE International Conference on Big Data (Big},
doi = {10.1109/bigdata47090.2019.9005644},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2020 - The new science of cause and effect, with reflections on data science and artificial intelligence.pdf:pdf},
month = {feb},
pages = {4},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{The new science of cause and effect, with reflections on data science and artificial intelligence}},
year = {2020}
}
@article{article,
abstract = {The availability of gigantic volume of geospatial data, often continually updated (e.g. remote sensing data), has greatly challenged our ability to digest the data and to gain useful knowledge that would otherwise lost. Currently, most research efforts on data mining in geospatial data take the static view of geospatial phenomena, which captures “spatiality” only. However, as all geographic phenomena evolve over time, both “spatiality” and “temporality” is central to our understanding of geographic process and events. In addition, knowledge extracted from spatio-temporal data will help us to have better prediction of spatial processes or events. It is therefore very important to conduct research on data mining in spatio-temporal datasets.},
author = {Yao, Xiaobai},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yao - 2003 - Research Issues in Spatio-temporal Data Mining.pdf:pdf},
institution = {University of Georgia, Athens, GA 30602},
journal = {Workshop on Geospatial Visualization and Knowledge Discovery},
keywords = {Big Data,research issue,spatial,spatiotemporal},
mendeley-tags = {Big Data},
number = {November},
pages = {1--6},
title = {{Research Issues in Spatio-temporal Data Mining}},
url = {https://www.researchgate.net/publication/250189471{\_}Research{\_}Issues{\_}in{\_}Spatio-temporal{\_}Data{\_}Mining},
year = {2003}
}
@inproceedings{price2004technologyperspective,
annote = {Unmapped Imported Data : Isbn = ISBN EUR 21473 EN},
author = {Price, S and Summers, R and Conway, P P and Palmer, P J},
booktitle = {EU-US Scientific Seminar on New Technology Foresight, Forecasting and Assessment Methods},
editor = {Scapolo, F and Cahill, E E},
month = {may},
organization = {Seville, Spain},
pages = {67--76},
publisher = {EU Joint Research Centre},
title = {{Technology Roadmapping: A New Perspective}},
year = {2004}
}
@misc{Brodie2020,
abstract = {A technical issue was identified overnight on Friday 2 October in the data load process that transfers COVID-19 positive lab results into reporting dashboards. After rapid investigation, we have identified that 15,841 cases between 25 September and 2 October were not included in the reported daily COVID-19 cases. The majority of these cases occurred in most recent days.},
author = {Brodie, Michael},
booktitle = {Public Health England Website},
file = {:Users/TechTrends/Documents/Temp/government/PHE statement on delayed reporting of COVID-19 cases - GOV.UK.pdf:pdf},
title = {{PHE statement on delayed reporting of COVID-19 cases - GOV.UK}},
url = {https://www.gov.uk/government/news/phe-statement-on-delayed-reporting-of-covid-19-cases},
urldate = {2020-10-07},
year = {2020}
}
@article{Razzaq2017,
abstract = {—Interaction with the machines and computers is achieved using user interfaces. Nowadays, with the tremendous growth of technology, the interaction is made more simple and flexible. The study of user interfaces for human-computers and machines interaction is the main focus of this paper. In particular, an extensive overview of different user interfaces available so far is provided. The review covers text-based, graphical-based, and new class of emerging user interfaces to interact with the machines and computers. This work will be helpful for the development of new user interfaces.},
author = {Razzaq, Mirza Abdur and Qureshi, Muhammad Ali and {Hussain Memon}, Kashif and Ullah, Saleem},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Razzaq et al. - 2017 - A Survey on User Interfaces for Interaction with Human and Machines(2).pdf:pdf},
journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {brain-computer interface,emerging user interfaces,graphical user in-terface (GUI),natural lan-guage interface,sixth sense device,user interface (UI),—Command line interface (CLI)},
number = {7},
title = {{A Survey on User Interfaces for Interaction with Human and Machines}},
url = {https://pdfs.semanticscholar.org/a5ab/e99363013237cbc0cc278aa3724fe2924c9d.pdf},
volume = {8},
year = {2017}
}
@misc{Kristensen2008,
author = {Kristensen, Niels P and Scoble, Malcolm J and Karsholt, Ole},
booktitle = {Zootaxa},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kristensen, Scoble, Karsholt - 2008 - Lepidoptera phylogeny and systematics The state of inventorying moth and butterfly diversity (Zoot.pdf:pdf},
issn = {11755326},
number = {1725},
pages = {67--68},
title = {{Lepidoptera phylogeny and systematics: The state of inventorying moth and butterfly diversity (Zootaxa (2007) 1668, 699-747}},
url = {www.mapress.com/zootaxa/},
volume = {1668},
year = {2008}
}
@article{Xu2017,
abstract = {We introduce Zipporah, a fast and scal-able data cleaning system. We propose a novel type of bag-of-words translation fea-ture, and train logistic regression models to classify good data and synthetic noisy data in the proposed feature space. The trained model is used to score parallel sen-tences in the data pool for selection. As shown in experiments, Zipporah selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one noisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus.},
author = {Xu, Hainan and Koehn, Philipp},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Xu, Koehn - 2017 - Zipporah a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora.pdf:pdf},
journal = {Emnlp},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {2935--2940},
title = {{Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora}},
url = {http://www.aclweb.org/anthology/D17-1319 http://aclweb.org/anthology/D17-1318},
year = {2017}
}
@article{Lerner2018,
author = {Lerner, J{\"{u}}rgen and Lomi, Alessandro},
doi = {10.1371/journal.pone.0190674},
editor = {Wray, K. Brad},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lerner, Lomi - 2018 - Knowledge categorization affects popularity and quality of Wikipedia articles.pdf:pdf},
isbn = {19326203 (Electronic)},
issn = {19326203},
journal = {PLoS ONE},
month = {jan},
number = {1},
pages = {e0190674},
pmid = {29293627},
publisher = {Public Library of Science},
title = {{Knowledge categorization affects popularity and quality of Wikipedia articles}},
url = {http://dx.plos.org/10.1371/journal.pone.0190674},
volume = {13},
year = {2018}
}
@inproceedings{Bogatu2017,
abstract = {Data wrangling is the process whereby data is cleaned and integrated for analysis. Data wrangling, even with tool support, is typically a labour intensive process. One aspect of data wrangling involves carrying out format transformations on attribute values, for example so that names or phone numbers are represented consistently. Recent research has developed techniques for synthesising format transformation programs from examples of the source and target representations. This is valuable, but still requires a user to provide suitable examples, something that may be challenging in applications in which there are huge data sets or numerous data sources. In this paper we investigate the automatic discovery of examples that can be used to synthesise format transformation programs. In particular, we propose an approach to identifying candidate data examples and validating the transformations that are synthesised from them. The approach is evaluated empirically using data sets from open government data.},
author = {Bogatu, Alex and Paton, Norman W. and Fernandes, Alvaro A.A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-60795-5_4},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bogatu, Paton, Fernandes - 2017 - Towards automatic data format transformations Data wrangling at scale.pdf:pdf},
isbn = {9783319607948},
issn = {16113349},
keywords = {Data wrangling,Format transformations,Program synthesis},
pages = {36--48},
publisher = {Springer Verlag},
title = {{Towards automatic data format transformations: Data wrangling at scale}},
volume = {10365 LNCS},
year = {2017}
}
@inproceedings{Almaliki2015,
abstract = {Users' feedback is vital to improve software quality and it provides developers with a rich knowledge on how software meets users' requirements in practice. Feedback informs how software should adapt, or be adapted, at runtime and what evolutionary actions to take in the next release. However, studies have noted that accommodating the different preferences of users on how feedback should be requested is a complex task and requires a careful engineering process. This calls for an adaptive feedback acquisition mechanisms to cater for such variability. In this paper, we tackle this problem by employing the concept of Persona to aid software engineers understand the various users' behaviours and improve their ability to design feedback acquisition techniques more efficiently. We create a set of personas based on a mixture of qualitative and quantitative studies and propose PAFA, a Persona-based method for Adaptive Feedback Acquisition.},
address = {Athens, Greece.},
author = {Almaliki, Malik and Ncube, Cornelius and Ali, Raian},
booktitle = {Proceedings - International Conference on Research Challenges in Information Science},
doi = {10.1109/RCIS.2015.7128868},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Almaliki, Ncube, Ali - 2015 - Adaptive software-based Feedback Acquisition A Persona-based design.pdf:pdf},
isbn = {978-1-4673-6630-4},
issn = {21511357},
keywords = {Persona,Requirement engineering,User Feedback},
month = {may},
pages = {100--111},
publisher = {IEEE},
title = {{Adaptive software-based Feedback Acquisition: A Persona-based design}},
url = {http://ieeexplore.ieee.org/document/7128868/},
year = {2015}
}
@article{McCullough2008,
author = {McCullough, Bruce D and Heiser, David A},
journal = {Computational Statistics {\&} Data Analysis},
number = {10},
pages = {4570--4578},
publisher = {Elsevier},
title = {{On the accuracy of statistical procedures in Microsoft Excel 2007}},
volume = {52},
year = {2008}
}
@article{Altermatt2016,
abstract = {The globally increasing light pollution is a well-recognized threat to ecosystems, with negative effects on human, animal and plant wellbeing. The most well-known and widely documented consequence of light pollution is the generally fatal attraction of nocturnal insects to artificial light sources. However, the evolutionary consequences are unknown. Herewe report that moth populations from urban areas with high, globally relevant levels of light pollution over several decades show a significantly reduced flight-to-light behaviour compared with populations of the same species from pristine dark-sky habitats. Using a common garden setting, we reared moths from 10 different populations from early-instar larvae and experimentally compared their flight-to-light behaviour under standardized conditions. Moths from urban populations had a significant reduction in the flight-to-light behaviour compared with pristine populations. The reduced attraction to light sources of ‘city moths' may directly increase these individuals' survival and reproduction. We anticipate that it comes with a reduced mobility, which negatively affects foraging as well as colonization ability. As nocturnal insects are of eminent significance as pollinators and the primary food source of many vertebrates, an evolutionary change of the flight-to-light behaviour thereby potentially cascades across species interaction networks.},
author = {Altermatt, Florian and Ebert, Dieter},
doi = {10.1098/rsbl.2016.0111},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Altermatt, Ebert - 2016 - Reduced flight-to-light behaviour of moth populations exposed to long-term urban light pollution.pdf:pdf},
isbn = {0000000248},
issn = {1744-9561},
journal = {Biology Letters},
pmid = {27072407},
title = {{Reduced flight-to-light behaviour of moth populations exposed to long-term urban light pollution}},
volume = {12},
year = {2016}
}
@techreport{Davenport2012,
abstract = {Executive Summary In order to succeed with big data and high-performance analytics, organizations require not only new technologies, but also a new set of human capabilities. A key component of these capabilities are data scientists—hybrids of analytical and computational skills, typically with science backgrounds. Data scientists are motivated not just to support internal decisions with analytics, but to create new products and processes for customers. In addition, big data and high-performance analytics require new approaches to deciding and acting on the part of executives and decision-makers. The concept of " big data " burst onto the technology and business scene in 2010, and since then has excited many executives with its potential to transform businesses and organizations. The concept refers to data that is either too voluminous, too unstructured, or from too many diverse sources to be managed and analyzed through traditional means. The definition is clearly a relative one that will change over time. At the moment, however, " Too voluminous " typically means databases or data flows in petabytes (1000 terabytes); Google, for example, processes about 24 petabytes per day of data. " Too unstructured " generally means that the data doesn't initially come in numeric form, and isn't easily put into the traditional rows and columns of conventional databases. Examples of big data include a massive amount of online information, including clickstream data from the web, and social media content (tweets, blogs, wall postings, etc.). Big data also incorporates video data from retail and crime/intelligence environments, location data from mobile devices, and data from the rendering of video footage. It includes voice data from call centers or intelligence interventions. In the life sciences, it includes genomic and proteomic data from biological research and medicine.},
author = {Davenport, Thomas H},
booktitle = {International Institute for Analytics},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Davenport - 2012 - The Human Side of Big Data and High-Performance Analytics.pdf:pdf},
institution = {Institute for Analytics},
number = {August},
pages = {1--13},
title = {{The Human Side of Big Data and High-Performance Analytics}},
url = {http://www.datascienceassn.org/sites/default/files/Human Side of Big Data and High-Performance Analytics.pdf},
year = {2012}
}
@article{Sivarajah2017a,
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.},
annote = {From Duplicate 2 (Critical analysis of Big Data challenges and analytical methods - Sivarajah, Uthayasankar; Kamal, Muhammad Mustafa; Irani, Zahir; Weerakkody, Vishanth)

This paper has an approach based on restricting the pool of articles extracted from Scopus by using a complex series of search terms.
This presupposes that you already know they terms that are relevent when reducing the results pool.},
author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
doi = {10.1016/j.jbusres.2016.08.001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sivarajah et al. - 2017 - Critical analysis of Big Data challenges and analytical methods.pdf:pdf;:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sivarajah et al. - 2017 - Critical analysis of Big Data challenges and analytical methods(2).pdf:pdf},
isbn = {01482963 (ISSN)},
issn = {01482963},
journal = {Journal of Business Research},
keywords = {Big Data,Big Data Analytics,Challenges,Methods,Systematic literature review},
mendeley-tags = {Big Data},
number = {August},
pages = {263--286},
publisher = {The Authors},
title = {{Critical analysis of Big Data challenges and analytical methods}},
url = {http://dx.doi.org/10.1016/j.jbusres.2016.08.001},
volume = {70},
year = {2017}
}
@article{Bouillardetal2019,
author = {Bouillard et al},
doi = {10.32942/OSF.IO/9Z63V},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bouillard et al - 2019 - Seeing rare birds where there are none self-rated expertise predicts correct species identification, but also m.pdf:pdf},
journal = {EcoEvoRxiv},
keywords = {Animal Sciences,Biodiversity,Citizen science,Life Sciences,Ornithology,Species identification,biodiversity,twitching},
publisher = {EcoEvoRxiv},
title = {{Seeing rare birds where there are none: self-rated expertise predicts correct species identification, but also more false rarities}},
url = {https://ecoevorxiv.org/9z63v/},
year = {2019}
}
@article{Landry1994,
abstract = {206 Journal of the Lepidopterists' Society ever, some adjustments, both at the time of collecting and of prepa- ration, and in the equipment, are needed because of the small size and fragility of microlepidoptera . Over the years we have tried every different method and variation of ...},
author = {Landry, Jean-Francois and Landry, Bernard},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Landry, Landry - 1994 - A technique for setting and mounting microlepidoptera(2).pdf:pdf},
journal = {Journal of the Lepidopterists Society},
number = {3},
pages = {205--227},
title = {{A technique for setting and mounting microlepidoptera}},
volume = {48},
year = {1994}
}
@article{Bogaert2000,
abstract = {Area-perimeter ratios are often used to quantify 2D shape compactness. Shape compactness is of main importance to evaluate the effect of external disturbance on natural habitats. Reference values (Amin(p), Amax(p), Pmin(a), Pmax(a)) for area (A) and perimeter (P) are defined and proven. The calculation of these reference values is based on the characteristics of the object studied and on pixel geometry. The cases for 4- and 8-connectivity are discussed. Using these references, alternative area-perimeter ratios are composed. Examples are elaborated to illustrate the use of the indices and their performance. {\textcopyright} 2000 Elsevier Science Inc. All rights reserved.},
author = {Bogaert, J. and Rousseau, R. and {Van Hecke}, P. and Impens, I.},
doi = {10.1016/S0096-3003(99)00075-2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bogaert et al. - 2000 - Alternative area-perimeter ratios for measurement of 2D shape compactness of habitats.pdf:pdf},
issn = {00963003},
journal = {Applied Mathematics and Computation},
keywords = {2D compactness,Area-perimeter ratio,Habitat shape,Isoperimetric principle,Landscape ecology,Reference values for area and perimeter},
month = {may},
number = {1},
pages = {71--85},
publisher = {Elsevier Inc.},
title = {{Alternative area-perimeter ratios for measurement of 2D shape compactness of habitats}},
volume = {111},
year = {2000}
}
@article{Amar2004,
abstract = {The design and evaluation of most current information visualization systems descend from an emphasis on a user's ability to "unpack" the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging the analytic gap, propose a framework for design and evaluation of information visualization systems, and demonstrate its use},
author = {Amar, R and Stasko, J},
doi = {10.1109/INFVIS.2004.10},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Amar, Stasko - 2004 - BEST PAPER A knowledge task-based framework for design and evaluation of information visualizations.pdf:pdf},
journal = {Information Visualization},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {143--150},
title = {{BEST PAPER: A knowledge task-based framework for design and evaluation of information visualizations}},
url = {http://ieeexplore.ieee.org/abstract/document/1382902/},
year = {2004}
}
@article{Ceri2018,
abstract = {Statistics and computer science are facing remarkably similar discussions on the role of big data. In this article, I advocate that the computer science community has taken advantage of big data since about five decades, thereby building the main commercial companies of today's computer industry, and specifically I describe the new emphasis on data as the emergence of the so-called Fourth Paradigm. Then, I draw a parallel between the debates on big data occurring within the statistics and computer science community; and finally I advocate for a joint, new and pervasive approach to data science, in which both communities can capitalize on each other's skills.},
author = {Ceri, Stefano},
doi = {10.1016/j.spl.2018.02.019},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ceri - 2018 - On the role of statistics in the era of big data A computer science perspective.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Big data,Fourth paradigm,Paradigm shift in statistics and computer science,Teaching data science,Two cultures},
month = {may},
pages = {68--72},
publisher = {North-Holland},
title = {{On the role of statistics in the era of big data: A computer science perspective}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300646},
volume = {136},
year = {2018}
}
@article{Spoelstra2015,
abstract = {Artificial night-time illumination of natural habitats has increased dramatically over the past few decades. Generally, studies that assess the impact of artificial light on various species in the wild make use of existing illumination and are therefore correlative. Moreover, studies mostly focus on short-term consequences at the individual level, rather than long-term consequences at the population and community level--thereby ignoring possible unknown cascading effects in ecosystems. The recent change to LED lighting has opened up the exciting possibility to use light with a custom spectral composition, thereby potentially reducing the negative impact of artificial light. We describe here a large-scale, ecosystem-wide study where we experimentally illuminate forest-edge habitat with different spectral composition, replicated eight times. Monitoring of species is being performed according to rigid protocols, in part using a citizen-science-based approach, and automated where possible. Simultaneously, we specifically look at alterations in behaviour, such as changes in activity, and daily and seasonal timing. In our set-up, we have so far observed that experimental lights facilitate foraging activity of pipistrelle bats, suppress activity of wood mice and have effects on birds at the community level, which vary with spectral composition. Thus far, we have not observed effects on moth populations, but these and many other effects may surface only after a longer period of time.},
author = {Spoelstra, K. and van Grunsven, R. H. A. and Donners, M. and Gienapp, P. and Huigens, M. E. and Slaterus, R. and Berendse, F. and Visser, M. E. and Veenendaal, E.},
doi = {10.1098/rstb.2014.0129},
isbn = {0962-8436},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1667},
pages = {20140129--20140129},
pmid = {25780241},
title = {{Experimental illumination of natural habitat--an experimental set-up to assess the direct and indirect ecological consequences of artificial light of different spectral composition}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2014.0129},
volume = {370},
year = {2015}
}
@article{Lee2015,
abstract = {Abstract: This paper is about better engineering of cyber-physical systems (CPSs) through better models. Deterministic models have historically proven extremely useful and arguably form the kingpin of the industrial revolution and the digital and information technology revolutions. Key deterministic models that have proven successful include differential equations, synchronous digital logic and single-threaded imperative programs. Cyber-physical systems, however, combine these models in such a way that determinism is not preserved. Two projects show that deterministic CPS models with faithful physical realizations are possible and practical. The ﬁrst project is PRET, which shows that the timing precision of synchronous digital logic can be practically made available at the software level of abstraction. The second project is Ptides (programming temporally-integrated distributed embedded systems), which shows that deterministic models for distributed cyber-physical systems have practical faithful realizations. These projects are existence proofs that deterministic CPS models are possible and practical.},
annote = {Paper in TAMS4CPS.

Addresses two problems; non-determinism of modrls, and the timing issues that are important to physical processes. He's talking about proof-of-concepts, not real, working systems.

Models of time:
"Broman et al. [36] list three requirements for a model of time:
1 The precision with which time is represented should be ﬁnite and should be the same for all observers in a model. Inﬁnite precision (as provided by real numbers) is not practically realizable in computers, and if precisions differ for different observers, then the different observers will not agree on which events are simultaneous.
2 The precision with which time is represented should be independent of the absolute magnitude of the time. In other words, the time origin (the choice for the meaning of time zero) should not affect the precision.
3 The addition of time should be associative. That is, for any three time intervals t1, t2 and t3, (t1 + t2) + t3 = t1 + (t2 + t3).
Lee adds a fourth:
4 Monotonicity: any observer of time in a model that is a sequential process (a sequence of state changes) should observe non-decreasing values of time.
[36 Broman, D.; Greenberg, L.; Lee, E.A.; Masin, M.; Tripakis, S.; Wetter, M. Requirements for Hybrid Cosimulation; Report UCB/EECS-2014-157 (to appear, HSCC 2015), EECS Department, University of California, Berkeley, 2014.]

Then he discusses Superdense Time; within a clock time quantum you allow multiple small steps to occur even when they are sequential, so that within the time quantum it appears that you have achieved parallel effects. Think Newton's Cradle.

PRET:
"The acronym “PRET” stands variously for Precision-Timed processors, Predictable, Repeatable Timing and Performance with Repeatable Timing.

The PRET project at Berkeley set out in 2007 to demonstrate that an ISA [Instruction Set Architecture] that gives programmers a rigorous timing model does not need to sacriﬁce performance [25,26]. The acronym “PRET” stands variously for Precision-Timed processors, Predictable, Repeatable Timing and Performance with Repeatable Timing.

The ﬁrst concrete demonstration of this was the PTARM (Precision Timed ARM) [27], a soft core implemented on an FPGA based on an ARM ISA. PTARM uses a thread-interleaved pipeline to mitigate pipelining effects and an interleaved memory controller [28] to get deterministic latency from a memory hierarchy. The size of the design is nearly the same as comparable 32-bit microprocessors, so there is no particular area penalty to providing programmers with precise control over timing. However, more importantly, the aggregate performance (total useful work done per unit time) actually exceeds that of a comparable conventional pipelined processor, provided that there is enough concurrency in the application to keep four threads busy at all times.
...
PRET machines bring a number of beneﬁts. They enable programs with repeatable timing, which improves assurance that a deployed system will behave in the ﬁeld as it did on the test bench. They improve testability, because fewer timing variations need to be considered. They reduce the need for over-provisioning, because timing requirements are explicit in the programs, and it can be statically determined whether a particular physical realization will meet those requirements; this reduces the system cost and may dramatically reduce energy consumption, enabling, for example, much slower clocking of systems when such slower clocking demonstrably meets the timing requirements. They enable replacement of hardware without requiring recertiﬁcation of the software; if the behavior of the interactions between the software and the physical plant is demonstrably identical when the same program is executed on new hardware, then it becomes possible to certify the software instead of certifying the implementation."
[i.e. they modelled an ARM processor in software, then ran their design on it and demonstrated ther wins.]

PTIDES:
"The Ptides modeling paradigm [addresses distributed component systems in a CPS and] is a variant of a discrete-event (DE) modeling paradigm [48–50]. In a DE model, components interact by sending each other time-stamped messages (called events). [note that a time-stamp comprises a clock time an a process sequence nuber] A key property of DE is that every component processes events in time-stamp order. This property underlies the determinism of the model.
...
Unlike any preceding DE modeling paradigm, Ptides uses superdense time and multiform time and therefore meets all of the requirements and desirable features in Section 5.1. Moreover, Ptides uses DE not for simulation, but rather for the design of deployable systems.



Quotes:
p.4838 "As an intellectual challenge, CPS is about the intersection, not the union, of the physical and the cyber. It combines engineering models and methods from mechanical, environmental, civil, electrical, biomedical, chemical, aeronautical and industrial engineering with the models and methods of computer science. This paper argues that these models and methods do not combine easily and that the consequently CPS constitutes a new engineering discipline that demands its own models and methods.
...
CPS connects strongly to the currently popular terms Internet of Things (IoT), Industry 4.0, the Industrial Internet, Machine-to-Machine (M2M), the Internet of Everything, TSensors (trillion sensors), and the fog (like the cloud, but closer to the ground). All of these reﬂect a vision of a technology that deeply connects our physical world with our information world. In our view, the term “CPS” is more foundational and durable than all of these, because it does not directly reference either implementation approaches (e.g., the “Internet” in IoT) nor particular applications (e.g., “Industry” in Industry 4.0). It focuses instead on the fundamental intellectual problem of conjoining the engineering traditions of the cyber and the physical worlds. One can talk about a “cyber-physical systems theory” in a manner similar to “linear systems theory”."

p.4839 "A model imitates the thing being modeled. A model may serve many purposes. In science, the purpose of a model is generally to yield insight into the thing being modeled and to predict its behavior. In engineering, however, models are often used to design systems that do not yet exist. In this case, the model serves as a speciﬁcation. It is now incumbent on the physical system to imitate the model, rather than the other way around. A C program, for example, gives the design, and the silicon and wires of the physical realization are expected to imitate the behavior speciﬁed by the program."

p.4844 "Deterministic models are valuable. They form the foundation of every engineering discipline. Differential equations model the physical dynamics of electrical, mechanical and many other systems. Synchronous digital logic models electrical circuits and, more recently, concurrent software. Imperative programs model computations performed in a computer.

Cyber-physical systems combine physical dynamics with computational processes, so it would seem that they could leverage the power of these deterministic models. Unfortunately, combinations of these deterministic models for physical dynamics and for computation are nearly always nondeterministic. The principal reason for this is the lack of temporal semantics in programs."

p.4845 :There is a long history of programming languages that include timing constructs (Modula, PEARL (Process and Experiment Automation Realtime Language), Ada, Occam, Real-Time Euclid and Erlang, for example). These improve things by including in the language some of the mechanisms of an RTOS [Real-Time Operating System], which means that a model (a program) is more self-contained. One does not need to combine the semantics of the language with the semantics of a separate and distinct RTOS to interpret the model. These languages, however, also fall far short of yielding a deterministic modeling paradigm. This is, in part, for very practical reasons. The underlying computers do not provide in their instruction set architectures mechanisms for controlling timing, and in fact, the trend has been towards less predictable and controllable timing."

p.4846 "3.2. The Cost of the Missing Temporal Semantics
Although the weaknesses of real-time mechanisms have been known for some time, little progress has been made. Today, real-time systems require considerable over-provisioning if the timing is important. This increases cost and energy consumption and decreases the reliability by increasing the part count.

Perhaps even more importantly, because timing emerges from the implementation, validating the models is not very useful. Engineers are forced instead to validate the implementation. This results in an implement-and-test style of design that dominates embedded systems today. It is hacking, not based on sound engineering principles.

One rather dramatic consequence of these failings is that manufacturers of cyber-physical systems cannot easily replace or update the hardware that is used to execute embedded software. An extreme example of this is found in commercial aircraft. These are safety-critical systems with extensive (and expensive) certiﬁcation requirements. In a ﬂy-by-wire aircraft, the ﬂight control system is realized in software, but it is not enough to certify the software. As a consequence, manufacturers of aircraft (such as Boeing and Airbus) stockpile enough microprocessors to last for the entire production run of a plane. The production run for a typical commercial airplane is, perhaps, 50 years, so towards the end of the production run, the plane will be made with microprocessors that are 50 years old!

What gets certiﬁed is the implementation, not the model. Indeed, with today's models, it has to be that way. It is not enough to correctly execute the C code that implements the ﬂight control system. Two distinct microprocessors that execute the code correctly will exhibit different timing, and therefore, the behavior of the cyber-physical composition will be different. Replacing a microprocessor with a more modern one would require repeating the (expensive) certiﬁcation process.
...
Software abstractions were not created for cyber-physical systems. They were created to run payrolls."

p.4847 "Vendors that are not subject to stringent certiﬁcation requirements may not even attempt to ensure deterministic concurrency. This can be extremely risky, as evidenced by the considerable criticism of Toyota's software that resulted from investigations of possible cases of unintended acceleration [21]. The design style in that software extensively uses unguarded shared variables accessed by concurrent threads. As a result, the programs have many correct behaviors, and given the same inputs, there are too many to test. Although there is no conclusive evidence that ﬂaws in the software caused any unintended acceleration events, Toyota nevertheless suffered searing criticism of their design style [22]."
[21 NASA Engineering and Safety Center. National Highway Trafﬁc Safety Administration Toyota Unintended Acceleration Investigation; Technical Assessment Report, NASA, 2011. Available online: http://www.nhtsa.gov/UA (accessed on 3 December 2014).
22 Koopman, P. A Case Study of Toyota Unintended Acceleration and Software Safety, 2014. Available online: http://betterembsw.blogspot.com/2014/09/a-case-study-of-toyota-unintended.html (accessed on 3 December 2014).]


p.4849 "[Timimg deadlines for when events occur], whether hard or soft, suffer from a practical problem that controlling the order of events becomes difﬁcult. If task A has a deadline of 21 ms and task B has a deadline of 42 ms, there is no assurance (in the model) that A will complete before B. An implementation may provide such assurance, for example by using a deadline-monotonic scheduling policy [10], but even then, such assurances become increasingly difﬁcult with multicore processors and distributed systems. Changing the order of events can have much more dramatic effects on a physical system than perturbing the timing. Consider, for example, if task A turns on a motor and task B turns off the same motor. The order in which these occur yields drastically different behaviors."},
author = {Lee, E A},
doi = {10.3390/s150304837},
journal = {Sensors},
keywords = {Cyber-physical systems,IoT,architecture,automation,business process re-engineering,certification:,complexity,design process,engineering,errors,models,networks,production control,reference architectures,reliability,resilience,robots,software,supply chain,theory},
pages = {4837--4869},
title = {{The past, present and future of cyber-physical systems:  a focus on models}},
volume = {15},
year = {2015}
}
@book{tsichritzis1982data,
author = {{Tsichritzis, Dionysios C and Lochovsky}, Frederick H},
publisher = {Prentice Hall Professional Technical Reference},
title = {{Data models}},
year = {1982}
}
@misc{Kroll2017,
abstract = {Many important decisions historically made by people are now made by computers. Algorithms count votes, approve loan and credit card applications, target citizens or neighborhoods for police scrutiny, select taxpayers for IRS audit, grant or deny immigration visas, and more. The accountability mechanisms and legal standards that govern such decision processes have not kept pace with technology. The tools currently available to policymakers, legislators, and courts were developed to oversee human decisionmakers and often fail when applied to computers instead. For example, how do you judge the intent of a piece of software? Because automated decision systems can return potentially incorrect, unjustified, or unfair results, additional approaches are needed to make such systems accountable and governable. This Article reveals a new technological toolkit to verify that automated decisions comply with key standards of legal fairness. We challenge the dominant position in the legal literature that transparency will solve these problems. Disclosure of source code is often neither necessary (because of alternative techniques from computer science) nor sufficient (because of the issues analyzing code) to demonstrate the fairness of a process. Furthermore, transparency may be undesirable, such as when it discloses private information or permits tax cheats or terrorists to game the systems determining audits or security screening. The central issue is how to assure the interests of citizens, and society as a whole, in making these processes more accountable. This Article argues that technology is creating new opportunities-subtler and more flexible than total transparency-to design decisionmaking algorithms so that they better align with legal and policy objectives. Doing so will improve not only the current governance of automated decisions, but also-in certain cases-the governance of decisionmaking in general. The implicit (or explicit) biases of human decisionmakers can be difficult to find and root out, but we can peer into the "brain" of an algorithm: computational processes and purpose specifications can be declared prior to use and verified afterward. The technological tools introduced in this Article apply widely. They can be used in designing decisionmaking processes from both the private and public sectors, and they can be tailored to verify different characteristics as desired by decisionmakers, regulators, or the public. By forcing a more careful consideration of the effects of decision rules, they also engender policy discussions and closer looks at legal standards. As such, these tools have far-reaching implications throughout law and society. Part I of this Article provides an accessible and concise introduction to foundational computer science techniques that can be used to verify and demonstrate compliance with key standards of legal fairness for automated decisions without revealing key attributes of the decisions or the processes by which the decisions were reached. Part II then describes how these techniques can assure that decisions are made with the key governance attribute of procedural regularity, meaning that decisions are made under an announced set of rules consistently applied in each case. We demonstrate how this approach could be used to redesign and resolve issues with the State Department's diversity visa lottery. In Part III, we go further and explore how other computational techniques can assure that automated decisions preserve fidelity to substantive legal and policy choices. We show how these tools may be used to assure that certain kinds of unjust discrimination are avoided and that automated decision processes behave in ways that comport with the social or legal standards that govern the decision. We also show how automated decisionmaking may even complicate existing doctrines of disparate treatment and disparate impact, and we discuss some recent computer science work on detecting and removing discrimination in algorithms, especially in the context of big data and machine learning.},
author = {Kroll, Joshua A. and Huey, Joanna and Barocas, Solon and Felten, Edward W. and Reidenberg, Joel R. and Robinson, David G. and Yu, Harlan},
booktitle = {University of Pennsylvania Law Review},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kroll et al. - 2017 - Accountable algorithms.pdf:pdf},
issn = {19428537},
number = {3},
pages = {633--705},
title = {{Accountable algorithms}},
url = {https://heinonline.org/HOL/Page?handle=hein.journals/pnlr165{\&}id=648{\&}div={\&}collection=},
volume = {165},
year = {2017}
}
@article{PhilipChen2014,
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. ?? 2014 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1312.4722},
author = {Chen, C.L. P and Zhang, Chun-Yang},
doi = {10.1016/j.ins.2014.01.015},
eprint = {1312.4722},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Zhang - 2014 - Data-intensive applications, challenges, techniques and technologies A survey on Big Data.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Big Data,Cloud computing,Data-intensive computing,Parallel and distributed computing,e-Science},
month = {aug},
pages = {314--347},
pmid = {96027714},
publisher = {Elsevier},
title = {{Data-intensive applications, challenges, techniques and technologies: A survey on Big Data}},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514000346 http://dx.doi.org/10.1016/j.ins.2014.01.015{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/S0020025514000346},
volume = {275},
year = {2014}
}
@article{Popov2016,
abstract = {In this paper we analyze the technology used as a backbone of iota (a cryp- tocurrency for Internet-of-Things industry). This technology naturally succeeds the blockchain technology as its next evolutionary step and comes out with features required for micropayments conducted on a global scale.},
annote = {Document in Contracts.

Attempts to explain the concept of the Tangle, but written from the inside, not the outside.

Blurb version @ Coleman IoT-A {\#}4256},
author = {Popov, S},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Popov - 2016 - The Tangle.pdf:pdf},
journal = {Semantic Scholar},
keywords = {Blockchain,IoT,contracts},
pages = {131},
publisher = {IoT-A},
title = {{The Tangle}},
url = {http://untangled.world/iota-whitepaper-tangle/ https://www.semanticscholar.org/paper/The-tangle-Popov/0ccea9f2a05db0e3b8b54d21b8af9e330b2b5e5b},
year = {2016}
}
@inproceedings{Glavic2014,
abstract = {Data Provenance is information about the origin and creation process of data. Such information is useful for debugging data and transformations, auditing, evaluating the quality of and trust in data, modelling authenticity, and implementing access control for derived data. Provenance has been studied by the database, workflow, and distributed systems communities, but provenance for Big Data - which we refer to as Big Provenance - is a largely unexplored field. This paper reviews existing approaches for large-scale distributed provenance and discusses potential challenges for Big Data benchmarks that aim to incorporate provenance data/management. Furthermore, we will examine how Big Data benchmarking could benefit from different types of provenance information. We argue that provenance can be used for identifying and analyzing performance bottlenecks, to compute performance metrics, and to test a system's ability to exploit commonalities in data and processing. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {Glavic, Boris},
booktitle = {WBDB 2012},
doi = {10.1007/978-3-642-53974-9_7},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Glavic - 2014 - Big data provenance Challenges and implications for benchmarking.pdf:pdf},
isbn = {9783642539732},
issn = {16113349},
keywords = {Benchmarking,Big Data,Data Provenance},
month = {dec},
pages = {72--80},
publisher = {Springer, Berlin, Heidelberg},
title = {{Big data provenance: Challenges and implications for benchmarking}},
url = {http://link.springer.com/10.1007/978-3-642-53974-9{\_}7},
volume = {8163 LNCS},
year = {2014}
}
@book{Woods2006,
address = {Basingstoke},
annote = {Book owned by Carys.


p.175 "Computers and automatons are not stakeholders in the processes in which they participate."

p.184 "There is no neutral in design; we either hobble or support people's natural ability to express forms of expertise.""Complexity is conserved under transformation and translation"[But they may have missed intrinsic {\&} induced complexity (though note Nancy Leveson, 2011). Also, this doesn't address costs, temporal aspects. But note Unilever example; transformation and translation can make it easier to accommodate complexity. Also, I can't find this in the book, although it is attributed to them. in IEEE Spectrum]"Success in complex systems is created through work accomplished by joint cognitive systems (delivered as socio-technical systems), not a given which can be degraded through human limits and error." (p.1.)


p.2 Bounded Rationaltiy Syllogism:
"All cognitive systems are finite (people, machines, or combinations)
All finite cognitive systems in uncertain, changing situations are fallible.
Therefore, machine cognitive systems (and joint systems across people and amhines) are fallible.
The question, then, ... is the development of strategies to handle the trade-offs produced by the need to act in a finite, dynamic, conflicted and uncertain world."


p.18 The Law of Stretched Systems:
"Every system is stretched to operate at its capacity; as soon as there is some improvement, for example, in the form of new technology, it will be exploited to achieve a new intensity and tempo of activity."
[c.f. Rasmussen's drift to disaster]p.20 Law of Fluency:
"Well-adapted work occurs with a facility that belies the difficulty of the demands resolved and the dilemmas balanced."
[So, always look at precursor systems before starting design. Arthur Gardner: “There is alwauys a precursor system”]p.100 Escalation Principle:
"Escalation describes how a situation moves from canonical to non-routine or exceptional. Escalation demonstrates the growing cognitive and co-ordination demands that bring out the penalties of poor support for work."

p.111 Dangers of simplification:
"Multi-threaded work challenges simplification heuristics. These heuristics expose complex systems to new vulnerabilities and failure routes."

p.113 Rasmussens's 2nd observation:
"The operator's job is to make up for holes in the designers' work." [Rasmussen's 1st observation: The operator is the designer's on-site representative.”]p.117 Ingterface: an arbitrary line of demarcation set up n order to apportion the blame for malfunction
S. Kelly-Bootle, 1995, The Computer Contra-Dictionary, 2nd ed. MIT Press

p.120 Table of design benefits belief versus reality:
Better results within same sysem (i.e. substitution) X Transforms practice and roles/co-ordination of people
Frees resources by offloading work X Creates new kinds of work, often at the wrong times
Frees up resources by focussing attention on the right information/answer X More threads to track; males it harder to control attention
Less knowledge required X New knowledge/skills required
Autonomous machine X Interoperation with people is critical to success
Same feedback X New levels and types of feedback are required to support people's new roles
Generic flexibility X Explosion of features, options and modes create new demands, types of errors, and paths toward failure
Reduce human error X Both machines and people are limited in resource and fallible; new problems are associted with co-ordination breakdowns between human and machine agents

p.122 Warnings of impending issues from training:
"They are building a system that takes more time than we have for training people."
"There is more to know - how it works, but especiallyhow to work the system in different situations."
"The most important thing to learn is when to click it off."
"We need more chances to explore how it works and how to use it."
"Well we don't use those features or capability."
"We are handling that issue with a policy."
"We are forced to rely on recipe training much more than anyone likes."
"Wet each of them [a certain number of] basic modes in training, they learn the rest of the system online."

p.123 Design obstinacy/evidence of clumsy design:
"The system performed as designed."
"The incident was due to erratic human behaviour."
"We are training the users so they don't do this again."
"The system is logical in use; some people/organisations/countries just don't understand it."
"The customer's constraints prevented us."
"Other parts of the industry haven't kept up."
"Our next release will be better."

p.125 Robin Murphy's Law:
"Any deployment of robotic systems will fall short of the target level of autonomy, creating or exacerbating a shortfall in mechanisms for co-ordination with human stakeholders."

p.158 Norbert's contrast:
"Artificial agents are literal-minded and disconnected from the world whereas human agents are context-sensitive and have a stake in the outcomes."

p.158 Ackoff's warning on models:
"The optimal solution according to the model is not the optimal solution of a real-world problem unless the model is a perfect representation of the problem, which it never is."

p.173 For systems, the cognitive vacuum does not exist:
"When designers create, represent, or shape some system that interoperates with people, those people learn something about the system and it's operations. They may learn what was not intended."

p.174 Mr. Weasley's rule:
"Never trust anything that can think for itself, especially if you can't see where it keeps its brain." [Highly apposite for devices that can learn]LAWS OF COGNITIVE SYSTEMS AT WORK (p.171-177) (re-phrased with commentary in italics):
1 Laws of adaptation:
• Law of requisite variety Only variety can subsume variety [MAS version]• Context conditioned variability Skill is the ability to adapt behaviour in changing circumstances to pursue goals [?definition of resilience]• Law of stretched systems Every system, once instantiated, is stretched to operate at its capacity. Any improvement or alteration is exploited for a new intensity and tempo of action [But note Rasmussen's diagram - stretched to breaking point]• Law of demands Those demands that make work difficult are likely to make work hard for any JCS, regradless of the composition of human and/or machine agents

• Law of fluency ‘Well'-adapted work is executed with a facility that belies the difficulty of the demands resolved and the dilemmas balanced [but only if there are patterns that can be learnt by experience on the basis of prior training. Novelty is always a problem]• Potential for surprise Following Heracleitus, all JCS over time will be presented with surprises. Resilient JCS are prepared to be surprised; i.e. set up to re-frame and re-conceptualise assessments of how well plans/models/automata fit novel situations and adjust behaviour accordingly [This comment is a major re-phrasing]2 Laws of models:
• Avery's wish: “I yearn for something that shows me an image of the system and its world 10 minutes from now” [re-phrased]• Law of regulation ‘Every good controller must be a model of what it controls': (a) structure, function and dynamics, (b) environmental impacts and expected changes to be compensated [Conant {\&} Ashby]• Ackoff's catch: The optimal solution obtained from a model is not an optimal solution for the probkem, unless the model is a perfect representation of the problem, which it never is.

• The representation effect How a problem is represented in a JCS changes the work intended to solve that problem, either enhancing or degradng performance, or both. [The ‘wicked pronblem' characteristic]• Law of anticipation Change of commands, based on feedback, relies on anticipation, which is the property of being able to adjust future behaviour on the basis of past performnce (i.e. expertise tuned to the future, utilising data about the past). [re-defined; original is not sense]• Law of expectations People's behaviour depends not only on whxt did happen, but also on what could have happened, and especially on what they believe could happen next. [powerful law, that should govern displays]• Cognitive vacuums do not exist When we designers create, represent of shape some experience for others, those people who receive the experience learn something (not necessarily what we intended), form some model or explanation for the experience (which may not be veridical), extract and index some gist (perhaps quite different to what we intended), see some patterns (even if we encoded a different pattern or no pattern at all), pursue some goal (which may not be that which we envisaged in the design) and balance trade-offs (which may not have been visible in design) [another all-emcompassing powerful law/observation]3 Laws of collaboration:
• First law of collaboration It's not collaboration if either you do it all or I do it all

• Second law of collaboration You can't collaborate with another agent (human or device) if you believe it/he/she is incompetent [over-statement; you can have reduced collaboration, based on you being the boss. This statement excludes collaborating with trainees or apprentices]• Co-ordination costs, continually Co-ordinated activity across agents consumes resources and requires ongoing investments and practice [cf Nancy Cooke]• The collaborative variation on the law of regulation Every controller is a model of the target processes and of the other agents that are capable of influencing the target processes and environment, including their collaboration activities; i.e. the model includes the activities, models, capabilities and expectations of the other agents. [re-worded for clarity, and to include AI-based agents, not just humans]• Mr. Weasley's rule Never trust anything that can think for itself (if you can't see where it keepds its brain) [doveriai no proveriai is better]4 Laws of responsibility:
• Computers are not stakeholders in the processes in which they participate. Whatever the artefacts and however autonomous they are under some conditions, it is peiple who create, operate and modify thse artefacts in human-centric systems for human purposes

• Computer artifacts are stand-ins for distant human groups and organisations, including their models of local conditions

• Conflicting goals Multiple and potentially conflicting goals apply to all systems

• Differential responsibility Different parts of a distributed system are differentially responsible for different subsets of goalsthat can interact and conflict.


5 Norbert's contrasts between people and computers:• Bounded rationality syllogism All cognitive systems are finite, and are fallible when system environments change. The issue becomes the development of stratgiesto handle the trade-offs produced by the need to act in a finite, dynamic, conflicted and uncertain world. [c.f. Heracleitus]• Joint systems maxim People and computers are not separate and independent; instead, they are interwoven into a distributed system that performs work in context. Success is achieved by the loint cognitive system; it is not a given that can be degraded through human limits and error.


• Norbert's contrast Artificial agents are literal-minded and disconnected from the world, whereas human agents are context-sensitive and have a stake in outcomes. People and computers start from these opposite poles and will default back to these poles without the continued investment of energy and effort from outside the system.

• Polycentric balance All systems are balancing distant plans with local adaptations to cope with the potential for surprise.

• There is no neutral in JCS design We either hinder or support people's natural ability to express forms of expertise and competence. And each person is different. [Added and extended from elsewhere in the book]


p.180 TABLE 6 Generic requirements to support JCS that work [NB Rasmussen, Vicente, etc.]Support for Observability [recall the User's Questions]
• Integrate data based on a model of the process
• Align data to reeal patterns and relationships in a process [But note visualisation problems in Big Data]
• Provide context around details of interest
• Overcome ‘keyhole' issues; extend peripheral awareness
• Visualise sequence and evolution over time
• Decompose integrations and inferences into sources, process, base evidence
• Provide an answer route for ‘Why is this so?' questions; i.e. make history available [added by MAS]Support for Directability[for application of resources, possible activities, priorities as situations change]
• Anticipation/projection capabilities [i.e. dynamic models and simulations]
• Models of capability
• Policies for adaptation
• Intent communication

Support for directing attention[recall issues of selective and sustained attention] [nothing about how to accomplish these goals]
• Track others' focus of attention
• Judge interruptability of others
• Use Pre-attentive reference

Support for shifting perspectives - alternative points of view
• Seeding - structure and kick-start actions at initial activity of task
• Reminding - provide alternatives as task progresses
• Critiquing - point out alternatives as task comes to a close},
author = {Woods, D D and Hollnagel, E},
isbn = {0-8493-3933-2},
keywords = {IoT,UAV,agents,complexity,errors,expertise,federated control,org. trust,organisational design,quotes,resilience,robots,robustness,situation awareness,socio-tech,systems},
publisher = {Taylor {\&} Francis},
title = {{Joint cognitive systems: patterns in cognitive systems engineering}},
year = {2006}
}
@book{Murrell2012,
abstract = {2nd ed. Extensively updated to reflect the evolution of statistics and computing, the second edition of the bestselling R Graphics comes complete with new packages and new examples. Paul Murrell, widely known as the leading expert on R graphics, has developed an in-depth resource that helps both neophyte and seasoned users master the intricacies of R graphics. New in the Second EditionUpdated information on the core graphics engine, the traditional graphics system, the grid graphics system, and the lattice packageA new chapter on the ggplot2 packageNew chapters on applications and extensions of R Graph. Front Cover; Contents; List of Figures; List of Tables; Preface; Chapter 1. An Introduction to R Graphics; Part I. TRADITIONAL GRAPHICS; Chapter 2. Simple Usage of Traditional Graphics; Chapter 3. Customizing Traditional Graphics; Part II. GRID GRAPHICS; Chapter 4. Trellis Graphics: The lattice Package; Chapter 5. The Grammar of Graphics: The ggplot2 Package; Chapter 6. The grid Graphics Model; Chapter 7. The grid Graphics Object Model; Chapter 8. Developing New Graphics Functions and Objects; Part III. THE GRAPHICS ENGINE; Chapter 9. Graphics Formats; Chapter 10. Graphical Parameters. Part IV. GRAPHICS PACKAGESChapter 11. Graphics Extensions; Chapter 12. Plot Extensions; Chapter 13. Graphics for Categorical Data; Chapter 14. Maps; Chapter 15. Node-and-edge Graphs; Chapter 16. 3D Graphics; Chapter 17. Dynamic and Interactive Graphics; Chapter 18. Importing Graphics; Chapter 19. Combining Graphics Systems; Bibliography; Back Cover.},
author = {Murrell, Paul.},
isbn = {1439831777},
pages = {536},
publisher = {CRC Press},
title = {{R graphics}},
year = {2012}
}
@misc{Karkouch2016,
abstract = {In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.},
author = {Karkouch, Aimad and Mousannif, Hajar and {Al Moatassime}, Hassan and Noel, Thomas},
booktitle = {Journal of Network and Computer Applications},
doi = {10.1016/j.jnca.2016.08.002},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Karkouch et al. - 2016 - Data quality in internet of things A state-of-the-art survey.pdf:pdf},
isbn = {9781509004782},
issn = {10958592},
keywords = {Big Data,Data cleaning,Data quality,Internet of things,Outlier detection},
mendeley-tags = {Big Data},
month = {sep},
pages = {57--81},
publisher = {Academic Press},
title = {{Data quality in internet of things: A state-of-the-art survey}},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301564},
volume = {73},
year = {2016}
}
@article{Jain1999,
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.1881v2},
author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
doi = {10.1145/331499.331504},
eprint = {arXiv:1101.1881v2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {sep},
number = {3},
pages = {264--323},
pmid = {17707831},
title = {{Data clustering: a review}},
url = {http://portal.acm.org/citation.cfm?doid=331499.331504},
volume = {31},
year = {1999}
}
@article{Drummond2018,
abstract = {AbstractReproducible research, a growing movement within many scientific fields, including machine learning, would require the code, used to generate the experimental results, be published along with any paper. Probably the most compelling argument for this is that it is simply following good scientific practice, established over the years by the greats of science. The implication is that failure to follow such a practice is unscientific, not a label any machine learning researchers would like to carry. It is further claimed that misconduct is causing a growing crisis of confidence in science. That, without this practice being enforced, science would inevitably fall into disrepute. This viewpoint is becoming ubiquitous but here I offer a differing opinion. I argue that far from being central to science, what is being promulgated is a narrow interpretation of how science works. I contend that the consequences are somewhat overstated. I would also contend that the effort necessary to meet the movement's aim...},
author = {Drummond, Chris},
doi = {10.1080/0952813X.2017.1413140},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Drummond - 2018 - Reproducible research a minority opinion.pdf:pdf},
issn = {13623079},
journal = {Journal of Experimental and Theoretical Artificial Intelligence},
keywords = {machine learning,reproducible research,scientific communication,scientific evaluation},
month = {jan},
number = {1},
pages = {1--11},
publisher = {Taylor {\&} Francis},
title = {{Reproducible research: a minority opinion}},
url = {https://www.tandfonline.com/doi/full/10.1080/0952813X.2017.1413140},
volume = {30},
year = {2018}
}
@inproceedings{palmer2004deepprocess,
author = {Palmer, P J and Williams, D J},
booktitle = {Proceedings of the 5th International Conference on Integrated Design and Manufacturing in Mechanical Engineering},
isbn = {1-85790-129-0},
organization = {University of Bath, UK},
pages = {71 / 6},
title = {{Deep Design for Component and Process}},
year = {2004}
}
@techreport{Hayhow2016,
abstract = {England's natural heritage includes a range of special habitats, each home to rich and diverse flora and fauna. Of particular interest are the remnant lowland heathlands in the south of England, and the wide open blanket bogs of the Pennines. Broadleaved and coniferous woodland habitats, including most of the UK's lowland and wet woodland, support around a quarter of our breeding bird and butterfly species, and all of our bat species. England's varied urban green spaces provide refuge to species that may have lost significant semi-natural habitat elsewhere. Yet all of this diversity is contained in the one-third of the land area not dominated by arable farmland or improved grassland. The history of wildlife recording in England over the last three centuries means that our knowledge of the species present is among the best in the world. The position of England on the edge of continental Europe, and our relatively wet and mild climate, has resulted in a distinctive mix of species. It also means that as species' ranges shift north as the climate warms, we may increasingly see new species colonising 1},
author = {Hayhow, D.B. and Burns, F. and Eaton, M.A. and {Al Fulaij}, N. and August, T.A. and Babey, L. and Bacon, L. and Bingham, C. and Boswell, J. and Boughey, K.L. and Brereton, T. and Brookman, E. and Brooks, D.R. and Bullock, D.J. and Burke, O. and Collis, M. and Corbet, L. and Cornish, N. and {De Massimi}, S. and Dens, R.D.F. and Eaton, M.A.},
booktitle = {The State of Nature partnership},
doi = {papers3://publication/uuid/4867C02F-DA69-4049-99B2-56D78259289D},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hayhow et al. - 2016 - State of Nature 2016.pdf:pdf},
institution = {Royal Society for the Protection of Birds},
isbn = {0003-0805},
issn = {0010-9312},
pages = {1--87},
title = {{State of Nature 2016}},
url = {https://www.rspb.org.uk/globalassets/downloads/documents/conservation-projects/state-of-nature/state-of-nature-uk-report-2016.pdf},
year = {2016}
}
@article{Trouwborst2020,
abstract = {Abstract Free-ranging domestic cats Felis catus, from owned pets to feral cats, impact biodiversity through predation, fear effects, competition, disease and hybridization. Scientific knowledge regarding these impacts has recently increased, making it timely to assess the role of nature conservation legislation in this connection. We do so with particular regard to the obligations of governments around the world under international wildlife law. First, we provide an overview of current knowledge, based on a literature review, concerning the ways in which domestic cats impact wildlife; the resulting effects on native species? populations and ecosystems; and available strategies for addressing these issues. In light of this knowledge, using standard legal research methodology, we then identify and interpret relevant legal instruments, with a particular focus on international wildlife treaties. Lastly, we identify and assess factors that may influence the implementation of relevant obligations. The outcomes of this analysis indicate that numerous legal obligations of relevance to free-ranging domestic cats already apply under global treaties such as the Convention on Biological Diversity, Convention on Migratory Species and World Heritage Convention, and a range of regional legal instruments for biodiversity conservation. Of particular significance are obligations concerning (a) invasive alien species; (b) protected areas and (c) protected species. Many national authorities around the world are currently required, under international law, to adopt and implement policies aimed at preventing, reducing or eliminating the biodiversity impacts of free-ranging domestic cats, in particular by (a) removing feral and other unowned cats from the landscape to the greatest extent possible and (b) restricting the outdoor access of owned cats. Factors that can influence or impair the application of these obligations include considerations of feasibility, scientific uncertainty, the interests of cat owners and the (perceived) interests of domestic cats themselves. Even if such factors may to some extent explain why many authorities have hitherto failed to take effective action to address the threats posed by free-ranging domestic cats, from a legal perspective these factors provide little ground for justifying non-compliance with international wildlife law. A free Plain Language Summary can be found within the Supporting Information of this article.},
author = {Trouwborst, Arie and McCormack, Phillipa C. and {Mart{\'{i}}nez Camacho}, Elvira},
doi = {10.1002/pan3.10073},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Trouwborst, McCormack, Mart{\'{i}}nez Camacho - 2020 - Domestic cats and their impacts on biodiversity A blind spot in the application of nat.pdf:pdf},
issn = {2575-8314},
journal = {People and Nature},
keywords = {Convention on Biological Diversity,Convention on Migratory Species,domestic cat Felis catus,feral cat,international law,invasive alien species,nature conservation law,protected areas},
month = {mar},
number = {1},
pages = {235--250},
publisher = {Wiley},
title = {{Domestic cats and their impacts on biodiversity: A blind spot in the application of nature conservation law}},
volume = {2},
year = {2020}
}
@misc{Microsoft2013,
abstract = {Big data absolutely has the potential to change the way governments, organizations, and academic institutions conduct business and make discoveries, and its likely to change how everyone lives their day-to-day lives,” said Susan Hauser, corporate vice president of Microsoft's Enterprise and Partner Group.},
author = {Microsoft},
booktitle = {Microsoft News Center},
pages = {1--35},
title = {{The Big Bang: How the Big Data Explosion Is Changing the World}},
url = {https://news.microsoft.com/2013/02/11/the-big-bang-how-the-big-data-explosion-is-changing-the-world/},
year = {2013}
}
@misc{RCoreTeam2017a,
abstract = {This vignette introduces the data.table syntax, its general form, how to subset rows, select and compute on columns and perform aggregations by group. Familiarity with data.frame data structure from base R is useful, but not essential to follow this vignette.},
author = {{R Core Team}},
title = {{Data analysis using data.table}},
url = {https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html},
year = {2017}
}
@book{Hollnagel2005,
address = {Boca Raton, Florida},
annote = {See other book, as well. Essentially, this is a primer on HFI, re-arranged for co-agency. Reads mainly like Hollnagel; he's getting things off his chest. Makes a number of assertions about complexity, but he seems to treat it as 'extremely com-plicated'; no, he's wrong. It's more complex than that.

Refers to Bainbridge Ironies of automation

"Complexity is conserved under transformation and translation"[But they may have missed intrinsic {\&} induced complexity (though note Nancy Leveson, 2011). Also, this doesn't address costs, temporal aspects. But note Unilever example; transformation and translation can make it easier to accommodate complexity. Also, I can't find this in the book, although it is attributed to them. in IEEE Spectrum. This is probably the one that has it.]p.9 Useful quote:
"Since no system has ever built itself, since few systems operate by themselves, and since no systems maintain themselves, the search for a human in the path of failure is bound to succeed. If not found at the sharp end - as a 'human error' or unsafe act - it can usually be found a few steps back. The assumption that humans have failed therefore always vindicates itself."

p.22 definition of a JCS:
A Joint Cognitive System is a system, comprising humans and technology, that can modify its behaviour on the basis of experience so as to achieve anti-entropic ends. 
NB: bit between the commas was added by me. Definition without this is for a Cognitive system.
p.45 Consider an operator at a computer, which controls a process:
Hermeneutic relation characterises the situation where the artefact serves as an interpreter for the operator and takes care of all comminucation between the operator and the process. The artefact must be considered as part of the process, adding variety to the process variety, for the operator to understand.
Embodiment relation characterises the situation where the artefact serves as an extension of the body and as an amplifirer of the operator's capabilities. The requirsite variety of the operator is supplemented, thus providing better control of the process.
(this is a subtlety which is unnecessary, I think)p.80 Coping strategiesOverload:
Omission/non-processing
Reduced precision
Queuing
Filtering - ignoring some classes of information
Cutting categories - simplifying class categories
Decentralisation - passing on work, calling in assistance
Escape/abandonment
Underload:
Extrapolation
Frequence gambling - assessment based on past frequencies of occurrence.
Similarity matching to previous experience
Trial and error - random, or perhaps system response testing
Laissez-faire - just do what others do

Introduces COCOm model and its extension, ECOM.},
author = {Hollnagel, E and Woods, D D},
isbn = {0-8493-2821-1},
keywords = {UAV,agents,complexity,errors,expertise,federated control,org. trust,organisational design,quotes,resilience,robots,robustness,situation awareness,socio-tech,systems},
publisher = {Taylor {\&} Francis},
title = {{Joint cognitive systems:  foundations of cognitive systems engineering}},
year = {2005}
}
@article{Gentleman2007,
abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents—including figures, tables, and so on—can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept o...},
author = {Gentleman, Robert and {Temple Lang}, Duncan},
doi = {10.1198/106186007X178663},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gentleman, Temple Lang - 2007 - Statistical Analyses and Reproducible Research.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Big Data,Compendium,Dynamic documents,Literate programming,Markup language,Perl,Python,R},
mendeley-tags = {Big Data},
month = {mar},
number = {1},
pages = {1--23},
publisher = {Taylor {\&} Francis},
title = {{Statistical Analyses and Reproducible Research}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186007X178663},
volume = {16},
year = {2007}
}
@article{Aria2017,
annote = {Lookslike a good place to start....},
author = {Aria, Massimo and Cuccurullo, Corrado},
doi = {10.1016/j.joi.2017.08.007},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Aria, Cuccurullo - 2017 - bibliometrix An R-tool for comprehensive science mapping analysis.pdf:pdf},
issn = {17511577},
journal = {Journal of Informetrics},
keywords = {Big Data,Software},
mendeley-tags = {Big Data,Software},
month = {nov},
number = {4},
pages = {959--975},
title = {{bibliometrix : An R-tool for comprehensive science mapping analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1751157717300500},
volume = {11},
year = {2017}
}
@misc{OpenRefine2018,
abstract = {Open source tool for working with messy data.},
author = {OpenRefine},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{OpenRefine http://openrefine.org/}},
url = {http://openrefine.org/},
urldate = {2018-03-12},
year = {2018}
}
@article{Ritchey2017,
abstract = {As a conceptual modelling method, General Morphological Analysis (GMA) is especially interesting and useful for treating what have come to be known as "wicked problems". This is best done in multi-stakeholder workshops facilitated by a professional morphologist. This article discusses the art and science of organising and facilitating such GMA workshops, from its planning phase and through the entire process. It is based the experience of 100+ projects carried out over the last 25 years.},
author = {Ritchey, Tom},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 2017 - Facilitating GMA Workshops for Modelling Wicked Problems.pdf:pdf},
issn = {2001-2241},
journal = {Acta Morphologica Generalis},
keywords = {facilitation,general morphological analysis,modelling workshops,multi-stakeholder groups},
number = {2},
title = {{Facilitating GMA Workshops for Modelling Wicked Problems}},
url = {http://www.amg.swemorph.com/pdf/amg-6-2-2017.pdf},
volume = {6},
year = {2017}
}
@misc{Palmer2017,
abstract = {These files are sufficient for the diagrams to be replicated using the open source Gephi tool or similar. A file of extension .gephi is included along with two .csv files representing the nodes and edges respectively. The .gephi file will also preserve the formatting of size and colours, while the.csv files should be compatible with almost any other network visualisation software. The linked paper provides a full description of the process used.},
author = {Palmer, Paul and Siemieniuch, Carys and Sinclair, Murray and Henshaw, Michael},
doi = {10.17028/RD.LBORO.5082088.V1},
month = {jan},
title = {{Source Network Data}},
year = {2017}
}
@inproceedings{Jaffal2016,
abstract = {This paper presents a new method for automatically extracting smartphone users' contextual behaviors from the digital traces collected during their interactions with their devices. Our goal is in particular to understand the impact of users' context (e.g., location, time, environment, etc.) on the applications they run on their smartphones. We propose a methodology to analyze digital traces and to automatically identify the significant information that characterizes users' behaviors. In earlier work, we have used Formal Concept Analysis and Galois lattices to extract relevant knowledge from heterogeneous and complex contextual data; however, the interpretation of the obtained Galois lattices was performed manually. In this article, we aim at automating this interpretation process, through the provision of original metrics. Therefore our methodology returns relevant information without requiring any expertise in data analysis. We illustrate our contribution on real data collected from volunteer users.},
author = {Jaffal, Ali and {Le Grand}, Benedicte},
booktitle = {2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS)},
doi = {10.1109/RCIS.2016.7549334},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jaffal, Le Grand - 2016 - Towards an automatic extraction of smartphone users' contextual behaviors.pdf:pdf},
isbn = {978-1-4799-8710-8},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Towards an automatic extraction of smartphone users' contextual behaviors}},
url = {http://ieeexplore.ieee.org/document/7549334/},
year = {2016}
}
@book{Scott2015,
abstract = {Written to convey an intuitive feel for both theory and practice, its main objective is to illustrate what a powerful tool density estimation can be when used not only with univariate and bivariate data but also in the higher dimensions of trivariate and quadrivariate information. Major concepts are presented in the context of a histogram in order to simplify the treatment of advanced estimators. Features 12 four-color plates, numerous graphic illustrations as well as a multitude of problems and solutions.},
address = {London, UK},
author = {Scott, David W.},
doi = {10.1002/9781118575574},
isbn = {9781118575574},
issn = {09641998},
pages = {360},
pmid = {24643863},
publisher = {John Wiley {\&} Sons},
title = {{Multivariate density estimation: Theory, practice, and visualization: Second edition}},
year = {2015}
}
@article{Bainbridge2014,
abstract = {Practical examples from Scottish government.},
annote = {id: 1; issn: print 00218901; publication{\_}type: full{\_}text},
author = {Bainbridge, Ian},
doi = {10.1111/1365-2664.12294},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bainbridge - 2014 - How can ecologists make conservation policy more evidence based Ideas and examples from a devolved perspective.pdf:pdf},
isbn = {1365-2664},
issn = {13652664},
journal = {Journal of Applied Ecology},
keywords = {Conservation science,Knowledge exchange,Science-policy interface,Scientific evidence},
number = {5},
pages = {1153--1158},
title = {{How can ecologists make conservation policy more evidence based? Ideas and examples from a devolved perspective}},
url = {http://dx.doi.org/10.1111/1365-2664.12294},
volume = {51},
year = {2014}
}
@article{Drews2018,
abstract = {The enduring scientific debate about economic growth versus the environment has recently received new impetus. Nonetheless, there is virtually unanimous support for growth in politics. This may partly be due to an assumed social consensus about the desirability of growth. Here we examine public perceptions relevant to the growth debate by using data from a large number of representative surveys conducted in Europe and the US. The main findings are: (i) a relative majority of respondents seem to believe that economic growth and environmental protection are compatible, even though a fraction of the population might have unstable opinions; (ii) when people have to choose, environmental protection is prioritized in most surveys and countries; and (iii) the public has limited factual knowledge of relevant concepts and data, such as the meaning of economic growth and past GDP growth rates. These findings are discussed and further qualified. We highlight the importance of methodological aspects such as question wording and format in the interpretation of the results and draw implications for political debate and future research on economic growth and the environment.},
author = {Drews, Stefan and Antal, Mikl{\'{o}}s and van den Bergh, Jeroen C.J.M.},
doi = {10.1016/J.ECOLECON.2017.11.006},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Drews, Antal, van den Bergh - 2018 - Challenges in Assessing Public Opinion on Economic Growth Versus Environment Considering European a.pdf:pdf},
issn = {0921-8009},
journal = {Ecological Economics},
month = {apr},
pages = {265--272},
publisher = {Elsevier},
title = {{Challenges in Assessing Public Opinion on Economic Growth Versus Environment: Considering European and US Data}},
url = {https://www.sciencedirect.com/science/article/pii/S092180091730472X},
volume = {146},
year = {2018}
}
@book{Layton2015,
abstract = {Python has grown to become one of the central languages in data mining offering both a general programming language and libraries specifically targeted numerical computations. This book is continuously being written and grew out of course given at the Technical University of Denmark.},
author = {Layton, Robert},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Layton - 2015 - Learning Data Mining with Python.pdf:pdf},
isbn = {9781784396053},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {307},
publisher = {Technical University of Denmark},
title = {{Learning Data Mining with Python}},
url = {http://www2.imm.dtu.dk/pubdb/views/edoc{\_}download.php/6814/pdf/imm6814.pdf},
year = {2015}
}
@techreport{Mohan2018,
abstract = {This paper presents a unified approach for recovering causal and probabilistic queries using graphical models given missing (or incomplete) data. To this end, we develop a general algorithm that can recover conditional probability distributions and conditional causal effects in semi-Markovian models.},
author = {Mohan, Karthika and Pearl, Judea},
booktitle = {Proceedings of Machine Learning Research},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mohan, Pearl - 2018 - Consistent Estimation given Data that are Missing Not At Random.pdf:pdf},
keywords = {Causal Bayesian Network,Missing Data,Missing Not At Random},
pages = {284--295},
title = {{Consistent Estimation given Data that are Missing Not At Random}},
volume = {72},
year = {2018}
}
@book{Woodcock1996,
abstract = {Formal methods require a soundly based specification language. Until now the emphasis in the Z literature has been on the use of Z as a specification language. In this arena, use of Z is extensive and has been fostered by its many positive aspects, including the importance that has been placed on the careful merging of text and formal material. The construction of a clear specification is the cornerstone of any formal development and—as the authors of the current book make clear—sometimes there is little incentive to go further with formalism than to provide such a specification},
author = {Woodcock, Jim and Davies, Jim},
doi = {10.1145/1592434.1592436},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Woodcock, Davies - 1996 - Using Z Specification, Refinement and Proof.pdf:pdf},
isbn = {0139484728},
issn = {03600300},
pmid = {310},
publisher = {International Series in Computer Science. Prentice Hall},
title = {{Using Z: Specification, Refinement and Proof}},
url = {http://www.cs.cmu.edu/{~}15819/zedbook.pdf},
year = {1996}
}
@article{Palmer2016,
abstract = {This paper is a retrospective analysis describing the development of a custom tool to organise data snippets derived from a substantial body of information, and a summary of the insights that this means of analysis provided in a very short time scale. The creation of data driven visualisations are of particular interest as they uncovered more cross-domain aspects of Cyber-Physical Systems projects than expert opinion had anticipated. These findings will be discussed fully in a second paper.The focus here is the development of the “Vulture” data scavenging tool using Open Source software as system components to create a custom application to serve the data collection and analysis requirements of a REA (Rapid Evidence Assessment) work-package within an EU funded project, Road2CPS.},
author = {Palmer, P J and Sinclair, M A and Siemieniuch, C E and {De Henshaw}, M J},
doi = {10.1002/j.2334-5837.2016.00164.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Palmer et al. - 2016 - A practical example of a software factory building a custom application for analysing EU Cyber Physical System (C.pdf:pdf},
issn = {2334-5837},
journal = {INCOSE International Symposium},
month = {jul},
number = {1},
pages = {336--351},
publisher = {Edinburgh},
title = {{A practical example of a software factory: building a custom application for analysing EU Cyber Physical System (CPS) projects using Open Source software components}},
url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00164.x},
volume = {26},
year = {2016}
}
@misc{Piwek2015,
abstract = {The idea behind Tufte in R is to use R - the most powerful open-source statistical programming language - to replicate excellent visualisation practices developed by Edward Tufte. It's not a novel approach - there are plenty of excellent R functions and related packages wrote by people who have much more expertise in programming than myself. I simply collect those resources in one place in an accessible and replicable format, adding a few bits of my own coding discoveries.},
author = {Piwek, Lukasz},
title = {{Tufte in R}},
url = {http://motioninsocial.com/tufte/},
urldate = {2020-09-22},
year = {2015}
}
@article{Wolch2014,
abstract = {Urban green space, such as parks, forests, green roofs, streams, and community gardens, provides critical ecosystem services. Green space also promotes physical activity, psychological well-being, and the general public health of urban residents. This paper reviews the Anglo-American literature on urban green space, especially parks, and compares efforts to green US and Chinese cities. Most studies reveal that the distribution of such space often disproportionately benefits predominantly White and more affluent communities. Access to green space is therefore increasingly recognized as an environmental justice issue. Many US cities have implemented strategies to increase the supply of urban green space, especially in park-poor neighborhoods. Strategies include greening of remnant urban land and reuse of obsolete or underutilized transportation infrastructure. Similar strategies are being employed in Chinese cities where there is more state control of land supply but similar market incentives for urban greening. In both contexts, however, urban green space strategies may be paradoxical: while the creation of new green space to address environmental justice problems can make neighborhoods healthier and more esthetically attractive, it also can increase housing costs and property values. Ultimately, this can lead to gentrification and a displacement of the very residents the green space strategies were designed to benefit. Urban planners, designers, and ecologists, therefore, need to focus on urban green space strategies that are 'just green enough' and that explicitly protect social as well as ecological sustainability. {\textcopyright} 2014 Elsevier B.V.},
author = {Wolch, Jennifer R. and Byrne, Jason and Newell, Joshua P.},
doi = {10.1016/j.landurbplan.2014.01.017},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wolch, Byrne, Newell - 2014 - Urban green space, public health, and environmental justice The challenge of making cities 'just green eno.pdf:pdf},
issn = {01692046},
journal = {Landscape and Urban Planning},
keywords = {Ecosystem services,Environmental justice,Gentrification,Human health,Planning strategies,Urban green spaces},
pages = {234--244},
publisher = {Elsevier},
title = {{Urban green space, public health, and environmental justice: The challenge of making cities 'just green enough'}},
volume = {125},
year = {2014}
}
@article{Daim2006,
abstract = {It is rather difficult to forecast emerging technologies as there is no historical data available. In such cases, the use of bibliometrics and patent analysis have provided useful data. This paper presents the forecasts for three emerging technology areas by integrating the use of bibliometrics and patent analysis into well-known technology forecasting tools such as scenario planning, growth curves and analogies. System dynamics is also used to be able to model the dynamic ecosystem of the technologies and their diffusion. Technologies being forecasted are fuel cell, food safety and optical storage technologies. Results from these three applications help us to validate the proposed methods as appropriate tools to forecast emerging technologies. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Daim, Tugrul U. and Rueda, Guillermo and Martin, Hilary and Gerdsri, Pisek},
doi = {10.1016/j.techfore.2006.04.004},
isbn = {0040-1625},
issn = {00401625},
journal = {Technological Forecasting and Social Change},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {oct},
number = {8},
pages = {981--1012},
pmid = {344},
title = {{Forecasting emerging technologies: Use of bibliometrics and patent analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0040162506001168},
volume = {73},
year = {2006}
}
@misc{Allaire2019,
abstract = {Under the hood, LaTeX templates are used to ensure that documents conform precisely to submission standards. At the same time, composition and formatting can be done using lightweight markdown syntax, and R code and its output can be seamlessly included using knitr. Using rticles requires the prerequisites described below. You can get most of these automatically by installing the latest release of RStudio (instructions for using rticles without RStudio are also provided).},
author = {Allaire, J J and Xie, Yihui and {R Foundation} and Wickham, Hadley},
title = {{rticles: Article Formats for R Markdown}},
year = {2019}
}
@article{Daas2015,
abstract = {More and more data are being produced by an increasing number of electronic devices physically surrounding us and on the internet. The large amount of data and the high frequency at which they are produced have resulted in the introduction of the term ‘Big Data'. Because these data reflect many different aspects of our daily lives and because of their abundance and availability, Big Data sources are very interesting from an official statistics point of view. This article discusses the exploration of both opportunities and challenges for official statistics associated with the application of Big Data. Experiences gained with analyses of large amounts of Dutch traffic loop detection records and Dutch social media messages are described to illustrate the topics characteristic of the statistical analysis and use of Big Data.},
annote = {Desktop computing for Big Data.},
author = {Daas, Piet J.H. and Puts, Marco J and Buelens, Bart and van den Hurk, Paul A.M.},
doi = {10.1515/JOS-2015-0016},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Daas et al. - 2015 - Big data as a source for official statistics.pdf:pdf},
isbn = {20017367},
issn = {0282423X},
journal = {Journal of Official Statistics},
keywords = {Large data sets,Social media,Traffic data},
number = {2},
pages = {249--262},
title = {{Big data as a source for official statistics}},
url = {http://dx.doi.org/10.1515/JOS-2015-0016},
volume = {31},
year = {2015}
}
@misc{Crow2013,
abstract = {Drake is a text-based command line data workflow tool that organizes command execution around data and its dependencies. Data processing steps are defined along with their inputs and outputs. It automatically resolves dependencies and provides a rich set of options for controlling the workflow. It supports multiple inputs and outputs and has HDFS support built-in.},
author = {Crow, Aaron},
publisher = {Factual, Inc.},
title = {{Drake https://github.com/Factual/drake}},
url = {https://github.com/Factual/drake},
year = {2013}
}
@techreport{Winskel2010,
abstract = {Syllabus • Mathematical argument: Basic mathematical notation and argument, in-cluding proof by contradiction, mathematical induction and its variants. • Sets and logic: Subsets of a fixed set as a Boolean algebra. Venn diagrams. Propositional logic and its models. Validity, entailment, and equivalence of boolean propositions. Truth tables. Structural induction. Simplification of boolean propositions and set expressions. • Relations and functions: Product of sets. Relations, functions and partial functions. Composition and identity relations. Injective, surjective and bijective functions. Direct and inverse image of a set under a relation. Equivalence relations and partitions. Directed graphs and partial orders. Size of sets, especially countability. Cantor's diagonal argument to show the reals are uncountable. • Constructions on sets: Russell's paradox. Basic sets, comprehension, in-dexed sets, unions, intersections, products, disjoint unions, powersets. Characteristic functions. Sets of functions. Lambda notation for func-tions. Cantor's diagonal argument to show powerset strictly increases size. An informal presentation of the axioms of Zermelo-Fraenkel set theory and the axiom of choice. • Inductive definitions: Using rules to define sets. Reasoning principles: rule induction and its instances; induction on derivations. Applications, including transitive closure of a relation. Inductive definitions as least fixed points. Tarski's fixed point theorem for monotonic functions on a powerset. Maximum fixed points and coinduction.},
author = {Winskel, Glynn},
booktitle = {Book},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Winskel - 2010 - Set Theory for Computer Science.pdf:pdf},
title = {{Set Theory for Computer Science}},
year = {2010}
}
@inproceedings{webb2003foresightboards,
annote = {From Duplicate 2 (Foresight Vehicle: Physical Media for Automotive Multiplex Networks Implemented on Large Area Flexible Printed Circuit Boards - Webb, D P; Cottrill, M C W; Jaggernauth, W A; West, A A; Palmer, P J; Conway, P P)

Invited participation in UK/DTI delegation at world leading Society of Automotive Engineers (SAE) Congress in Detroit, 2002. Paper was selected by an SAE committee to appear in Transactions of the SAE. (Reference Dr Graham Farmer, GTS Flexible Materials Gwent.)
Unmapped Imported Data : Isbn = ISBN 0-7680-1291-0},
author = {Webb, D P and Cottrill, M C W and Jaggernauth, W A and West, A A and Palmer, P J and Conway, P P},
booktitle = {SAE Transactions 2002: Journal of Passenger Cars - Electronic and Electrical Systems, Paper No 2002-01-1130},
issn = {0148-7191},
month = {mar},
organization = {Detroit, USA},
pages = {5},
title = {{Foresight Vehicle: Multiplex Networks Implemented on Large Area Flexible Printed Circuit Boards}},
url = {http://www.sae.org/technical/papers/2002-01-1130},
year = {2002}
}
@misc{IBM2018,
abstract = {Artificial Intelligence (AI), mobile, social and Internet of Things (IoT) are driving data complexity, new forms and sources of data. Big data analytics is the use of advanced analytic techniques against very large, diverse data sets that include structured, semi-structured and unstructured data, from different sources, and in different sizes from terabytes to zettabytes.

Big data is a term applied to data sets whose size or type is beyond the ability of traditional relational databases to capture, manage, and process the data with low-latency. And it has one or more of the following characteristics – high volume, high velocity, or high variety. Big data comes from sensors, devices, video/audio, networks, log files, transactional applications, web, and social media - much of it generated in real time and in a very large scale.

Analyzing big data allows analysts, researchers, and business users to make better and faster decisions using data that was previously inaccessible or unusable. Using advanced analytics techniques such as text analytics, machine learning, predictive analytics, data mining, statistics, and natural language processing, businesses can analyze previously untapped data sources independent or together with their existing enterprise data to gain new insights resulting in better and faster decisions.},
author = {IBM},
booktitle = {IBM Website.},
title = {{Big Data Analytics | IBM Analytics}},
url = {https://www.ibm.com/analytics/hadoop/big-data-analytics},
urldate = {2018-08-14},
year = {2018}
}
@article{Muller2005,
abstract = {Real world data often has discrepancies in structure and content. Traditional methods for "cleaning" the data involve many iterations of time-consuming "data quality" analysis to and discrepancies, and long-running transformations to fix them. This process requires users to endure long waits and often write complex transformation programs. We present an interactive framework for data cleaning that tightly integrates transformation and discrepancy detection. Users gradually build transformations by adding or undoing transforms, in a intuitive, graphical manner through a spreadsheet-like interface; the e ect of a transform is shown at once on records visible on screen. Discrepancy detection is done incrementally in the background on the current transformed version of data, and discrepancies are flagged as they are found. This interactive combination of discrepancy detection and transformation allows users to gradually develop transformations as discrepancies are found, and clean the data without having to write complex programs or endure long waits. Balancing the goals of power, ease of specifcation, and interactive application, we develop a set of transforms that can be used for transformations within data records as well as for higher-order transformations. We also study the compilation of sequences of transforms into optimized programs.},
author = {M{\"{u}}ller, Heiko and Freytag, Johann-Christoph},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/M{\"{u}}ller, Freytag - 2005 - Problems, Methods, and Challenges in Comprehensive Data Cleansing.pdf:pdf},
journal = {Professoren des Inst. F{\"{u}}r Informatik},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {1--23},
title = {{Problems, Methods, and Challenges in Comprehensive Data Cleansing}},
url = {http://www.dbis.informatik.hu-berlin.de/fileadmin/research/papers/techreports/2003-hub{\_}ib{\_}164-mueller.pdf},
year = {2005}
}
@inproceedings{teh2003re-engineeringapplications,
author = {Teh, N J and Jones, M D and Bevan, D and Cleathero, I C and Kioul, A and Schmidt, O and Conway, P P and Prosser, S and Palmer, P J},
booktitle = {Proceedings of the 5th International Conference on Electronics Materials and Packaging (EMAP 2003)},
isbn = {981-05-0078-5},
month = {nov},
organization = {Singapore},
pages = {46--53},
title = {{Re-engineering of Thermoplastic Injection Moulding Process for Electronics Encapsulation in Automotive Applications}},
year = {2003}
}
@techreport{Dijcks2013,
abstract = {Today the term big data draws a lot of attention, but behind the hype there's a simple story. For decades, companies have been making business decisions based on transactional data stored in relational databases. Beyond that critical data, however, is a potential treasure trove of non-traditional, less structured data: weblogs, social media, email, sensors, and photographs that can be mined for useful information. Decreases in the cost of both storage and compute power have made it feasible to collect this data - which would have been thrown away only a few years ago. As a result, more and more companies are looking to include non-traditional yet potentially very valuable data with their traditional enterprise data in their business intelligence analysis.},
address = {Redwood Shores, CA},
author = {Dijcks, J.},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dijcks - 2013 - Oracle Big Data for the Enterprise.pdf:pdf},
institution = {Oracle Corporation},
title = {{Oracle: Big Data for the Enterprise}},
url = {http://www.oracle.com/us/products/database/big-data-for-enterprise-519135.pdf},
year = {2013}
}
@article{Sinclair2018,
abstract = {The paper reports some training, education, and operational findings from an EU Horizon 2020 project that included the production of technology road-maps for the domain of Cyber-Physical Systems (CPS). The project reviewed De- liverables from 72 CPS projects, all within Framework Pro- gramme 7 and Horizon 2020, including 18 from the ARTEMIS and ECSEL sub-programmes. This analysis led to the pro- duction of a ‘Knowledge Map' containing 75 technologies identified within the 72 projects as nodes in this map, con- nected by interoperability links. Filtering this map for each node in turn has led, in combination with other parts of the project, to some 48 recommendations for future focus and funding of developments in these technologies to assist in the rapid adoption of CPS in all domains. While the focus has been limited to European Union research and innovation, it is believed that the recommendations are transferable to other regions of the world.},
author = {Sinclair, Murray and Siemieniuch, Carys and Palmer, Paul},
doi = {10.1002/sys.21464},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Blackler - 1995 - Knowledge, Knowledge Work and Organizations An Overview and Interpretation(2).pdf:pdf},
journal = {Systems Engineering},
title = {{The identification of knowledge gaps in the technologies of Cyber-Physical Systems, with recommendations for closing these gaps}},
year = {2018}
}
@misc{Mathematics-2015,
annote = {id: 1},
author = {{E T H Zurich Department of Mathematics}},
number = {10/8/2015},
title = {{R: Hierarchical Clustering}},
url = {https://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html},
volume = {2015},
year = {2015}
}
@techreport{Kagermann2013,
abstract = {Germany has one of the most competitive manufac-turing industries in the world and is a global leader in the manufacturing equipment sector. This is in no small measure due to Germany's specialisation in re-search, development and production of innovative manufacturing technologies and the management of complex industrial processes. Germany's strong ma-chinery and plant manufacturing industry, its globally significant level of IT competences and its know-how in embedded systems and automation engineering mean that it is extremely well placed to develop its position as a leader in the manufacturing engineering industry. Germany is thus uniquely positioned to tap into the potential of a new type of industrialisation: Industrie 4.0.   The first three industrial revolutions came about as a result of mechanisation, electricity and IT. Now, the in-troduction of the Internet of Things and Services into the manufacturing environment is ushering in a fourth industrial revolution. In the future, businesses will es-tablish global networks that incorporate their machin-ery, warehousing systems and production facilities in the shape of Cyber-Physical Systems (CPS). In the manufacturing environment, these Cyber-Physical Systems comprise smart machines, storage systems and production facilities capable of autonomously ex-changing information, triggering actions and control-ling each other independently. This facilitates funda-mental improvements to the industrial processes involved in manufacturing, engineering, material us-age and supply chain and life cycle management. The smart factories that are already beginning to appear employ a completely new approach to production. Smart products are uniquely identifiable, may be lo-cated at all times and know their own history, current status and alternative routes to achieving their target state. The embedded manufacturing systems are ver-tically networked with business processes within fac-tories and enterprises and horizontally connected to dispersed value networks that can be managed in real time – from the moment an order is placed right through to outbound logistics. In addition, they both enable and require end-to-end engineering across the entire value chain.  Industrie 4.0 holds huge potential. Smart factories allow individual customer requirements to be met and mean that even one-off items can be manufactured profitably. In Industrie 4.0, dynamic business and engineering processes enable last-minute changes to production and deliver the ability to respond flexibly to disruptions and failures on behalf of suppliers, for example. End-to-end transparency is provided over the manufacturing process, facilitating optimised decision-making. In-dustrie 4.0 will also result in new ways of creating val-ue and novel business models. In particular, it will pro-vide start-ups and small businesses with the opportunity to develop and provide downstream services.   In addition, Industrie 4.0 will address and solve some of the challenges facing the world today such as resource and energy efficiency, urban production and demographic change. Industrie 4.0 enables continu-ous resource productivity and efficiency gains to be delivered across the entire value network. It allows work to be organised in a way that takes demograph-ic change and social factors into account. Smart as-sistance systems release workers from having to per-form routine tasks, enabling them to focus on creative, value-added activities. In view of the impending short-age of skilled workers, this will allow older workers to extend their working lives and remain productive for longer. Flexible work organisation will enable workers to combine their work, private lives and continuing professional development more effectively, promoting a better work-life balance.  Global competition in the manufacturing engineering sector is becoming fiercer and fiercer and Germany is not the only country to have recognised the trend to de-ploy the Internet of Things and Services in manufacturing industry. Moreover, it is not just competitors in Asia that pose a threat to German industry – the US is also taking measures to combat deindustrialisation through pro-grammes to promote “advanced manufacturing”.  In order to bring about the shift from industrial produc-tion to Industrie 4.0, Germany needs to adopt a dual strategy. Germany's manufacturing equipment indus-try should seek to maintain its global market leadership by consistently integrating information and communica-tion technology into its traditional high-tech strategies so that it can become the leading supplier of smart manufacturing technologies. At the same time, it will be necessary to create and serve new leading markets for CPS technologies and products. In order to deliver the goals of this dual CPS strategy, the following features of Industrie 4.0 should be implemented:  •  Horizontal integration through value networks •   End-to-end digital integration of engineering across the entire value chain •   Vertical integration and networked manufactur-ing systems  The journey towards Industrie 4.0 will require Germany to put a huge amount of effort into research and devel-opment. In order to implement the dual strategy, re-search is required into the horizontal and vertical inte-gration of manufacturing systems and end-to-end integration of engineering. In addition, attention should be paid to the new social infrastructures in the work-place that will come about as a result of Industrie 4.0 systems, as well as the continued development of CPS technologies.  If Industrie 4.0 is to be successfully implemented, re-search and development activities will need to be ac-companied by the appropriate industrial and industrial policy decisions. The Industrie 4.0 Working Group believes that action is needed in the following eight key areas:  •   Standardisation and reference architecture: Industrie 4.0 will involve networking and integration of several different companies through value networks. This collaborative partnership will only be possible if a single set of common standards is developed. A reference architecture will be needed to provide a technical description of these stand-ards and facilitate their implementation. •   Managing complex systems: Products and manufacturing systems are becoming more and more complex. Appropriate planning and explana-tory models can provide a basis for managing this growing complexity. Engineers should therefore be equipped with the methods and tools required to develop such models. •   A comprehensive broadband infrastructure for industry: Reliable, comprehensive and high-quality communication networks are a key requirement for Industrie 4.0. Broadband Internet infrastructure therefore needs to be expanded on a massive scale, both within Germany and between Germany and its partner countries. •   Safety and security: Safety and security are both critical to the success of smart manufacturing systems. It is important to ensure that production facilities and the products themselves do not pose a danger either to people or to the environment. At the same time, both production facilities and products and in particular the data and information they contain – need to be protected against misuse and unauthorised access. This will require, for example, the deployment of integrated safety and security architectures and unique identifiers, together with the relevant enhancements to training and continuing professional development content. •   Work organisation and design: In smart factories, the role of employees will change significantly. Increasingly real-time oriented control will transform work content, work processes and the working environment. Implementation of a socio-technical approach to work organisation will offer workers the opportunity to enjoy greater responsibility and enhance their personal development. For this to be possible, it will be necessary to deploy participative work design and lifelong learning measures and to launch model reference projects. •   Training and continuing professional develop-ment: Industrie 4.0 will radically transform workers' job and competence profiles. It will therefore be necessary to implement appropriate training strategies and to organise work in a way that fosters learning, enabling lifelong learning and workplace-based CPD. In order to achieve this, model projects and “best practice networks” should be promoted and digital learning techniques should be investigated.  •   Regulatory framework: Whilst the new manufacturing processes and horizontal business networks found in Industrie 4.0 will need to comply with the law, existing legislation will also need to be adapted to take account of new innovations. The challenges include the protection of corporate data, liability issues, handling of personal data and trade restrictions. This will require not only legisla-tion but also other types of action on behalf of businesses – an extensive range of suitable instruments exists, including guidelines, model contracts and company agreements or self-regula-tion initiatives such as audits. •   Resource efficiency: Quite apart from the high costs, manufacturing industry's consumption of large amounts of raw materials and energy also poses a number of threats to the environment and security of supply. Industrie 4.0 will deliver gains in resource productivity and efficiency. It will be necessary to calculate the trade-offs between the additional resources that will need to be invested in smart factories and the potential savings generated.  The journey towards Industrie 4.0 will be an evolution-ary process. Current basic technologies and experi-ence will have to be adapted to the specific require-ments of manufacturing engineering and innovative solutions for new locations and new markets will have to be explored. If this is done successfully, Industrie 4.0 will allow Germany to increase its global competitive-ness and pr},
annote = {Paper in Autonomy!!

Good section on contracts and regulations, p59-60

p41 Trustwortiness quote:
"Finally, it will be important to build trust in the reference architecture [for the Internet of Things {\&} Services]. This is particularly relevant with regard to know-how protection (see Chapter 5.7). It will also be key to ensure that the full range of intended users of the reference architecture are appropriately engaged in its design right from the early stages."

Implementation of Industrie 4.0:
• standardisation and reference architecture. Ref architecture is the important bit; needs several views: processing {\&} Transport; Smart automation devices {\&} Networking components; Software stds to control {\&} utilise these devices; Business application software; Engineering software {\&}kn capture for PLM. Can't do this top-down; would take too long. Only viable option is to use open sourcing, as in Linux, and TCP/IP.
• managing complex systems. Need Planning models to capture knowledge {\&} Explanatory models for simulations, etc.
• comprehensive broadband architecture for industry. High operational reliability and data link availability are crucial for mechanical engineering and automation en-gineering applications. Guaranteed latency times and stable connections are key, since they have a direct im-pact on application performance.
• Safety {\&} security. Security by design, also IT strategies, architectures, standards. Cautious implementation, recognising legacy issues for old network. Safety should be seen as a global approach.
• work organisation and design. Greater skills, especially in IT, more varied work in virtual realm, alienation, risk of Big Brother. SMEs are important for these issues. Loss of semi-skilled jobs; no answer for this.
• Training and continual professional development. New skills in problem-solving, comms skills, understanding of business issues, IT skills, avoidance of alienation through trg, change third-tier training to mphasise manuf skills.
• Regulatory framework. Regulatory analysis of new technologies needs to start now, not when they are implemented. Data merging risks, IPR issues, need new contractual models, liability issues in the network (who is responsible for what?)
• Resource efficiency, for sustainability and security of supply. Lightweight, except that the issues of circular manufacturing, etc. are acknowledged

p.35 Research requirements:
"This will require machine and plant manufacturers' strengths as integrators to be pooled with the competencies of the ICT and automation industries to establish targeted, creative development processes for creating new CPS. The new level of net-working required to achieve end-to-end integration of product models, manufacturing resources and manu-facturing systems is going to necessitate a huge re-search and development effort in the longer term. The priority for future research is shifting towards the investigation and development of fully describ-able, manageable, context-sensitive and controlla-ble or self-regulating manufacturing systems.
In the long term, these will consist of functional CPS components drawn from cross-discipline, modular tool-kits that can either be configured using help functions or integrate themselves synergistically into an existing infrastructure during production. It should also be pos-sible to achieve significantly enhanced integration of virtually planned and real manufacturing processes. In the long term, it will therefore be necessary for re-searchers to develop modular CPS and the corre-sponding component catalogues as a key feature of any model smart factory."

p.35 Central research themes:1 Horizontal integration through value networks
2 End-to-end engineering across the entire value chain
3 Vertical integration and networked manufacturing systems
4 Cyber-Physical Systems technology
5 New social infrastructures in the workplace. In the interests of social responsibility, it will be necessary to increase the involvement and promote the engagement of employees in terms of using their skills and experience with regard to both creative design and planning processes as well as in the operational work-ing environment. CPS will therefore require new work organisation structures covering the entire value net-work in order to boost employees' productivity and pro-vide organisational structures that support individuals' lifelong development. 

RECOMMENDED ACTIONS:
For Stds {\&} Ref Architecrture:
• Building a shared understanding of the goals, benefits, potential and risks, possible content and imple-mentation strategies with a view to creating the mutual confidence needed to support joint implementa-tion measures. The professional associations should take the lead with regard to the implementation of confidence-building measures.
• Alignment of the key terminology and subsequent production of a common “Industrie 4.0 glossary”. In addition, detailed attention should to be paid to those aspects of the following issues that are relevant to the specific content of Industrie 4.0:
+ Model universals (underlying core models, reference models, architecture designs)
+ Standards for the Industrie 4.0 service architecture
+ Standards for a supra-automation level procedural and functional description
+ Terminology standards and the use of ontologies
+ Understanding of autonomous and self-organising systems, including their planning, operation and security
+ Characteristics maintenance and system structure description
+ Approach to migrating existing architectures
• Production of a bottom-up map outlining the standardisation bodies that currently exist today. Current, established “automation reference architecture” approaches and examples would then be located on the map. This could serve as a basis for assessing the different themes with regard to further development and migration in the context of Industrie 4.0 and identifying clusters of themes that are not currently covered.
• Starting work on the production of a top-down roadmap, taking account of cost-benefit considerations and time constraints. A holistic approach should be adopted in order to strike a sensible balance between standardisation and individuality. The structure should be open and transparent and all the stakeholders should be engaged in the development and use of the standards. Licensing models should also be addressed.
• Development of an “Industrie 4.0 community” with members from several different companies that feels responsible for the technical implementation of the reference architecture and is able to roll it out and maintain it in the longer term. This will require a suitable licensing model and an appropriate commu-nity process to be chosen.
• Other tasks forming part of the Working Group's remit would include moderation, recommendations, evaluation, communication and motivation.

For modelling as a means of managing complex systems:
• A representative survey should be carried out to establish the most pressing requirements in the field of modelling, in order to narrow down this very broad subject area to the most important aspects in terms of implementation.
• Best practice sharing should be carried out, particularly among SMEs, in order to promote the fundamental importance of modelling among both practitioners and decision-makers. 
• The Working Group should encourage the establishment of common user groups for tool users, tool manufacturers (product managers, architects) and trainers with a view to participants gaining a better mutual understanding of each other's situations. 
• In addition, the Working Group should work on the development of the appropriate guidelines and recom-mended actions.
• targeted efforts should be made in terms of training and continuing professional development provision with regard to modelling and systems engineering. 

For comprehensive broadband:
Working Group strongly recommends implementation of the recommendations for expand-ing Germany's broadband Internet infrastructure proposed by Working Group 2 “Digital Infrastructure” at the 2011 National IT Summit. It will be necessary to carry out studies of concrete applications in order to establish the exact bandwidths and real-time capabilities required for Industrie 4.0.
+ Binding and reliable SLAs (Service Level Agreements)
+ Availability and performance of traffic capacity
+ Support for data link debugging/tracing, especially provision of the relevant technical aids
+ Information and physical safety and security, requiring integrated safety and security architectures and unique identifiers, including worker training and CPD.
+ Provision of widely available/guaranteed traffic capacity (fixed/guaranteed broadband)
+ SMS delivery status notification across all mobile network operators
+ Standardised Application Programming Interfaces (APIs) for provisioning covering all providers (SIM card activation/deactivation)
+ Tariff management
+ Cost control of mobile service contracts
+ Quality of service (fixed bandwidth)
+ Affordable global roaming
+ Widely available embedded SIM cards (note: doing the same thing as RFID, but more active, more competent; i.e. a step up from RFID. Would need a [ower source, whch RFID doesn't need.)
+ Satellite-based solutions for areas with no recep-tion [N.B. in sparsely populated areas, or Africa; note idea of airships or drones to provide coverage].”

For safety {\&} security:
• Integrated safety and security strategies, architectures and standards, across disciplies 
• Unique and secure IDs for products, processes and machines, including a kind of “security passport” containing details of the risks that were already taken into account and counteracted during development and the risks that need to be considered by the integrator, installer, operator or user.
• A migration strategy from Industrie 3.0 to Industrie 4.0
• User-friendly safety and security solutions. Or they will be circumvented.
• Safety and security in a business management context. W.r.t the IT risk of failures.
• Secure protection against product piracy. the co=operative environment adds emphasis to technical security and to corporate and competition law.
• Training and (in-house) CPD. Awareness of security issues and training in prevention
• “Community building” for data protection in Industrie 4.0. Paerticularly against Big Brother monitoring; need to involve unions, etc.

For Work organisation and design:
It is likely that their role will change significantly as a result of the increase in open, virtual work platforms and extensive human-machine and human-system interactions. Work content, work processes and the working environment will be dramatically transformed in a way that will have repercussions for flexibility, working time regulation, healthcare, demographic change and people's private lives. As a result, in order to achieve successful integration of tomorrow's technologies they will need to be smartly embedded into an innovative social organisation (within the work-place). 

It is very likely that the nature of work in Industrie 4.0 will place significantly higher demands on all members of the workforce in terms of managing complexity, abstraction and problem-solving. Employees will also be expected to be able to act much more on their own initiative and to possess excellent communication skills and the ability to organise their own work. In short, greater demands will be placed on employees' subjective skills and potential. This will provide opportunities in terms of qualitative enrichment of their work, a more interesting working environment, greater autonomy and more opportunities for self-development.

Nevertheless, the demands of the new, virtual work-place also present a threat to the maintenance and safeguarding of human capital. As the degree of technological integration increases, there is a danger of em-ployees being required to be more flexible and perform more demanding tasks, as well as a growing tension between the virtual world and the world of workers' own experience. would be important to ensure that these did not under any circumstances undercut existing data protection standards. In any event, these instruments would need to be adapted to take account of the specific charac-teristics of Industrie 4.0. could result in workers experienc-ing a loss of control and a sense of alienation from their work as a result of the progressive dematerialisation and virtualisation of business and work processes. It is also possible that “old” and “new” threats could com-bine to take on a new dimension, resulting in significant creativity and productivity losses and a tendency for employees to overwork themselves. 

Finally, it is important to consider the impact that the increasing presence of IT in manufacturing industry will have on headcount. It is likely that the number of simple manual tasks will continue to decline. This could pose a threat to at least some employee groups, notably semi-skilled workers. Such a scenario would be unacceptable both for the employees themselves and from the wider public's point of view in terms of the social inclusion dimension. Moreover, it would se-riously hamper the successful implementation of the Industrie 4.0 initiative.

• Adopt a socio-technical approach as the centrepiece of work design.
• Ascertain and document the impact on work and employment (opportunities and risks) together with the actions required to achieve employee-oriented labour and training policies
• Provide guidelines and practical aids for developing and implementing the socio-technical approach, together with the relevant reference projects
• Promote innovative approaches to participative work organisation and lifelong learning that embrace the entire workforce, irrespective of age, gender or qualifications
• Set up a regular dialogue between the social partners to enable transparent identification and discussion of the key advances, problems and potential solutions associated with the implementation of Industrie 4.0.

For training {\&} CPD (see Academy Cube, attched):
Industrie 4.0 will also require fundamental changes to the way IT experts are trained. The ability to identify application requirements in different industries and recruit development partners from around the world will increasingly take precedence over purely techno-logical expertise. The extremely wide range of poten-tial applications means that there is a limit to what can be achieved through standardised training pro-grammes.

As such, training partnerships between businesses and higher education institutions will be even more important in the future than they are today. Short basic training programmes will need to be followed by work place-ments and advanced study courses. It will be impor-tant to open up access to science and engineering studies and place greater emphasis on transferable skills such as business or project management.

There is a growing need for people to have a grasp of the overall context and to understand the interactions between all the actors involved in the manufacturing process. Consequent-ly, in addition to increased demand for metacognitive skills, social skills are also gaining in importance owing to the growing significance of real-life and computer-based interactions resulting from greater integration of formerly demarcated departments and disciplines.

In order to ensure that individuals' training potential can be recognised and described transparently, it will be nec-essary to develop standards for the recognition of non-formal and informal education. The goal is to teach peo-ple the principles of a new, holistic organisational model and ensure that systems are described transparently so that employees are confident in what they are doing.
• Promotion of model Training {\&} CPD projects
• Establishment and promotion of “best practice networks”
• Investigation of new approaches to knowledge and skills acquisition in the workplace, develop-ment of digital learning techniques
• Promotion of cross-cutting approaches to work organisation, including older workers.
• Promotion of Industrie 4.0-specific learning content and interdisciplinary cooperation
• IT-based modelling of technological systems

For regulatory framework (business perspective only):
Two things are required to reconcile regulation and technology: the formulation of criteria to ensure that the new technologies comply with the law and de-velopment of the regulatory framework in a way that facilitates innovation. In the context of Industrie 4.0, it will often be possible to achieve this through common law contracts. Both factors require the regulatory anal-ysis of new technologies to begin as early as possible during the R{\&}D phase rather than being left until they are already in use.

A number of specific dangers are associated with this new context – for example, data that were initially generated and ex-changed in order to coordinate manufacturing and lo-gistics activities between different companies could, if read in conjunction with other data, suddenly provide third parties with highly sensitive information about one of the partner companies that might, for example, give them an insight into its business strategies. 

New, regulated business models will also be necessary – the raw data that are generated may con-tain information that is valuable to third parties and companies may therefore wish to make a charge for sharing them. Innovative business models like this will also require legal safeguards (predominantly in the shape of contracts) in order to ensure that the value-added created is shared out fairly, e.g. through the use of dynamic pricing models. 

However, contracts do have their limitations when it comes to managing large volumes of voluntary legal ar-rangements, since a disproportionately large amount of work is involved in calculating the risks and negotiating separate contracts for each individual case. It is therefore necessary to develop new contractual models that allow businesses to retain sovereignty over “their” data whilst still facilitating entrepreneurial flexibility.

However, in Industrie 4.0 manufacturing facilities are responsible for a wider range of matters than in the past. They may now be subject to a liability action not only if they fail to meet their primary responsibilities (in terms of the prod-uct's durability, correct operation and appearance), but also for failings in their role as part of a network of smart objects.

In these scenarios, the issue of liability and responsibil-ity becomes even more important – when autonomous systems are deployed in networks, a lack of structural transparency could make it almost impossible to explic-itly determine who performed a particular action, result-ing in uncertainty with regard to legal liability. It is true that businesses employing manufacturing systems that use autonomous data processing are legally liable for the security of their manufacturing facilities and prod-ucts vis-{\`{a}}-vis third parties. Current tort and product lia-bility law already provides adequate solutions in this area. However, if the other partners in a network wish to avoid being held jointly liable, or if they want at least to have recourse against the other partners, then it is es-sential for their responsibilities to be contractually stip-ulated from the outset and/or for actions to be clearly attributed to the owners of the respective systems. This also has implications for the insurability of residual risk and the way that the insurance industry calculates the relevant premiums.

Current regulation fails to adequately address these problems [of handling personal data]. Outsourced data processing models are al-ready encountering difficulties (e.g. in the realm of cloud computing), since local data protection stand-ards are generally not applicable in countries outside of Europe, meaning that in practice it is impossible for cli-ent companies to comply with their data protection re-sponsibilities.

Businesses therefore have a growing need for a legally unambiguous and practical solution for handling per-sonal data. It may be possible, up to a point, to achieve this through in-house binding corporate rules, collec-tive agreements and company agreements, although it would be important to ensure that these did not under any circumstances undercut existing data protection standards. In any event, these instruments would need to be adapted to take account of the specific charac-teristics of Industrie 4.0.

• Raise awareness amng SMEs of the problems (and solutions)
• Solutions may not need legislation; a mix of regulatory, policy and technical elements may be enough.
• Liability should focus on data security and documentary evidence beteeen organisation involved.
• Harmonisation of trade restrictions is necessary, including through the WTO.
• Interdisciplinary collaboration will be necessary to deliver the points above.

For resource efficiency:
• See ISO 14040, ISO 14044
• Adopt 'Efficiency Factory' initiative
• Demonstrations of resource savings
• Use of Life-Cycle Assessments
• Assess resource trade-offs when utilising CPS
• Utilise metrics {\&} KPIs for resource prod},
author = {Kagermann, H and Wahlster, W and Helbig, J},
keywords = {BPR,CSCW,Cyber-physical systems,IoT,SMEs,Work flow management,agents,agility,business process re-engineering,competitive challenges,complexity,corporate governance,culture,engineering,ergonomics,federated control,green,knowledge structures,learning organisation,legislation,networks,organisational design,outsourcing,product cycles,reference architectures,reliability,resilience,skills,socio-tech,standards,strategic planning,strategy,supply chain,sustainability,systems,training},
publisher = {acatech:  National Academy of Science and Technology},
title = {{Recommendations for implementing the strategic initiative INDUSTRIE 4.0}},
year = {2013}
}
@techreport{Wooodward2018,
abstract = {Newsletter of the Leicestershire Entomological Society covering Vice County 55. Plans for book page 6.},
author = {Wooodward, Steve (Editor)},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wooodward - 2018 - LES NEWSLETTER Number 58.pdf:pdf},
number = {January},
pages = {8},
title = {{LES NEWSLETTER Number 58}},
year = {2018}
}
@article{Vaughan2020,
abstract = {One big focus for Cummings is using AI to inform how the civil service is run. David Curtis at University College London is sceptical. “It looks like he has bought all the hype about AI and is intending to attempt to apply it in places where it won't work at all,” he says. Matt Jukes at Notbinary, who has worked on digital government projects, said in a blog post that transforming the civil service may prove harder than Cummings thinks, because so much government data is in spreadsheets and old legacy systems. “It just fails to acknowledge just how messy the underlying data is.” Read more: https://www.newscientist.com/article/2229002-dominic-cummings-wants-weirdos-to-help-run-the-uk-will-it-work/{\#}ixzz6CJXjbuDo},
author = {Vaughan, Adam},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vaughan - 2020 - Analysis Dominic Cummings wants 'weirdos' to help run the UK. Will it work.pdf:pdf},
journal = {New Scientist},
number = {3264},
pages = {1},
title = {{Analysis: Dominic Cummings wants 'weirdos' to help run the UK. Will it work?}},
volume = {0},
year = {2020}
}
@article{Fruchterman1991,
abstract = {We present a modification of the spring‐embedder model of Eades [Congressus Numerantium, 42, 149–160, (1984)] for drawing undirected graphs with straight edges. Our heuristic strives for uniform edge lengths, and we develop it in analogy to forces in natural systems, for a simple, elegant, conceptually‐intuitive, and efficient algorithm. Copyright {\textcopyright} 1991 John Wiley {\&} Sons, Ltd},
author = {Fruchterman, Thomas M.J. and Reingold, Edward M.},
doi = {10.1002/spe.4380211102},
issn = {1097024X},
journal = {Software: Practice and Experience},
keywords = {Force‐directed placement,Graph drawing,Multi‐level techniques,Simulated annealing},
number = {11},
pages = {1129--1164},
title = {{Graph drawing by force‐directed placement}},
volume = {21},
year = {1991}
}
@misc{Kandel2011a,
abstract = {In spite of advances in technologies for working with data, analysts still spend an inordinate amount of time diagnosing data quality issues and manipulating data into a usable form. This process of data wrangling' often constitutes the most tedious and time-consuming aspect of analysis. Though data cleaning and integration arelongstanding issues in the database community, relatively little research has explored how interactive visualization can advance the state of the art. In this article, we review the challenges and opportunities associated with addressing data quality issues. We argue that analysts might more effectively wrangle data through new interactive systems that integrate data verification, transformation, and visualization. We identify a number of outstanding research questions, including how appropriate visual encodings can facilitate apprehension of missing data, discrepant values, and uncertainty; how interactive visualizations might facilitate data transform specification; and how recorded provenance and social interaction might enable wider reuse, verification, and modification of data transformations.},
author = {Kandel, Sean and Heer, Jeffrey and Plaisant, Catherine and Kennedy, Jessie and {Van Ham}, Frank and Riche, Nathalie Henry and Weaver, Chris and Lee, Bongshin and Brodbeck, Dominique and Buono, Paolo},
booktitle = {Information Visualization},
doi = {10.1177/1473871611415994},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kandel et al. - 2011 - Research directions in data wrangling Visualizations and transformations for usable and credible data.pdf:pdf},
isbn = {1473871611},
issn = {14738716},
keywords = {Data cleaning,Data quality,Data transformation,Uncertainty,Visualization},
number = {4},
pages = {271--288},
title = {{Research directions in data wrangling: Visualizations and transformations for usable and credible data}},
url = {http://journals.sagepub.com/doi/pdf/10.1177/1473871611415994},
volume = {10},
year = {2011}
}
@article{Tingley2009,
abstract = {The difficulty of making valid comparisons between historical and contemporary data is an obstacle to documenting range change in relation to environmental modifications. Recent statistical advances use occupancy modeling to estimate simultaneously the probability of detection and the probability of occupancy, and enable unbiased comparisons between historical and modern data; however, they require repeated surveys at the same locations within a time period. We present two models for explicitly comparing occupancy between historical and modern eras, and discuss methods to measure range change. We suggest that keepers of historical data have crucial roles in curating and aiding accessibility to data, and we recommend that collectors of contemporary specimen data organize their sampling efforts to include repeated surveys to estimate detection probabilities. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Tingley, Morgan W. and Beissinger, Steven R.},
doi = {10.1016/j.tree.2009.05.009},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Tingley, Beissinger - 2009 - Detecting range shifts from historical species occurrences new perspectives on old data.pdf:pdf},
isbn = {0169-5347},
issn = {01695347},
journal = {Trends in Ecology and Evolution},
month = {nov},
number = {11},
pages = {625--633},
pmid = {19683829},
publisher = {Elsevier Current Trends},
title = {{Detecting range shifts from historical species occurrences: new perspectives on old data}},
url = {https://www.sciencedirect.com/science/article/pii/S0169534709002006},
volume = {24},
year = {2009}
}
@techreport{Macfarlane2018,
abstract = {Pandoc is a Haskell library for converting from one markup format to another, and a command-line tool that uses this library. Pandoc can read Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, Mul- tiMarkdown, and (subsets of) Textile, reStructuredText, HTML, LaTeX, MediaWiki markup, TWiki markup, TikiWiki markup, Creole 1.0, Haddock markup, OPML, Emacs Org mode, DocBook, JATS, Muse,},
address = {Berkeley, CA},
author = {Macfarlane, John},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Macfarlane - 2017 - Pandoc User's Guide.pdf:pdf},
institution = {University of California},
pages = {78},
title = {{Pandoc User's Guide}},
url = {https://pandoc.org/MANUAL.pdf},
year = {2017}
}
@inproceedings{palmer2003towardspartnership,
author = {Palmer, P J},
isbn = {1861-940947},
month = {apr},
organization = {NPL, Teddington},
pages = {96--97},
title = {{Towards Incorporating Technology Trends into a Maintainable Roadmap for the PRIME Faraday Partnership}},
year = {2003}
}
@article{Plummer,
annote = {id: 1; issn: electronic 2058-5543; publication{\_}type: full{\_}text},
author = {Plummer, Kate E and Hale, James D and O'Callaghan, Matthew J and Sadler, Jon P and Siriwardena, Gavin M},
doi = {10.1093/jue/juw004},
issn = {2058-5543},
journal = {Journal of Urban Ecology},
number = {1},
pages = {juw004},
title = {{Investigating the impact of street lighting changes on garden moth communities }},
url = {http://dx.doi.org/10.1093/jue/juw004},
volume = {2}
}
@misc{Anon2017,
author = {Anon},
keywords = {Physical Web},
mendeley-tags = {Physical Web},
title = {{The Physical Web}},
url = {http://google.github.io/physical-web/},
urldate = {2017-11-02},
year = {2017}
}
@misc{Zappia2020,
author = {Zappia, Luke},
title = {{Plotting clustering trees}},
url = {https://cran.r-project.org/web/packages/clustree/vignettes/clustree.html},
urldate = {2020-10-02},
year = {2020}
}
@inproceedings{Wiggins2011,
abstract = {Data quality is a primary concern for researchers employing a public participation in scientific research (PPSR) or "citizen science" approach. This mode of scientific collaboration relies on contributions from a large, often unknown population of volunteers with variable expertise. In a survey of PPSR projects, we found that most projects employ multiple mechanisms to ensure data quality and appropriate levels of validation. We created a framework of 18 mechanisms commonly employed by PPSR projects for ensuring data quality, based on direct experience of the authors and a review of the survey data, noting two categories of sources of error (protocols, participants) and three potential intervention points (before, during and after participation), which can be used to guide project design. {\textcopyright} 2011 IEEE.},
author = {Wiggins, Andrea and Newman, Greg and Stevenson, Robert D. and Crowston, Kevin},
booktitle = {Proceedings - 7th IEEE International Conference on e-Science Workshops, eScienceW 2011},
doi = {10.1109/eScienceW.2011.27},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wiggins et al. - 2011 - Mechanisms for data quality and validation in citizen science.pdf:pdf},
isbn = {9780769545981},
keywords = {citizen science,data quality,data validation},
pages = {14--19},
title = {{Mechanisms for data quality and validation in citizen science}},
year = {2011}
}
@misc{Amazon2018,
abstract = {A registry ofdatasets that are available via AWS resources},
author = {Amazon},
booktitle = {Amazon},
title = {{Registry of Open Data on AWS}},
urldate = {2018-07-31},
year = {2018}
}
@article{VanLangevelde2011,
abstract = {During the last decades, artificial night lighting has increased globally, which largely affected many plant and animal species. So far, current research highlights the importance of artificial light with smaller wavelengths in attracting moths, yet the effect of the spectral composition of artificial light on species richness and abundance of moths has not been studied systematically. Therefore, we tested the hypotheses that (1) higher species richness and higher abundances of moths are attracted to artificial light with smaller wavelengths than to light with larger wavelengths, and (2) this attraction is correlated with morphological characteristics of moths, especially their eye size. We indeed found higher species richness and abundances of moths in traps with lamps that emit light with smaller wavelengths. These lamps attracted moths with on average larger body mass, larger wing dimensions and larger eyes. Cascading effects on biodiversity and ecosystem functioning, e.g. pollination, can be expected when larger moth species are attracted to these lights. Predatory species with a diet of mainly larger moth species and plant species pollinated by larger moth species might then decline. Moreover, our results indicate a size-bias in trapping moths, resulting in an overrepresentation of larger moth species in lamps with small wavelengths. Our study indicates the potential use of lamps with larger wavelengths to effectively reduce the negative effect of light pollution on moth population dynamics and communities where moths play an important role.},
annote = {From Duplicate 2 (Effect of spectral composition of artificial light on the attraction of moths - van Langevelde, Frank; Ettema, Jody A; Donners, Maurice; WallisDeVries, Michiel F; Groenendijk, Dick)
And Duplicate 4 (Effect of spectral composition of artificial light on the attraction of moths - van Langevelde, Frank; Ettema, Jody A; Donners, Maurice; WallisDeVries, Michiel F; Groenendijk, Dick)

id: 1; issn: print 00063207; publication{\_}type: full{\_}text},
author = {van Langevelde, Frank and Ettema, Jody A and Donners, Maurice and WallisDeVries, Michiel F and Groenendijk, Dick},
doi = {10.1016/j.biocon.2011.06.004},
issn = {0006-3207},
journal = {Biological Conservation},
keywords = {Body-size dependent effect,Cascading effects,Ecology of the night,Lepidoptera,Light pollution},
number = {9},
pages = {2274 {\textless}last{\_}page{\textgreater} 2281},
title = {{Effect of spectral composition of artificial light on the attraction of moths}},
url = {http://www.sciencedirect.com/science/article/pii/S000632071100231X http://dx.doi.org/10.1016/j.biocon.2011.06.004},
volume = {144},
year = {2011}
}
@techreport{Mack2018,
abstract = {In traditional usability studies, researchers talk to users of tools to understand their needs and challenges. Insights gained via such interviews offer context, detail, and background. Due to costs in time and money, we are beginning to see a new form of tool interrogation that prioritizes scale, cost, and breadth by utilizing existing data from online forums. In this case study, we set out to apply this method of using online forum data to a specific issue—challenges that users face with Excel spreadsheets. Spreadsheets are a versatile and powerful pro-cessing tool if used properly. However, with versatility and power come errors, from both users and the software, which make using spreadsheets less effective. By scraping posts from the website Reddit, we collected a dataset of questions and complaints about Excel. Specifically, we explored and charac-terized the issues users were facing with spreadsheet software in general, and in particular, as resulting from a large amount of data in their spreadsheets. We discuss the implications of our findings on the design of next-generation spreadsheet software.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.03829v1},
author = {Mack, Kelly and Lee, John and Chang, Kevin and Karahalios, Karrie and Parameswaran, Aditya},
eprint = {arXiv:1801.03829v1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mack et al. - 2018 - Characterizing Scalability Issues in Spreadsheet Software using Online Forums.pdf:pdf},
institution = {University of Illinois},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jan},
title = {{Characterizing Scalability Issues in Spreadsheet Software using Online Forums}},
url = {http://arxiv.org/abs/1801.03829 https://arxiv.org/pdf/1801.03829.pdf},
year = {2018}
}
@inproceedings{Hedeler2009,
abstract = {The vision of dataspaces has been articulated as providing various of the benefits of classical data integration, but with reduced up-front costs, combined with opportunities for incremental refinement, enabling a "pay as you go" approach. However, results that seek to realise the vision exhibit considerable variety in their contexts, priorities and techniques, to the extent that the definitional characteristics of dataspaces are not necessarily becoming clearer over time. With a view to clarifying the key concepts in the area, encouraging the use of consistent terminology, and enabling systematic comparison of proposals, this paper defines a collection of dimensions that capture both the components that a dataspace management system may contain and the lifecycle it may support, and uses these dimensions to characterise representative proposals. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Hedeler, Cornelia and Belhajjame, Khalid and Fernandes, Alvaro A.A. and Embury, Suzanne M. and Paton, Norman W.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02843-4_8},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hedeler et al. - 2009 - Dimensions of dataspaces.pdf:pdf},
isbn = {364202842X},
issn = {03029743},
pages = {55--66},
title = {{Dimensions of dataspaces}},
volume = {5588 LNCS},
year = {2009}
}
@article{Fortunato2016,
abstract = {Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.},
archivePrefix = {arXiv},
arxivId = {1608.00163},
author = {Fortunato, Santo and Hric, Darko},
doi = {10.1016/j.physrep.2016.09.002},
eprint = {1608.00163},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fortunato, Hric - 2016 - Community detection in networks A user guide.pdf:pdf},
isbn = {0370-1573},
issn = {03701573},
journal = {Physics Reports},
keywords = {Clustering,Communities,Networks},
month = {nov},
pages = {1--44},
publisher = {North-Holland},
title = {{Community detection in networks: A user guide}},
url = {https://www.sciencedirect.com/science/article/pii/S0370157316302964},
volume = {659},
year = {2016}
}
@inproceedings{Ritchey1998a,
abstract = {Fritz Zwicky pioneered the development of morphological analysis (MA) as a method for investigating the totality of relationships contained in multi-dimensional, usually non- quantifiable problem complexes. During the past two decades, MA has been extended and applied in the area of futures studies and for structuring and analysing complex policy spaces. This article outlines the fundamentals of the morphological approach and describes recent applications in policy analysis.},
author = {Ritchey, Tom},
booktitle = {16th EURO Conference on Operational Analysis},
doi = {10.1007/978-3-642-19653-9},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 1998 - General morphological analysis.pdf:pdf},
isbn = {9783642196539},
keywords = {analysis synthesis,fritz zwicky,general morphology,methodology,modeling complex problems,morphological analysis,morphological box,morphological modeling,non quantified modeling,policy analysis,policy space,qualitative modeling,typology analysis},
pages = {2002-- 2006},
title = {{General morphological analysis}},
year = {1998}
}
@article{Alexander2011,
abstract = {With the exponential growth in data acquisition and generation-whether by next-generation telescopes, high-throughput experiments, peta-scale scientific computing, or high-resolution sensors-it's an extremely exciting time for scientific discovery. As a result of these technological advances, the next decade will see even more significant impacts in fields such as medicine, astronomy and cosmology, materials science, social sciences, and climate. Discoveries will likely be made possible with amounts of data previously unavailable (as an example, see work by Alon Halevy and his colleagues.},
author = {Alexander, Francis J. and Hoisie, Adolfy and Szalay, Alexander},
doi = {10.1109/MCSE.2011.99},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Alexander, Hoisie, Szalay - 2011 - Big Data.pdf:pdf},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {nov},
number = {6},
pages = {10--13},
title = {{Big Data}},
url = {http://ieeexplore.ieee.org/document/6077842/},
volume = {13},
year = {2011}
}
@misc{Parsons2010,
abstract = {A scientific publication is fundamentally an argument consisting of a set of ideas and expectations supported by observations and calculations that serve as evidence of its veracity. An argument without evidence is only a set of assertions. Consider the difference between the statement “The hairy woodpecker population is declining in the northwest region of the United States” and the statement “Hairy woodpecker populations in the northwest region of the United States have declined by 11{\%} between 1992 and 2003, according to data from the Institute for Bird Populations (http://www.birdpop.org/).” Both or neither of these statements could be true, but only the second one can be verified. Scientific papers do, of course, present specific data points as evidence for their arguments, but how well do papers guide readers to the body of those data, where the the data's integrity can be further examined? In practice, a chasm may lie across the path of a reviewer seeking the source data of a scientific argument.},
author = {Parsons, Mark A. and Duerr, Ruth and Minster, Jean Bernard},
booktitle = {Eos},
doi = {10.1029/2010EO340001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Parsons, Duerr, Minster - 2010 - Data citation and peer review.pdf:pdf},
isbn = {2010101029},
issn = {00963941},
number = {34},
pages = {297--298},
title = {{Data citation and peer review}},
url = {http://doi.wiley.com/10.1029/2010EO340001},
volume = {91},
year = {2010}
}
@misc{KnowledgeBasedSystemsInc.2006,
abstract = {DEF{\O} is a method designed to model the decisions, actions, and activities of an organization or system. IDEF{\O} was derived from a well-established graphical language, the Structured Analysis and Design Technique (SADT). The United States Air Force commissioned the developers of SADT to develop a function modeling method for analyzing and communicating the functional perspective of a system. Effective IDEF{\O} models help to organize the analysis of a system and to promote good communication between the analyst and the customer. IDEF{\O} is useful in establishing the scope of an analysis, especially for a functional analysis. As a communication tool, IDEF{\O} enhances domain expert involvement and consensus decision-making through simplified graphical devices. As an analysis tool, IDEF{\O} assists the modeler in identifying what functions are performed, what is needed to perform those functions, what the current system does right, and what the current system does wrong. Thus, IDEF{\O} models are often created as one of the first tasks of a system development effort.},
author = {{Knowledge Based Systems Inc.}},
booktitle = {Online},
title = {{IDEF0 Function Modeling Method}},
url = {http://www.idef.com/idefo-function{\_}modeling{\_}method/ http://www.idef.com/IDEF0.htm},
urldate = {2019-05-07},
year = {2006}
}
@techreport{Preston2012,
abstract = {• A fundamentally new model of industrial organization is needed to de-link rising prosperity from resource consumption growth – one that goes beyond incremental efficiency gains to deliver transformative change. • A ‘circular economy' (CE) is an approach that would transform the function of resources in the economy. Waste from factories would become a valuable input to another process – and products could be repaired, reused or upgraded instead of thrown away. • In a world of high and volatile resource prices, a CE offers huge business opportunities. Pioneering companies are leading the way on a CE, but to drive broader change it is critical to collect and share data, spread best practice, invest in innovation and encourage business-to-business collaboration. • Policy-makers should focus on accelerating transition to a CE in a timescale consistent with the response to climate change, water scarcity and other global challenges. Smart regulation can reward private-sector leadership and align incentives along the supply chain – for example, to deliver a step-change in remanufacturing rates. • Resource consumption targets that reflect environmental constraints should be considered at a global level. Coordination of national policies would help create a level playing field across major markets, easing competitiveness concerns and reducing the costs of implementation.},
address = {London},
annote = {Paper in sustainability. 

VERY GOOD FOR REFERENCES. Note also the table for 'Smart regulations' in File Attachments. Also, include the effects of the ESOS regulations on energy efficiency; these should push companies towards a circular economy because of the relatively high cost of refining metals from ores.

Quotes:
"For example, in China, the CE is defined in legislation as a generic term for reducing, reusing and recycling activities conducted in the process of production, circulation and consumption. In the 12th Five-Year Plan (2011–15), the priority is a shift from the resource (primarily energy and water) efficiency of heavy industries to recycling of metals and minerals and remanufacturing of products, especially the exchange of materials between companies. This is intimately linked to the upgrading and reorganization of China's industry to boost competitiveness, and to its wider development strategy.
[and water) efficiency of heavy industries to recycling of metals and minerals and remanufacturing of products, especially the exchange of materials between companies.13 This is intimately linked to the upgrading and reorganiza-tion of China's industry to boost competitiveness, and to its wider development strategy."
[ www.chinaenvironmentallaw.com/wp-content/uploads/2008/09/circular-economy-law-cn-en-final.pdf
www.uncrd.or.jp/env/spc/docs/3rd{\_}3r/PS5-2{\_}NDRC-China{\_}Guo{\%}20and{\%}20Li-new.pdf. 

"The EU has agreed a strategy for ‘a resource-efficient Europe' under its ‘Europe 2020 Strategy'15 and introduced an initiative to address raw-materials security.16 Relevant country strategies include the National Resource Efficiency Programme in Germany17 and the proposed ‘materials roundabout' – a hub for the high-grade recycling of materials and products – in the Netherlands.18 In the United Kingdom, the environ-mental think tank Green Alliance has produced one of the most detailed studies of the CE to date, focusing on economic instruments and raw-materials security. It proposes, for example, product standards and a ‘recovery reward' for metals.19 "
[16 See European Commission website, http://ec.europa.eu/enterprise/policies/raw-materials/
18 Han van de Wiel (2011), ‘The Netherlands as materials roundabout', Waste Forum special edition, January, http://www.wastematters.eu/uploads/media
19 Hislop, H. and Hill, J. (2011), ‘Reinventing the Wheel: A Circular Economy for Resource Security', Green Alliance, www.green-alliance.org.uk/grea{\_}p.aspx?id=604]

"Proximity can also produce significant positive effects on the rates of formation of new firms and firms' produc-tivity, innovation, profitability and growth, according to the Organisation for Economic Co-operation and Development (OECD).27 
[27 Davis, C., Arthurs, D. et al. (2006), ‘What Indicators for Science, Technology and Innovation Policies in the 21st Century?' (Ottawa: OECD), www.oecd.org/dataoecd/22/18/37443546.pdf.
Accept that proximity has benefits, as report ays; however, does the IoT Networked Factory still indicate this? Does the financial payoff of distribution outweigh the benefits of localisation? If the real benefit of localisation is reduced time and cost for internal physical distribution plus collective wisdom, allied to the extra cost of extrnal distribution, then should we move to clusters (or blobs) for subsystems/modules?]

"McKinsey have argued that the most profitable opportunities lie in products with a medium life span – longer than a single use but short enough for reuse and remanufacturing to be attractive."
[ Ellen MacArthur Foundation (2012), ‘Towards the Circular Economy: Economic and Business Rationale for an Accelerated Transition', www.thecirculareconomy.org. ]

"The Japanese electronic firm Kyocera was an early pioneer of refillable toner cartridges. The company says that conventional cartridges can have over 60 parts made from numerous materials – and are typically thrown away at the end of their life. Instead, it produces much simpler cartridges that can be easily refilled. Over the lifetime of the product this saves money because the materials cost is reduced by 50{\%} (while waste is down by 90{\%}). However, despite its efforts over the past two decades, Kyocera admits it has struggled to displace the conven-tional business model. The reason is that buying decisions are often determined by the retail price of a printer and not the lifetime cost, which includes the cost of toner and maintenance."
[But note how Ricoh changed their business model in their COMET cycle to 'pay per page'; Kyocera seemingly didn't see this need - insufficiently disruptive thinking?]

"Experience in China suggests that using waste steel in steelmaking requires 60{\%} less energy than making primary steel from iron ore.38 One key issue is how to extract relatively small amounts of key materials such as metals and minerals from electronic goods. Even if the component containing this material cannot be directly reused, with good product design and an appropriate waste stream it can be extracted and recycled without significant degradation of its material quality.
The opportunities are huge if technical barriers could be overcome. For example, a tonne of ore from a gold mine produces just five grams of gold on average, whereas a tonne of discarded mobile phones can yield up to 150 grams.39 Tailings ponds (containing the waste from metal processing) and landfill sites are similarly rich in useful material."
[ 39 Yoshikawa, M. (2008), ‘Urban miners look for precious metals in cell phones', Reuters, www.reuters.com/article/2008/04/27/us-japan-metals-recycling-idUST13528020080427. ]

"Jane Jacobs predicted in 1969 that ‘cities will become huge, rich and diverse mines of raw materials'.40 Such approaches could allow companies to sustain large high-grade ore deposits in the ‘urban mine' practically indefinitely – a fact that is highly relevant in a world where the discovery of large ore bodies is becoming increasingly rare and ore grades have been declining for decades."
[ 40 Jacobs, J. (1969), The Economy of Cities (New York: Random House)]

"Further international cooperation is important for progress on the CE because trade in waste and resources is rising and supply chains for many products today involve multiple countries, so that separate domestic policies can only address part of the problem. Key technologies will need to spread across borders and be adapted to local needs. Coordination of national policies in key areas and standardization could help to create a level playing field across major markets, easing competitiveness concerns and reducing the costs of implementation for business. It would also create larger markets and investment opportunities."

"Countries and companies need metrics for processes that go beyond incremental resource efficiency improve-ments and capture more transformative actions, including designing out waste and using sustain-able materials. While a credible technical approach is crucial, international experience of developing eco-indicators at national and city levels suggests that the process for agreeing approaches will be equally important to encourage widespread adoption of a methodology.67"
[ 67 Pint{\'{e}}r, L. (2006), International Experience in Establishing Indicators for the Circular Economy and Considerations for China: Report for the Environment and Social Development Sector Unit, East Asia and Pacific Region (Washington, DC: World Bank), www.iisd.org/pdf/2006/measure{\_}circular{\_}economy{\_}china.pdf.]},
author = {Preston, F},
institution = {Energy, Environment and Resource Governance},
isbn = {EERG BP 2012/02},
keywords = {competitive challenges,design process,engineering,green,legislation,organisational design,smart city,socio-tech,standards,strategic planning,strategy,supply chain,sustainability},
publisher = {Chatham House},
title = {{A global redesign? Shaping the circular economy}},
year = {2012}
}
@misc{Dalbey.John2011,
abstract = {Pseudocode is a kind of structured English for describing algorithms. It allows the designer to focus on the logic of the algorithm without being distracted by details of language syntax. At the same time, the pseudocode needs to be complete. It describes the entire logic of the algorithm so that implementation becomes a rote mechanical task of translating logic into source code. A line of pseudocode translates to one or many lines of actual code (normally, it is the latter than former).},
author = {{Dalbey. John}},
booktitle = {Academic Web page},
pages = {1--7},
title = {{Pseudocode standard}},
url = {http://users.csc.calpoly.edu/{~}jdalbey/SWE/pdl{\_}std.html},
urldate = {2019-07-16},
year = {2011}
}
@article{Peh2013,
abstract = {Sites that are important for biodiversity conservation can also provide significant benefits (i.e. ecosystem services) to people. Decision-makers need to know how change to a site, whether development or restoration, would affect the delivery of services and the distribution of any benefits among stakeholders. However, there are relatively few empirical studies that present this information. One reason is the lack of appropriate methods and tools for ecosystem service assessment that do not require substantial resources or specialist technical knowledge, or rely heavily upon existing data. Here we address this gap by describing the Toolkit for Ecosystem Service Site-based Assessment (TESSA). It guides local non-specialists through a selection of relatively accessible methods for identifying which ecosystem services may be important at a site, and for evaluating the magnitude of benefits that people obtain from them currently, compared with those expected under alternative land-uses. The toolkit recommends use of existing data where appropriate and places emphasis on enabling users to collect new field data at relatively low cost and effort. By using TESSA, the users could also gain valuable information about the alternative land-uses; and data collected in the field could be incorporated into regular monitoring programmes. {\textcopyright} 2013 Elsevier B.V.},
author = {Peh, Kelvin S.H. and Balmford, Andrew and Bradbury, Richard B. and Brown, Claire and Butchart, Stuart H.M. and Hughes, Francine M.R. and Stattersfield, Alison and Thomas, David H.L. and Walpole, Matt and Bayliss, Julian and Gowing, David and Jones, Julia P.G. and Lewis, Simon L. and Mulligan, Mark and Pandeya, Bhopal and Stratford, Charlie and Thompson, Julian R. and Turner, Kerry and Vira, Bhaskar and Willcock, Simon and Birch, Jennifer C.},
doi = {10.1016/j.ecoser.2013.06.003},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Peh et al. - 2013 - TESSA A toolkit for rapid assessment of ecosystem services at sites of biodiversity conservation importance.pdf:pdf},
issn = {22120416},
journal = {Ecosystem Services},
keywords = {Climate regulation,Cultivated goods,Ecosystem-service tools,Harvested wild goods,Nature-based recreation,Water-related services},
month = {sep},
pages = {51--57},
title = {{TESSA: A toolkit for rapid assessment of ecosystem services at sites of biodiversity conservation importance}},
volume = {5},
year = {2013}
}
@article{Chen2012,
abstract = {There is a tremendous interest in big data by academia, industry and a large user base. Several commercial and open source providers unleashed a variety of products to support big data storage and processing. As these products mature, there is a need to evaluate and compare the performance of these systems. In this paper, we present BigBench, an end-to-end big data benchmark proposal. The underlying business model of BigBench is a product retailer. The proposal covers a data model and synthetic data generator that addresses the variety, velocity and volume aspects of big data systems con- taining structured, semi-structured and unstructured data. The structured part of the BigBench data model is adopted from the TPC-DS benchmark, which is enriched with semi- structured and unstructured data components. The semi- structured part captures registered and guest user clicks on the retailer's website. The unstructured data captures product reviews submitted online. The data generator de- signed for BigBench provides scalable volumes of raw data based on a scale factor. The BigBench workload is designed around a set of queries against the data model. From a busi- ness prospective, the queries cover the different categories of big data analytics proposed by McKinsey. From a technical prospective, the queries are designed to span three different dimensions based on data sources, query processing types and analytic techniques. We illustrate the feasibility of BigBench by implement- ing it on the Teradata Aster Database. The test includes generating and loading a 200 Gigabyte BigBench data set and testing the workload by executing the BigBench queries (written using Teradata Aster SQL-MR) and reporting their response times},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.1406v2},
author = {Chen, H and Chiang, R. H. and Storey, V},
doi = {10.1145/2463676.2463712},
eprint = {arXiv:1401.1406v2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Chiang, Storey - 2013 - BUSINESS INTELLIGENCE AND ANALYTICS FROM BIG DATA TO BIG IMPACT.pdf:pdf},
isbn = {9781450320375},
issn = {07308078},
journal = {MIS Quarterly},
keywords = {0,big data analytics,business intelligence and analytics,web 2},
number = {4},
pages = {1165--1188},
pmid = {83466038},
title = {{BUSINESS INTELLIGENCE AND ANALYTICS: FROM BIG DATA TO BIG IMPACT}},
url = {http://dl.acm.org/citation.cfm?doid=2463676.2463712},
volume = {36},
year = {2013}
}
@article{Clift2011,
annote = {Paper in Sustainability. 

Very quotable.

Has a nice diagram of thermodynamics of the circular economy. We could use this. See also Clift paper about costs of mobile phone recycling; it ties in with the diagram.},
author = {Clift, R and Allwood, J M},
journal = {The Chemical Engineer},
keywords = {engineering,green,strategy,sustainability},
number = {837},
pages = {30--31},
title = {{Rethinking the economy}},
year = {2011}
}
@article{Belaire2015,
abstract = {As our world becomes increasingly urbanized, cities are often where we come into contact with the natural world-not just in parks and urban nature preserves, but in more familiar places like residential yards. We conducted bird surveys and social surveys in Chicago-area residential landscapes near forest preserves (primarily in middle- and highincome areas) to examine residents' perceptions of the birds that co-inhabit their neighborhoods and the relationship of those perceptions with characteristics of the bird community. We found that residents value many aspects of neighborhood birds, especially those related to aesthetics and birds' place in the ecosystem. Our results indicate that while birds were generally well liked and annoyances were minor, several common and visible urban species, such as the House Sparrow (Passer domesticus), European Starling (Sturnus vulgaris), and Blue Jay (Cyanocitta cristata), may attract attention for their negative qualities, such as their sounds and effects on personal property. The results also indicate that residents' valuations of ecosystem services are linked to their perceptions of bird species richness rather than the actual species richness, and people may perceive only a subset of the birds in their neighborhoods. Although birds provide many important ecosystem services, perhaps one of their most important roles in cities is as a relatable and likable connecting point between city dwellers and the broader environment. {\&}copy ;2015 Cooper Ornithological Society.},
author = {Belaire, J. Amy and Westphal, Lynne M. and Whelan, Christopher J. and Minor, Emily S.},
doi = {10.1650/condor-14-128.1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Belaire et al. - 2015 - Urban residents' perceptions of birds in the neighborhood Biodiversity, cultural ecosystem services, and disserv.pdf:pdf},
issn = {0010-5422},
journal = {The Condor},
month = {may},
number = {2},
pages = {192--202},
publisher = {Oxford University Press (OUP)},
title = {{Urban residents' perceptions of birds in the neighborhood: Biodiversity, cultural ecosystem services, and disservices}},
volume = {117},
year = {2015}
}
@article{Fiske2011,
abstract = {Ecological research uses data collection techniques that are prone to substantial and unique types of measurement error to address scientic questions about species abundance and distribution. These data collection schemes include a number of survey methods in which unmarked individuals are counted, or determined to be present, at spatially- referenced sites. Examples include site occupancy sampling, repeated counts, distance sampling, removal sampling, and double observer sampling. To appropriately analyze these data, hierarchical models have been developed to separately model explanatory variables of both a latent abundance or occurrence process and a conditional detection process. Because these models have a straightforward interpretation paralleling mecha- nisms under which the data arose, they have recently gained immense popularity. The common hierarchical structure of these models is well-suited for a unied modeling in- terface. The R package unmarked provides such a unied modeling framework, including tools for data exploration, model tting, model criticism, post-hoc analysis, and model comparison.},
author = {Fiske, Ian J. and Chandler, Richard B.},
doi = {10.18637/jss.v043.i10},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fiske, Chandler - 2011 - Unmarked An R package for fitting hierarchical models of wildlife occurrence and abundance.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Distance,Ecological,Hierarchical,Occupancy,Occurrence,Point count,Wildlife},
number = {10},
pages = {1--23},
publisher = {American Statistical Association},
title = {{Unmarked: An R package for fitting hierarchical models of wildlife occurrence and abundance}},
volume = {43},
year = {2011}
}
@article{Powell2008,
abstract = {Among those who study spreadsheet use, it is widely accepted that errors are prevalent in operational spreadsheets and that errors can lead to poor decisions and cost millions of dollars. However, relatively little is known about what types of errors actually occur, how they were created, how they can be detected, and how they can be avoided or minimized. This paper summarizes and critiques the research literature on spreadsheet errors from the viewpoint of a manager who wishes to improve operational spreadsheet quality. We also offer suggestions for future research directions that can improve the state of knowledge about spreadsheet errors and mitigate spreadsheet risks. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Powell, Stephen G. and Baker, Kenneth R. and Lawson, Barry},
doi = {10.1016/j.dss.2008.06.001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Powell, Baker, Lawson - 2008 - A critical review of the literature on spreadsheet errors.pdf:pdf},
isbn = {01679236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Big Data,Decision support,End-user computing,Error classification,Spreadsheets},
mendeley-tags = {Big Data},
month = {dec},
number = {1},
pages = {128--138},
publisher = {North-Holland},
title = {{A critical review of the literature on spreadsheet errors}},
url = {https://www.sciencedirect.com/science/article/pii/S0167923608001127},
volume = {46},
year = {2008}
}
@article{Codd1970,
abstract = {for Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information. Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data. In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced. In Section 2, certain opera-tions on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model. KEY WORDS AND PHRASES: data bank, data base, data structure, data organization, hierarchies of data, networks of data, relations, derivability, redundancy, consistency, composition, join, retrieval language, predicate calculus, security, data integrity CR CATEGORIES: 3.70, 3.73, 3.75, 4.20, 4.22, 4.29 1. R e l a t i o n a l Model and N o r m a l F o r m},
author = {Codd, E F},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Codd - 1970 - Information Retrieval A Relational Model of Data Large Shared Data Banks.pdf:pdf},
journal = {Communications of the ACM},
number = {6},
pages = {377--387},
title = {{Information Retrieval A Relational Model of Data Large Shared Data Banks}},
url = {https://cs.uwaterloo.ca/{~}david/cs848s14/codd-relational.pdf},
volume = {13},
year = {1970}
}
@article{PalmerPaulHenshaw2019,
abstract = {We introduce a novel conceptual framework using task orientated templates, for the analysis of large data that effectively separates: data curation, analysis, and reporting of large datasets, creating a reproducible analysis. Outputs saved include calculated secondary data, with associated metadata capturing all the transformations that have been applied, to provide an auditable connection to the source data. Data sharing is encouraged by many research funders and academic publishers, but supplying provenance is not mandatory. Enhancing data sharing will benefit many sectors as the pool of trusted data increases and is used with confidence in downstream analysis. While such benefits are likely to initially impact academic research due to active encouragement of data sharing, business intelligence processes will also benefit through increased confidence of source data. Using task orientated templates will allow a more structured approach to data analysis, and facilitate reuse of data through the use of verifiable digital signatures. This template based approach will reduce the programmatic skills required for the analysis large data for a wide range of commercial, academic and social applications on desktop computers.},
address = {Loughborough},
author = {Palmer, Paul; and Henshaw, Michael; and Lock, R},
doi = {10.31219/osf.io/ys2vw},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Palmer, Henshaw, Lock - 2019 - A Modular Task Orientated Approach for the Analysis of Large Datasets(2).pdf:pdf},
institution = {Loughborough University},
journal = {OSF 10.31219/osf.io/ys2vw},
title = {{A Modular Task Orientated Approach for the Analysis of Large Datasets}},
year = {2019}
}
@article{Persson1986,
abstract = {A method of using of commonly available online services for bibliometric studies is demonstrated. Distributions of papers by subfield, time, author and journal can be generated almost instantly and at very low cost. This article gives information on how to perform such studies.},
author = {Persson, O.},
doi = {10.1007/BF02016861},
isbn = {0138-9130},
issn = {01389130},
journal = {Scientometrics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jul},
number = {1-2},
pages = {69--75},
title = {{Online bibliometrics. A research tool for every man}},
url = {http://link.springer.com/10.1007/BF02016861},
volume = {10},
year = {1986}
}
@article{Macgregor2019,
abstract = {Steep insect biomass declines ('insectageddon') have been widely reported, despite a lack of continuously collected biomass data from replicated long-term monitoring sites. Such severe declines are not supported by the world's longest running insect population database: annual moth biomass estimates from British fixed monitoring sites revealed increasing biomass between 1967 and 1982, followed by gradual decline from 1982 to 2017, with a 2.2-fold net gain in mean biomass between the first (1967–1976) and last decades (2008–2017) of monitoring. High between-year variability and multi-year periodicity in biomass emphasize the need for long-term data to detect trends and identify their causes robustly.},
author = {Macgregor, Callum J. and Williams, Jonathan H. and Bell, James R. and Thomas, Chris D.},
doi = {10.1038/s41559-019-1028-6},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Macgregor et al. - 2019 - Moth biomass increases and decreases over 50 years in Britain.pdf:pdf;:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Macgregor et al. - 2019 - Moth biomass increases and decreases over 50 years in Britain(2).pdf:pdf},
issn = {2397-334X},
journal = {Nature Ecology {\&} Evolution},
month = {nov},
title = {{Moth biomass increases and decreases over 50 years in Britain}},
url = {http://www.nature.com/articles/s41559-019-1028-6},
year = {2019}
}
@techreport{RStudio2015,
abstract = {List of commands},
author = {{R Studio}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/R Studio - 2015 - Graphical Primitives Data Visualization with ggplot2 Geoms-Use a geom to represent data points, use the geom's aesthet.pdf:pdf},
keywords = {Continuous Y f {\textless}-ggplot(mpg,Continuous Y g {\textless}-ggplot(mpg,Discrete Y h {\textless}-ggplot(diamonds,R!,aes(class,aes(cty,aes(cut,aes(date,aes(hwy)) Two Variables Continuous Function Discre,aes(hwy)) Two Variables Discrete X,aes(long,aes(year,alpha,angle,color,color)) h + geom{\_}jitter() x,colour,family,fill,fill g + geom{\_}violin(scale = "area") x,fontface,hjust,hwy)) f + geom{\_}blank() f + geom{\_}jitter() x,hwy)) g + geom{\_}bar(stat = "identity") x,label,lat)) g {\textless}-ggplot(economics,lineheight,linetype,middle,rating)),shape,size,size Continuous Bivariate Distribution h {\textless}-ggplot(,size Discrete X,size f + geom{\_}point() x,size f + geom{\_}quantile() x,size f + geom{\_}smooth(model = lm) x,size g + geom{\_}line() x,size g + geom{\_}step(direction = "hv") x,sqrt(delta{\_}long{\^{}}2 + delta{\_}lat{\^{}}2)) i {\textless}-ggplot(seals,stackdir = "center") x,unemploy)) Continuous Function g + geom{\_}area() x,upper,vjust Three Variables i + geom{\_}contour(aes(z = z)),vjust alpha,weight Continuous X,weight Continuous a {\textless}-ggplot(mpg,weight f + geom{\_}rug(sides = "bl") alpha,weight f + geom{\_}text(aes(label = cty)) x,weight g + geom{\_}boxplot() lower,weight g + geom{\_}dotplot(binaxis = "y",weight seals{\$}z {\textless}-with(seals,x,y,ymax,ymin,z},
mendeley-tags = {R!},
pages = {2},
title = {{Graphical Primitives Data Visualization with ggplot2 Geoms-Use a geom to represent data points, use the geom's aesthetic properties to represent variables. Each function returns a layer One Variable}},
year = {2015}
}
@article{Foster2012,
abstract = {Summary. In ecology, a common form of statistical analysis relates a biological variable to variables that delineate the physical environment, typically by fitting a regression model or one of its extensions. Unfortunately, the biological data and the physical data are frequently obtained from separate sources of data. In such cases there is no guarantee that the biological and physical data are co-located and the regression model cannot be used. A common and pragmatic solution is to predict the physical variables at the locations of the biological variables and then to use the predictions as if they were observations. We show that this procedure can cause potentially misleading inferences and we use generalized linear models as an example. We propose a Berkson error model which overcomes the limitations. The differences between using predicted covariates and the Berkson error model are illustrated by using data from the marine environment, and a simulation study based on these data.},
author = {Foster, Scott D. and Shimadzu, Hideyasu and Darnell, Ross},
doi = {10.1111/j.1467-9876.2011.01030.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Foster, Shimadzu, Darnell - 2012 - Uncertainty in spatially predicted covariates Is it ignorable.pdf:pdf},
isbn = {1467-9876},
issn = {00359254},
journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
keywords = {Berkson error,Ecological modelling,Geostatistics,Measurement error},
month = {aug},
number = {4},
pages = {637--652},
title = {{Uncertainty in spatially predicted covariates: Is it ignorable?}},
url = {http://doi.wiley.com/10.1111/j.1467-9876.2011.01030.x},
volume = {61},
year = {2012}
}
@article{Boakes2010,
author = {Boakes, Elizabeth H. and McGowan, Philip J.K. and Fuller, Richard A. and Chang-Qing, Ding and Clark, Natalie E. and O'Connor, Kim and Mace, Georgina M.},
doi = {10.1371/journal.pbio.1000385},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Boakes et al. - 2010 - Distorted views of biodiversity Spatial and temporal bias in species occurrence data.pdf:pdf},
issn = {15449173},
journal = {PLoS Biology},
month = {jun},
number = {6},
title = {{Distorted views of biodiversity: Spatial and temporal bias in species occurrence data}},
volume = {8},
year = {2010}
}
@article{Lewis2018a,
abstract = {Changes in technology have made it possible to gather vast amounts of data, often of high quality, that in turn can improve the quality of wildlife biology. However, with this growth in data, practices such as data management, exploratory data analysis, data-sharing, and reproducibility of an analysis have become increasingly complex. These practices often depend heavily on computer scripting languages, and are often hidden from the peer-review process despite their influence on the final results. Although these issues have been discussed in the literature, they are generally dealt with in a piecemeal fashion, preventing synthesis, and thereby slowing progress. We offer a conceptual framework to illustrate relationships among these practices, and show where wildlife biology as a field has embraced these changes, where awareness is growing, and where it lags behind other fields. We then present several case studies to emphasize the importance of adopting these practices. Any of these case studies could have been conducted with little attention to these practices or employing scripting languages, but there are many disadvantages to this approach including increased chance of errors, inefficiency, and lack of reproducibility. We suggest that a change in the culture of how wildlife biology is conducted is required and that this change will be fostered by integrating these practices into wildlife biology education, implementation, and embracing the idea of open data and open computer code. {\'{O}} 2018 The Wildlife Society.},
author = {Lewis, Keith P. and {Vander Wal}, Eric and Fifield, David A.},
doi = {10.1002/wsb.847},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lewis, Vander Wal, Fifield - 2018 - Wildlife biology, big data, and reproducible research.pdf:pdf},
issn = {19385463},
journal = {Wildlife Society Bulletin},
keywords = {data management,data pipeline,exploratory data analysis,open science,reproducible research},
month = {mar},
number = {1},
pages = {172--179},
publisher = {Wiley-Blackwell},
title = {{Wildlife biology, big data, and reproducible research}},
url = {http://doi.wiley.com/10.1002/wsb.847},
volume = {42},
year = {2018}
}
@inproceedings{Dallachiesa2013,
abstract = {Despite the increasing importance of data quality and the rich theoretical and practical contributions in all aspects of data cleaning, there is no single end-to-end off-the-shelf solution to (semi-)automate the detection and the repairing of violations w.r.t. a set of heterogeneous and ad-hoc quality constraints. In short, there is no commodity platform similar to general purpose DBMSs that can be easily customized and deployed to solve application-specific data quality problems. In this paper, we present NADEEF, an extensible, generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows the users to specify multiple types of data quality rules, which uniformly define what is wrong with the data and (possibly) how to repair it through writing code that implements predefined classes. We show that the programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. Treating user implemented interfaces as black-boxes, the core provides algorithms to detect errors and to clean data. The core is designed in a way to allow cleaning algorithms to cope with multiple rules holistically, i.e. detecting and repairing data errors without differentiating between various types of rules. We showcase two implementations for core repairing algorithms. These two implementations demonstrate the extensibility of our core, which can also be replaced by other user-provided algorithms. Using real-life data, we experimentally verify the generality, extensibility, and effectiveness of our system.},
address = {New York, New York, USA},
author = {Dallachiesa, Michele and Ebaid, Amr and Eldawy, Ahmed and Elmagarmid, Ahmed and Ilyas, Ihab F. and Ouzzani, Mourad and Tang, Nan},
booktitle = {Proceedings of the 2013 international conference on Management of data - SIGMOD '13},
doi = {10.1145/2463676.2465327},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dallachiesa et al. - 2013 - NADEEF A Commodity Data Cleaning System.pdf:pdf},
isbn = {9781450320375},
keywords = {Big Data,conditional functional dependency,data cleaning,etl,matching dependency},
mendeley-tags = {Big Data},
pages = {541--552},
publisher = {ACM Press},
title = {{NADEEF: A Commodity Data Cleaning System}},
url = {http://dl.acm.org/citation.cfm?doid=2463676.2465327},
year = {2013}
}
@inproceedings{Rammelaere2017,
abstract = {n recent years, research on detecting inconsistencies in data has focused on a constraint-based data quality approach: a set of constraints in some logical formalism is associated with a database, and the data is considered consistent or clean if and only if all constraints are satisfied. Many such formalisms exist, capturing a wide variety of inconsistencies, and systematic ways of repairing the detected inconsistencies are in place. A frequently asked question is, “where do these constraints come from?”. The common answer is that they are either supplied by experts, or automatically discovered from the data [1], [2]. In most real-world scenario's, however, the underlying data is dirty. This raises concerns about the reliability of the discovered constraints.},
address = {San Diego, CA, USA},
author = {Rammelaere, Joeri and Geerts, Floris},
booktitle = {Data Engineering (ICDE)},
doi = {10.1109/ICDE.2017.138},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Rammelaere, Geerts - 2017 - Cleaning Data with Forbidden Itemsets.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {15},
title = {{Cleaning Data with Forbidden Itemsets}},
url = {http://ieeexplore.ieee.org/abstract/document/7930034/},
year = {2017}
}
@techreport{Lee2008,
abstract = {Cyber-Physical Systems (CPS) are integrations of com-putation and physical processes. Embedded computers and networks monitor and control the physical processes, usu-ally with feedback loops where physical processes affect computations and vice versa. The economic and soci-etal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are consider-able challenges, particularly because the physical compo-nents of such systems introduce safety and reliability re-quirements qualitatively different from those in general-purpose computing. Moreover, physical components are qualitatively different from object-oriented software com-ponents. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the ques-tion of whether today's computing and networking tech-nologies provide an adequate foundation for CPS. It con-cludes that it will not be sufﬁcient to improve design pro-cesses, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today's abstractions. To realize the full potential of CPS, we will have to re-build computing and networking abstractions. These ab-stractions will have to embrace physical dynamics and com-putation in a uniﬁed way.},
annote = {Document in Road2CPS. 

Contains a definition of CPS, very tightly bound to the physical, computational world:
A Cyber-Physical System (CPS) is “a network of interacting elements with physical input and output instead of as standalone devices.”


QUOTES!!
"There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in general-purpose computing. Moreover, physical components are qualitatively different from object-oriented software com-ponents. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether today's computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufﬁcient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today's abstractions." (from abstract)

"For example, the lack of temporal semantics and ade-quate concurrency models in computing, and today's “best effort” networking technologies make predictable and reli-able real-time performance difﬁcult, at best. Software com-ponent technologies, including object-oriented design and service-oriented architectures, are built on abstractions that match software much better than physical systems. Many of these applications may not be achievable without substan-tial changes in the core abstractions."

"The principle that we need to follow is simple. Components at any level of abstraction should be made predictable and reliable if this is technologically feasible. If it is not technologically feasible, then the next level of abstraction above these components must compensate with robustness.
[For example,] Successful designs today follow this principle. It is (still) technically feasible to make predictable and reliable gates. So we design systems that count on this. It is harder to make wireless links predictable and reliable. So we compensate one level up, using robust coding and adaptive protocols."

What he is on about:

"Given an imperative programming language with no concurrency, like C, designers can count on a computer to perform exactly what is speciﬁed with essentially 100{\%} reliability.

The problem arises when we scale up from simple programs to software systems, and particularly to cyber-physical systems. The fact is that even the simplest C program is not predictable and reliable in the context of CPS because the program does not express aspects of the behavior that are essential to the system. It may execute perfectly, exactly matching its semantics, and still fail to deliver the behavior needed by the system. For example, it could miss timing deadlines. Since timing is not in the semantics of C, whether a program misses deadlines is in fact irrelevant to determining whether it has executed correctly. But it is very relevant to determining whether the system has performed correctly. A component that is perfectly predictable and reliable turns out not to be predictable and reliable in the dimensions that matter. This is a failure of abstraction.

The problem gets worse as software systems get more complex. If we step outside C and use operating system primitives to perform I/O or to set up concurrent threads, we immediately move from essentially perfect predictabil-ity and reliability to wildly nondeterministic behavior that must be carefully reigned in by the software designer [19]. Semaphores, mutual exclusion locks, transactions, and pri-orities are some of the tools that software designers have developed to attempt to compensate for this loss of pre-dictability and reliability."

"Let us examine further the failure of abstraction. Figure 1 [see File Attachments] illustrates schematically some of the abstraction layers on which we depend when designing embedded systems. In this three-dimensional Venn diagram, each box represents a set. E.g., at the bottom, we have the set of all microproces-sors. An element of this set, e.g., the Intel P4-M 1.6GHz, is a particular microprocessor. Above that is the set of all x86 programs, each of which can run on that processor. This set is deﬁned precisely (unlike the previous set, which is difﬁ-cult to deﬁne) by the x86 instruction set architecture (ISA). Any program coded in that instruction set is a member of the set, such as a particular implementation of a Java vir-tual machine. Associated with that member is another set, the set of all JVM bytecode programs. Each of these programs is (typically) synthesized by a compiler from a Java program, which is a member of the set of all syntactically valid Java programs. Again, this set is deﬁned precisely by Java syntax.

Each of these sets provides an abstraction layer that is intended to isolate a designer (the person or program that selects elements of the set) from the details below. Many of the best innovations in computing have come from careful and innovative construction and deﬁnition of these sets.

However, in the current state of embedded software, nearly every abstraction has failed. The instruction-set ar-chitecture, meant to hide hardware implementation details from the software, has failed because the user of the ISA cares about timing properties the ISA does not guarantee. The programming language, which hides details of the ISA from the program logic, has failed because no widely used programming language expresses timing properties. Timing is merely an accident of the implementation. A real-time operating system hides details of the program from their concurrent orchestration, yet this fails because the timing may affect the result. The RTOS provides no guarantees. The network hides details of electrical or optical signaling from systems, but many standard networks provide no tim-ing guarantees and fail to provide an appropriate abstrac-tion. A system designer is stuck with a system design (not just implementation) in silicon and wires.

All embedded systems designers face versions of this problem. Aircraft manufacturers have to stockpile the elec-tronic parts needed for the entire production line of an air-craft model to avoid having to recertify the software if the hardware changes. “Upgrading” a microprocessor in an en-gine control unit for a car requires thorough re-testing of the system. Even “bug ﬁxes” in the software or hardware can be extremely risky, since they can change timing behavior."

Solutions suggested (among others):

"Complementing bottom-up approaches are top-down so-lutions that center on the concept of model-based design [28]. In this approach, “programs” are replaced by “mod-els” that represent system behaviors of interest. Software is synthesized from the models. This approach opens a rich semantic space that can easily embrace temporal dynamics (see for example [33]), including even the continuous temporal dynamics of the physical world.
...
Some of the most intriguing aspects of model-based de-sign center on explorations of rich possibilities for inter-face speciﬁcations and composition. Reﬂecting behavioral properties in interfaces, of course, has also proved useful in general-purpose computing (see for example [22]). But where we are concerned with properties that have not tra-ditionally been expressed at all in computing, the ability to develop and compose specialized “interface theories” [11] is extremely promising. These theories can reﬂect causality properties [34], which abstract temporal behavior, real-time resource usage [30], timing constraints [14], protocols [17], depletable resources [9], and many others [1]."
[11] L. deAlfaro and T. A. Henzinger. Interface theories for component-based design. In First International Workshop on Embedded Software (EMSOFT), volume LNCS 2211, pages 148–165, Lake Tahoe, CA, October, 2001 2001. Springer-Verlag.
[19] E. A. Lee. The problem with threads. Computer, 39(5):33–42, 2006},
author = {Lee, E A},
doi = {10.1109/ISORC.2008.25},
isbn = {UCB/EECS-2008-8.},
keywords = {Cyber-physical systems,IoT,agents,definitions},
publisher = {University of California, Berkeley},
title = {{Cyber Physical Systems: design challenges}},
year = {2008}
}
@article{Wang2015,
abstract = {Big Data analytics plays a key role through reducing the data size and complexity in Big Data applications. Visualization is an important approach to helping Big Data get a complete view of data and discover data values. Big Data analytics and visualization should be integrated seamlessly so that they work best in Big Data applications. Conventional data visualization methods as well as the extension of some conventional methods to Big Data applications are introduced in this paper. The challenges of Big Data visualization are discussed. New methods, applications, and technology progress of Big Data visualization are presented.},
author = {Wang, Lidong and Wang, Guanghui and Alexander, Cheryl Ann},
doi = {10.12691/dt-1-1-7},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Wang, Alexander - 2015 - Big Data and Visualization Methods, Challenges and Technology Progress.pdf:pdf},
journal = {Digital Technologies},
keywords = {Big Data,cloud computing,information technology,interactive visualization,networks,telecommunication systems,virtual reality,visualization},
mendeley-tags = {Big Data},
number = {1},
pages = {33--38},
title = {{Big Data and Visualization: Methods, Challenges and Technology Progress}},
url = {http://pubs.sciepub.com/dt/1/1/7},
volume = {1},
year = {2015}
}
@article{PENG2008,
abstract = {Despite the rapid development, the field of data mining and knowledge discovery (DMKD) is still vaguely defined and lack of integrated descriptions. This situation causes difficulties in teaching, learning, research, and application. This paper surveys a large collection of DMKD literature to provide a comprehensive picture of current DMKD research and classify these research activities into high-level categories using grounded theory approach; it also evaluates the longitudinal changes of DMKD research activities during the last decade.},
author = {PENG, YI and KOU, GANG and SHI, YONG and CHEN, ZHENGXIN},
doi = {10.1142/S0219622008003204},
isbn = {0769527027},
issn = {0219-6220},
journal = {International Journal of Information Technology {\&} Decision Making},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {04},
pages = {639--682},
title = {{A DESCRIPTIVE FRAMEWORK FOR THE FIELD OF DATA MINING AND KNOWLEDGE DISCOVERY}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0219622008003204},
volume = {07},
year = {2008}
}
@article{Spyns2002,
abstract = {Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. Unlike data models, the fundamental asset of ontologies is their relative independence of particular applications, i.e. an ontology consists of relatively generic knowledge that can be reused by different kinds of applications/tasks. The first part of this paper concerns some aspects that help to understand the differences and similarities between ontologies and data models. In the second part we present an ontology engineering framework that supports and favours the genericity of an ontology. We introduce the DOGMA ontology engineering approach that separates "atomic" conceptual relations from "predicative" domain rules. A DOGMA ontology consists of an ontology base that holds sets of intuitive context-specific conceptual relations and a layer of "relatively generic" ontological commitments that hold the domain rules. This constitutes what we shall call the double articulation of a DOGMA ontology.},
author = {Spyns, Peter and Meersman, Robert and Jarrar, Mustafa},
doi = {10.1145/637411.637413},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Spyns, Meersman, Jarrar - 2002 - Data modelling versus ontology engineering.pdf:pdf},
isbn = {0163-5808},
issn = {01635808},
journal = {ACM SIGMOD Record},
keywords = {data modelling,ontology and knowledge engineering},
month = {dec},
number = {4},
pages = {12},
publisher = {ACM},
title = {{Data modelling versus ontology engineering}},
url = {http://portal.acm.org/citation.cfm?doid=637411.637413},
volume = {31},
year = {2002}
}
@techreport{MinisterforState2012,
abstract = {Data is the 21st century's new raw material. Its value is in holding governments to account; in driving choice and improvements in public services; and in inspiring innovation and enterprise that spurs social and economic growth. In the last 20 years the world has opened up and citizens across the globe are proclaiming their right to data; this White Paper sets out how we intend to use that energy to unlock the potential of Open Data and for the first time the technology exists to make the demand for greater openness irresistible. We are at the start of a global movement towards transparency – and the UK is leading the world in making data more freely available. We are currently co-chairing the Open Government Partnership of 55 governments; the theme of our chairmanship is ‘Transparency Drives Prosperity' – demonstrating the value of open governance to economic growth, inclusive development and improved citizen engagement and empowerment. Transparency is at the heart of our agenda for government. We believe that opening up will empower citizens, foster innovation and reform public services. The regular publication of government spending is holding our feet to the fire all year round, not just at election time. We're creating an information marketplace for entrepreneurs and businesses; releasing valuable raw data from real-time transport information to weather data. Opening up data is underpinning our public service reforms by offering people informed choices that simply haven't existed before, exposing what is inadequate and driving improvement. So far we've released almost 9,000 datasets on our flagship data portal www.data.gov.uk that cover health, education, transport, crime and justice. People can scrutinise local crime statistics, sentencing rates, school results, hospital infection rates and GP outcomes. The transparency story by no means ends here. Today we're at a pivotal moment – where we consider the rules and ways of working in a data-rich world and how we can use this resource effectively, creatively and responsibly. This White Paper sets out clearly how the UK will continue to unlock and seize the benefits of data sharing in the future in a responsible way. First, to ensure that there are no inequalities in the data market we will enhance access to data. We are unflinching in our belief that data that can be published should be published. As well as continuing to produce statutory publication schemes under the Freedom of Information Act, all departments have now published their first ever Open Data Strategies which include commitments to publish more data. People's rights to access data have been strengthened in legislation, vehicles for redress will also be enhanced and standards for higher data usability introduced. Second, we will build greater trust in public data. The success of the information marketplace hinges on our ability to safeguard people's data from misuse and rigorously protect the public's right to privacy. We will ensure that privacy is not considered as an afterthought but at the beginning of all discussions concerning the release of a new dataset. We will ensure that we keep pace with the 6 Open Data White Paper latest technology so anonymised datasets remain anonymised and personal data remains personal. Third, to ensure that our public services are more personalised and efficient in the future we must be much smarter with the data public bodies hold. In the past the public sector has not been clever or effective at sharing key data. We are determined to shift the culture of the public sector to improve data sharing where it is in the public interest and within legislative boundaries, and we will use the latest technology to deliver this. There is nothing easy about transparency. The formative years of open government will be tricky, difficult and uncomfortable at times. But the prize is effective, personalised, 21st-century democracy. It's a more prosperous United Kingdom where the public services on which we all rely are strengthened and improved. We are determined to ensure that all of us can reap the benefits of transparency and data sharing in the future. The future will be Open.},
author = {{Minister for State}},
booktitle = {Printed in the UK for The Stationery Office Limited on behalf of the Controller of Her Majesty's Stationery Office},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Minister for State - 2012 - Open Data White Paper Unleashing the Potential.pdf:pdf},
institution = {UK Cabinet Office},
isbn = {9780101835329},
keywords = {Open data},
pages = {51},
title = {{Open Data White Paper: Unleashing the Potential}},
url = {www.cabinetoffice.gov.uk. https://data.gov.uk/library/open-data-white-paper},
year = {2012}
}
@article{Ofli2016,
abstract = {Aerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center ( JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be imple- mented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap). The results suggest that the platform we have developed to combine crowdsourcing and machine learning to make sense of large volumes of aerial images can be used for disaster response.},
archivePrefix = {arXiv},
arxivId = {arXiv:1310.5463},
author = {Ofli, Ferda and Meier, Patrick and Imran, Muhammad and Castillo, Carlos and Tuia, Devis and Rey, Nicolas and Briant, Julien and Millet, Pauline and Reinhard, Friedrich and Parkan, Matthew and Joost, St{\'{e}}phane},
doi = {10.1089/big.2014.0064},
eprint = {arXiv:1310.5463},
isbn = {2167-6461},
issn = {2167-6461},
journal = {Big Data},
keywords = {Big Data analytics,UAV,crowdsourcing,machine learning,remote sensing},
month = {mar},
number = {1},
pages = {47--59},
pmid = {27441584},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{Combining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response}},
url = {http://online.liebertpub.com/doi/10.1089/big.2014.0064},
volume = {4},
year = {2016}
}
@book{Bhaskar2008,
address = {London, UK},
author = {Bhaskar, Prof. Roy},
isbn = {9781844672042},
pages = {286},
publisher = {Verso},
title = {{A Realist Theory of Science}},
year = {2008}
}
@article{Kambatla2014,
abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications' considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1204.6078},
author = {Kambatla, Karthik and Kollias, Giorgos and Kumar, Vipin and Grama, Ananth},
doi = {10.1016/j.jpdc.2014.01.003},
eprint = {1204.6078},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kambatla et al. - 2014 - Trends in big data analytics.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Analytics,Big-data,Data centers,Distributed systems},
month = {jul},
number = {7},
pages = {2561--2573},
pmid = {21984515},
publisher = {Academic Press},
title = {{Trends in big data analytics}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514000057?via{\%}3Dihub},
volume = {74},
year = {2014}
}
@article{Bowers2004,
abstract = {Ecologists spend considerable effort integrating heterogene- ous data for statistical analyses and simulations, for example, to run and test predictive models. Our research is focused on reducing this effort by providing data integration and transformation tools, allowing researchers to focus on “real science,” that is, discovering new knowledge through analysis and modeling. This paper defines a generic framework for transforming heterogeneous data within scientific workflows. Our ap- proach relies on a formalized ontology, which serves as a simple, unstruc- tured global schema. In the framework, inputs and outputs of services within scientific workflows can have structural types and separate seman- tic types (expressions of the target ontology). In addition, a registration mapping can be defined to relate input and output structural types to their corresponding semantic types. Using registration mappings, ap- propriate data transformations can then be generated for each desired service composition. Here, we describe our proposed framework and an initial implementation for services that consume and produce XML data.},
author = {Bowers, Shawn},
doi = {10.1007/978-3-540-24745-6_1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bowers - 2004 - An Ontology-Driven Framework for Data Transformation in Scientific Workflows.pdf:pdf},
journal = {LECTURE NOTES IN COMPUTER SCIENCE},
pages = {1--16},
publisher = {Springer, Berlin, Heidelberg},
title = {{An Ontology-Driven Framework for Data Transformation in Scientific Workflows}},
url = {http://link.springer.com/10.1007/978-3-540-24745-6{\_}1},
volume = {2994},
year = {2004}
}
@article{Hill2017,
abstract = {Using data from social media can be of great value to businesses and other interested parties. However, harvesting data from social media networks such as Twitter, cleaning the data, and analyzing the data can be difficult. In this article, a step-by-step approach to obtaining data via the Twitter application program interface (API) is described. Cleaning of the data and basic sentiment analysis are also described.},
annote = {Although this looks difficult it is actually very easy to replicate with standard R packages.},
author = {Hill, Stephen and Scott, Rebecca},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hill, Scott - 2017 - Developing an Approach to Harvesting, Cleaning, and Analyzing Data from Twitter Using R.pdf:pdf},
journal = {Information Systems Education Journal},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {3},
pages = {42--54},
title = {{Developing an Approach to Harvesting, Cleaning, and Analyzing Data from Twitter Using R}},
url = {http://isedj.org/2017-15/n3/ISEDJv15n3p42.html},
volume = {15},
year = {2017}
}
@article{Tumminello2011,
abstract = {Many complex systems present an intrinsic bipartite structure where elements of one set link to elements of the second set. In these complex systems, such as the system of actors and movies, elements of one set are qualitatively different than elements of the other set. The properties of these complex systems are typically investigated by constructing and analyzing a projected network on one of the two sets (for example the actor network or the movie network). Complex systems are often very heterogeneous in the number of relationships that the elements of one set establish with the elements of the other set, and this heterogeneity makes it very difficult to discriminate links of the projected network that are just reflecting system's heterogeneity from links relevant to unveil the properties of the system. Here we introduce an unsupervised method to statistically validate each link of a projected network against a null hypothesis that takes into account system heterogeneity. We apply the method to a biological, an economic and a social complex system. The method we propose is able to detect network structures which are very informative about the organization and specialization of the investigated systems, and identifies those relationships between elements of the projected network that cannot be explained simply by system heterogeneity. We also show that our method applies to bipartite systems in which different relationships might have different qualitative nature, generating statistically validated networks in which such difference is preserved.},
archivePrefix = {arXiv},
arxivId = {1008.1414},
author = {Tumminello, Michele and Miccich{\`{e}}, Salvatore and Lillo, Fabrizio and Piilo, Jyrki and Mantegna, Rosario N.},
doi = {10.1371/journal.pone.0017994},
editor = {Ben-Jacob, Eshel},
eprint = {1008.1414},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Tumminello et al. - 2011 - Statistically validated networks in bipartite complex systems.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
month = {mar},
number = {3},
pages = {e17994},
pmid = {21483858},
publisher = {Public Library of Science},
title = {{Statistically validated networks in bipartite complex systems}},
url = {http://dx.plos.org/10.1371/journal.pone.0017994},
volume = {6},
year = {2011}
}
@article{Dobson2001,
abstract = {It is argued that recent research in the information systems field has tended to either emphasise the structural/collective dimension or the agency/individual dimension, not both. Structuration theory is a more recent attempt to address both agency and structure, however there are a number of issues with the use of structuration theory in information systems research, not the least of which is its lack of recognition of the temporal and longitudinal nature of information systems development. A relatively new philosophy, critical realism, provides the potential fora new approach to social investigations in its provision of an ontology for the analytical separation of structure and agency. The philosophy is introduced and its implications for sociological investigation are discussed.},
author = {Dobson, Philip J.},
doi = {10.1023/A:1011495424958},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dobson - 2001 - The Philosophy of Critical Realism - An Opportunity for Information Systems Research.pdf:pdf},
issn = {13873326},
journal = {Information Systems Frontiers},
keywords = {Agency,Critical realism,Morphogenetic theory,Structuration theory,Structure},
number = {2},
pages = {199--210},
title = {{The Philosophy of Critical Realism - An Opportunity for Information Systems Research}},
volume = {3},
year = {2001}
}
@article{Moed2015,
abstract = {This article introduces the Multidimensional Research Assessment Matrix of scientific output. Its base notion holds that the choice of metrics to be applied in a research assessment process depends on the unit of assessment, the research dimension to be assessed, and the purposes and policy context of the assessment. An indicator may by highly useful within one assessment process, but less so in another. For instance, publication counts are useful tools to help discriminate between those staff members who are research active, and those who are not, but are of little value if active scientists are to be compared with one another according to their research performance. This paper gives a systematic account of the potential usefulness and limitations of a set of 10 important metrics, including altmetrics, applied at the level of individual articles, individual researchers, research groups, and institutions. It presents a typology of research impact dimensions and indicates which metrics are the most appropriate to measure each dimension. It introduces the concept of a "meta-analysis" of the units under assessment in which metrics are not used as tools to evaluate individual units, but to reach policy inferences regarding the objectives and general setup of an assessment process.},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Moed, Henk F. and Halevi, Gali},
doi = {10.1002/asi.23314},
eprint = {0803.1716},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Moed, Halevi - 2015 - Multidimensional assessment of scholarly research impact.pdf:pdf},
isbn = {2330-1643},
issn = {23301643},
journal = {Journal of the Association for Information Science and Technology},
keywords = {evaluation},
month = {oct},
number = {10},
pages = {1988--2002},
pmid = {502955140},
title = {{Multidimensional assessment of scholarly research impact}},
url = {http://doi.wiley.com/10.1002/asi.23314 http://onlinelibrary.wiley.com/doi/10.1002/asi.23314/full},
volume = {66},
year = {2015}
}
@article{Griffing2011,
abstract = {On 27 March 1689, Richard Waller, Fellow and Secretary of the Royal Society presented his "Tables of the English Herbs reduced to such an order, as to find the name of them by their external figures and shapes" to his assembled colleagues at a meeting of the Royal Society. These tables were developed for the novice by being color images, composed in pencil and watercolor, of selected plants and their distinguishing characteristics. The botanical watercolors for the tables are now a Turning-the-Pages document online on the website of the Royal Society. However, for the past 320 years, the scientific context for the creation of these outstanding botanical watercolors has remained obscure. These tables were developed by Waller as an image-based dichotomous key, pre-dating by almost 100 years the text-based dichotomous keys in the first edition of Flora Fran{\c{c}}aise (1778) by Jean Baptiste Lamarck, who is generally given priority for the development of the dichotomous key. How these large folio images were arranged to illustrate a dichotomous key is unknown, but an arrangement based on Waller's description is illustrated here as leaf-ordering for the separate hierarchical clusters (tables). Although only 24 species of watercolored dicot herbs out of a total of 65 in the set of watercolors (the others being monocots) are used in these tables, they are a "proof of concept", serving as models upon which a method is based, that of using a key composed of dichotomous choices for aiding identifi cation. {\textcopyright} 2011 Botanical Society of America.},
author = {Griffing, Lawrence R.},
doi = {10.3732/ajb.1100188},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Griffing - 2011 - Who invented the dichotomous key Richard Waller's watercolors of the herbs of Britain.pdf:pdf},
issn = {00029122},
journal = {American Journal of Botany},
keywords = {Botanical illustration,Dichotomous key,Hierarchical clusters,Historia Plantarum,John Ray,Leaf-ordering,Royal Society},
month = {dec},
number = {12},
pages = {1911--1923},
pmid = {22074776},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Who invented the dichotomous key? Richard Waller's watercolors of the herbs of Britain}},
url = {http://doi.wiley.com/10.3732/ajb.1100188},
volume = {98},
year = {2011}
}
@techreport{Applications2008,
abstract = {Inhalt: 1. Starting Up 2. An Overview of R 3. Plotting 4. Lattice Graphics 5. Linear (Multiple Regressions) Models And Analysis Of Variance 6. Multivariate and Tree-based Methods 7. R Data Structures 8. Functions 9. GLM, and General Non-linear Models 10. Multi-level Models, Repeated Meaures and Time Series 11. Advanced Programming Topics 12. Appendix 1},
author = {Maindonald, J H},
doi = {10.1002/hbm.20951},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Maindonald - 2008 - Using R for Data Analysis and Graphics Introduction , Code and Commentary.pdf:pdf},
institution = {Australian National University},
isbn = {1097-0193; 1065-9471},
issn = {1654-1103},
pages = {1--99},
pmid = {20108225},
title = {{Using R for Data Analysis and Graphics Introduction , Code and Commentary}},
url = {http://www.mendeley.com/research/using-r-for-data-analysis-and-graphics-introduction-code-and-commentary-2/},
year = {2008}
}
@article{Simonsen2003,
abstract = {Significant positive correlation between wing length and scale length was found in a broad sample of lepidopteran taxa. No breakdown of the correlation was observed in the lowest wing size range. The best fit is a non-linear (power) curve. The minimum individual scale length observed was 40 $\mu$m (in a nepticulid), while the maximum was close to 500 $\mu$m (in a castniid; there are literature records of even larger scales in this family). Scale size variation probably reflects variation in the size of scale-forming trichogen cells. The latter, in turn, may at least partly reflect difference in genome size, such as has long been known to be the case with the cells that form, respectively, the cover and ground scales in some higher moths.},
author = {Simonsen, Thomas J. and Kristensen, Niels P.},
doi = {10.1080/00222930110096735},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Simonsen, Kristensen - 2003 - Scale lengthwing length correlation in Lepidoptera (Insecta).pdf:pdf},
isbn = {0022-2933},
issn = {00222933},
journal = {Journal of Natural History},
keywords = {Cell size,Lepidoptera,Wing length,Wing scales},
month = {mar},
number = {6},
pages = {673--679},
pmid = {1329},
publisher = {Taylor {\&} Francis Group},
title = {{Scale length/wing length correlation in Lepidoptera (Insecta)}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00222930110096735},
volume = {37},
year = {2003}
}
@misc{Wikipedia2017,
abstract = {The European robin (Erithacus rubecula), known simply as the robin or robin redbreast in the British Isles, is a small insectivorous passerine bird, specifically a chat, that was formerly classified as a member of the thrush family (Turdidae) but is now considered to be an Old World flycatcher. About 12.5–14.0 cm (5.0–5.5 inch) in length, the male and female are similar in colouration, with an orange breast and face lined with grey, brown upperparts and a whitish belly. It is found across Europe, east to Western Siberia and south to North Africa; it is sedentary in most of its range except the far north.},
author = {Wikipedia},
title = {{European robin}},
url = {https://en.wikipedia.org/wiki/European{\_}robin},
urldate = {2017-08-24},
year = {2017}
}
@article{Yu2000,
abstract = {Manufacturing enterprises must satisfy customers' needs, in order to sustain their position in the competitive market. This requires the enterprises to continuously design and produce new products. To handle new product introductions or changes to existing product ranges, enterprises must modify or  re-design their processes. Enterprise design and implementation is very expensive and time consuming. Hence, application of modelling techniques can be beneficial, first to avoid mistakes, which may result in loss of market position, and secondly to test alternative designs with minimum cost. Modelling a complex manufacturing enterprise requires a great amount of varied information. Thus, efficient collection and assessment of information are key factors in successful design. This paper introduces a multi-view support tool, called Factory Design Process Views (FDP Views), which aims to facilitate the capture, evaluation, access and modification of all the information required by designers and management during an enterprise design.},
author = {Yu, Bing and {Anne Harding}, Jennifer and Popplewell, Keith},
doi = {10.1108/14654650010312633},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Anne Harding, Popplewell - 2000 - Supporting enterprise design through multiple views.pdf:pdf},
issn = {1465-4652},
journal = {International Journal of Agile Management Systems},
keywords = {Information modelling,Manufacturing,Models,Organizational design},
month = {apr},
number = {1},
pages = {71--83},
publisher = {Emerald},
title = {{Supporting enterprise design through multiple views}},
volume = {2},
year = {2000}
}
@article{Poria2009,
abstract = {Research on interpretation at heritage settings commonly centers on the display. The current study highlights visitor pref-erences for on-site interpretation, an essential element in the management of heritage tourist attractions. This research focuses on the Wailing Wall, a religious " must-see " attraction in Jerusalem. The role of interpretation as a facilitator of emo-tional experience rather than a means to gain knowledge is explored. Results indicate the need to customize the interpreta-tion to meet visitor preferences and motives. Furthermore, the study reveals the need to capture heritage tourism not only as a search for na{\"{i}}ve nostalgia or a simplified romantic version of the past but also as a more complex phenomenon. Implications for marketers and heritage site operators are suggested, highlighting the need to adopt innovative approaches to the management of heritage tourist attractions and provide different interpretations for different visitors. H eritage tourism, whether defined as visits to cultural settings or visits to spaces considered by the visitors as relevant to their own heritage, is one of the fastest grow-ing tourism sectors (Bonn et al. 2007). Research on her-itage tourism sites often focuses on the display rather than on latent heritage (Caton and Santos 2007), a line of research that often highlights the concept of " power, " emphasizing the stakeholders' impact on the presentation and interpretation. The current study adopts Tunbridge and Ashworth's (1996, p. 69) approach that the study of her-itage settings " must shift from the uses of heritage to the users themselves and thus from the producers (whether cultural institutions, governments or enterprises) to the consumers. " Specifically, the approach presented here focuses on the visitors, exploring the relationships between the visitors' perception of the site relative to their own her-itage and their preferences toward on-site interpretation. The significance of this relationship derives from studies in areas such as environmental psychology, human geogra-phy, and sociology of tourism as well as heritage site man-agement and construction of history and commemoration. The main assertion in this study is that research on tourist experiences in spaces presenting cultural assets should be based on the interrelationships among site attributes, visitors, and presentation. Specifically, in line with the " experientially-based " approach (Apostolakis 2003, p. 799), the current research explores visitors' per-ceptions of the site relative to their personal heritage, which may be linked to individual personal characteris-tics such as gender, age, religious affiliation, and prefer-ences with respect to elements of the on-site interpretation. These interrelationships, especially pref-erences for interpretation, which are often ignored in heritage tourism literature, are essential to the manage-ment of heritage tourist attractions. The study also pro-vides a better understanding of people's experience of heritage settings by clarifying visitors' subjective and personal perceptions of the site and their motives for vis-iting. Thus, the view that only a na{\"{i}}ve search for nostal-gia or a simplified commoditized past is at the core of heritage tourism is challenged. Highlighting visitor perceptions and preferences rela-tive to the interpretation is most valuable for the man-agement of heritage attractions. The findings shed light on the need to mass customize visitor experience of her-itage settings rather than provide monolithic experiences only; this is specifically the case where there is an inter-est in attracting visitors and increasing revenues.},
author = {Poria, Yaniv and Biran, Avital and Reichel, Arie},
doi = {10.1177/0047287508328657},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Poria, Biran, Reichel - Unknown - Visitors' Preferences for Interpretation at Heritage Sites.pdf:pdf},
issn = {0047-2875},
journal = {Journal of Travel Research},
keywords = {interpretation,perception,personal heritage,preferences,tourist experiences},
month = {aug},
number = {1},
pages = {92--105},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Visitors' Preferences for Interpretation at Heritage Sites}},
url = {http://journals.sagepub.com/doi/pdf/10.1177/0047287508328657 http://journals.sagepub.com/doi/10.1177/0047287508328657},
volume = {48},
year = {2009}
}
@inproceedings{Billestrup2014,
abstract = {—The Personas technique has been promoted as a strong tool for providing software developers with a better understanding of the prospective users of their software. This paper reports from a questionnaire survey regarding knowledge about Personas and their usage in software development companies. The questionnaire survey was conducted in a limited geographical area to establish the extent of Personas usage within all companies in the chosen region and determine whether software development companies used Personas during the development process. Several issues were identified as reasons for either not using the technique or for poor application of it. The study showed that 55{\%} of the respondents had never heard about Personas. Among those who had heard about the Personas technique, the following obstacles towards usage of the technique were identified: Lack of knowledge of the technique, lack of resources (time and funding), Sparse descriptions – when applied and Personas not being integrated in the development.},
address = {Barcelona, Spain},
author = {Billestrup, Jane and Stage, Jan and Nielsen, Lene and Hansen, Kira S},
booktitle = {Conference on Advances in Computer-Human Interactions},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Billestrup et al. - 2014 - Persona usage in software development Advantages and obstacles.pdf:pdf},
isbn = {9781612083254},
keywords = {-application,center for personas research,grounded theory,hansen,kira s,lene nielsen,personas,questionnaire,software development,survey},
pages = {359--364},
title = {{Persona usage in software development : Advantages and obstacles}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.671.163{\&}rep=rep1{\&}type=pdf},
year = {2014}
}
@article{Angeler2020,
abstract = {►1. Rapidly changing ecological and social systems currently pose significant societal challenges. Navigating the complexity of social‐ecological change requires approaches able to cope with, and potentially solve, both foreseen and unforeseen societal challenges. ►2. The emergent field of convergence addresses the intricacies of such challenges, and is thus relevant to a broad range of interdisciplinary issues. ►3. This paper suggests a way to conceptualize convergence research. It discusses how it relates to two major societal challenges (adaptation, transformation), and to the generation of policy‐relevant science. It also points out limitations to the further development of convergence research.},
author = {Angeler, David G. and Allen, Craig R. and Carnaval, Ana},
doi = {10.1002/pan3.10069},
editor = {Palmer, Clare},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Angeler, Allen, Carnaval - 2020 - Convergence science in the Anthropocene Navigating the known and unknown.pdf:pdf},
issn = {2575-8314},
journal = {People and Nature},
keywords = {adaptation,convergence,interdisciplinary science,resilience,societal change,sustainability,transformation},
month = {mar},
number = {1},
pages = {96--102},
publisher = {Wiley},
title = {{Convergence science in the Anthropocene: Navigating the known and unknown}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pan3.10069},
volume = {2},
year = {2020}
}
@book{Pyle1993,
abstract = {An engrossing memoir and eloquent portrait of place, The Thunder Tree shows how powerful the relationship between people and the natural world can be. "When people connect with nature, it happens somewhere," Pyle writes. "My own point of intimate contact with the land was a ditch... Without a doubt, most of the elements of my life flowed from that canal." The High Line Canal, originally built outside of Denver, Colorado, as part of a plan to move river water to the Western plains for irrigation, became the author's place of sanctuary and play, and his birthplace as a naturalist. This reprint of the classic book, updated with a new foreword by Richard Louv and a preface to this edition, makes one of Pyle's important early works once again available. For a new generation of readers, it offers a powerful argument for preserving opportunities for exploring nature.},
address = {Boston, MA},
author = {Pyle, Robert Michael},
edition = {2011},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pyle - 1993 - Thunder Tree Lessons from an Urban Wildland.pdf:pdf},
isbn = {978-0870716027},
pages = {224},
publisher = {Oregon State University Press},
title = {{Thunder Tree: Lessons from an Urban Wildland}},
year = {1993}
}
@article{Mitsyn2015,
abstract = {{\textcopyright} 2015, Pleiades Publishing, Ltd. A method for clustering of large amounts of data is presented which is a sequenced composition of two algorithms: the former builds a partition of input space into Voronoi regions and the latter partitions them. First, a model of clusters as high-density regions in input space is presented, then it is shown how a Voronoi partition and it's topological map (a) can be build and (b) used as a low complexity approximation of the input space. During the (b) step, the usage of “watershed” algorithm is presented which has been previously used for image segmentation, but it is its first application to a data space partition that is proposed by the authors.},
author = {Mitsyn, S. V. and Ososkov, G. A.},
doi = {10.1134/S1547477115010173},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mitsyn, Ososkov - 2015 - Watershed on vector quantization for clustering of big data.pdf:pdf},
issn = {1547-4771},
journal = {Physics of Particles and Nuclei Letters},
month = {jan},
number = {1},
pages = {170--172},
publisher = {Pleiades Publishing},
title = {{Watershed on vector quantization for clustering of big data}},
url = {http://link.springer.com/10.1134/S1547477115010173},
volume = {12},
year = {2015}
}
@misc{RCoreTeamandcontributorsworldwide2018,
author = {{R Core Team and contributors worldwide}},
title = {{R: The R Datasets Package version 3.5.1}},
url = {https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html},
urldate = {2018-10-31},
year = {2018}
}
@article{Knuth1984,
author = {Knuth, D. E.},
doi = {10.1093/comjnl/27.2.97},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Knuth - 1984 - Literate Programming(2).pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
month = {feb},
number = {2},
pages = {97--111},
publisher = {Oxford University Press},
title = {{Literate Programming}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/27.2.97},
volume = {27},
year = {1984}
}
@article{Yang2011,
abstract = {{\textless}div class="abstract" data-abstract-type="normal"{\textgreater}{\textless}p{\textgreater}Scanning electron microscopy on the ultrastructure of scales on the forewings and labial palpi suggests species-diagnostic differences among six sibling species of the genus {\textless}span class='italic'{\textgreater}Ostrinia{\textless}/span{\textgreater} H{\"{u}}bner. Among four species with small mid-tibiae, {\textless}span class='italic'{\textgreater}O. furnacalis{\textless}/span{\textgreater} (Guen{\'{e}}e) and {\textless}span class='italic'{\textgreater}O. nubilalis{\textless}/span{\textgreater} (H{\"{u}}bner) show similar ultrastructure of the distal forewing scales, which is distinctly different from that of {\textless}span class='italic'{\textgreater}O. orientalis{\textless}/span{\textgreater} Mutuura and Munroe and {\textless}span class='italic'{\textgreater}O. dorsivittata{\textless}/span{\textgreater} (Moore). The diameter of windows between longitudinal ridges and cross ribs of forewing scales in {\textless}span class='italic'{\textgreater}O. dorsivittata{\textless}/span{\textgreater} is the largest among examined species, and clearly different from that in the other three small mid-tibiae species. Scales of the labial palpi of {\textless}span class='italic'{\textgreater}O. orientalis{\textless}/span{\textgreater} have indistinct vestigial windows; windows of {\textless}span class='italic'{\textgreater}O. nubilalis{\textless}/span{\textgreater} are more numerous and larger than in the other three small mid-tibiae species. Among two species with massive mid-tibiae, window diameter of forewing scales is larger in {\textless}span class='italic'{\textgreater}O. zealis{\textless}/span{\textgreater} (Guen{\'{e}}e) than in {\textless}span class='italic'{\textgreater}O. scapulalis{\textless}/span{\textgreater} (Walker). Moreover, the number and diameter of windows in scales of the labial palpi differs between these two species. In addition to other known morphological differences, these ultrastructural differences provide further evidence that closely related {\textless}span class='italic'{\textgreater}Ostrinia{\textless}/span{\textgreater} species are distinct.{\textless}/p{\textgreater}{\textless}/div{\textgreater}},
author = {Yang, Zhaofu and Zhang, Yalin},
doi = {10.4039/n10-049},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Zhang - 2011 - Comparison of ultrastructure among sibling species of iOstriniai (Lepidoptera Crambidae) from China.pdf:pdf},
issn = {0008-347X},
journal = {The Canadian Entomologist},
month = {mar},
number = {2},
pages = {126--135},
publisher = {Cambridge University Press},
title = {{Comparison of ultrastructure among sibling species of {\textless}i{\textgreater}Ostrinia{\textless}/i{\textgreater} (Lepidoptera: Crambidae) from China}},
url = {http://journals.cambridge.org/abstract{\_}S0008347X00000109},
volume = {143},
year = {2011}
}
@article{Cohen2009,
abstract = {As massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this paper we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present our design philosophy, techniques and experience providing MAD analytics for one of the world's largest advertising networks at Fox Audience Network, using the Greenplum parallel database system. We describe database design methodologies that support the agile working style of analysts in these settings. We present dataparallel algorithms for sophisticated statistical techniques, with a focus on density methods. Finally, we reflect on database system features that enable agile design and flexible algorithm development using both SQL and MapReduce interfaces over a variety of storage mechanisms.},
author = {Cohen, Jeffrey and Dolan, Brian and Dunlap, Mark and Hellerstein, Joseph M and Welton, Caleb},
doi = {10.14778/1687553.1687576},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cohen et al. - 2009 - MAD Skills New Analysis Practices for Big Data.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowmen},
keywords = {Big data,VLDB,big data analytics},
number = {2},
pages = {1481--1492},
title = {{MAD Skills : New Analysis Practices for Big Data}},
url = {http://delivery.acm.org/10.1145/1690000/1687576/p1481-cohen.pdf?ip=158.125.51.236{\&}id=1687576{\&}acc=ACTIVE SERVICE{\&}key=BF07A2EE685417C5.F25E547D993D41C5.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1531161274{\_}3b733afa12b68933243e8c544eb1e2e3 http://dl.acm.org/c},
volume = {2},
year = {2009}
}
@inproceedings{palmer2000strategicenvironment,
author = {Palmer, P J and Hyslop, S M},
booktitle = {IEE Intellectual Property Seminar Publication No. 00/082},
month = {jul},
organization = {Edinburgh},
pages = {8/1----8/4},
title = {{Strategic Re-use of IP in a modern business environment}},
year = {2000}
}
@article{Abramo2011,
author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea},
doi = {10.1007/s11192-011-0352-7},
issn = {0138-9130},
journal = {Scientometrics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jun},
number = {3},
pages = {499--514},
title = {{Evaluating research: from informed peer review to bibliometrics}},
url = {http://link.springer.com/10.1007/s11192-011-0352-7},
volume = {87},
year = {2011}
}
@inproceedings{CortesRios2019,
abstract = {To bridge the digital skills gap, we need to train more people in Software Engineering techniques. This paper reports on a project exploring the way students solve tasks using collaborative development platforms and version control systems, such as GitLab, to find patterns and evaluation metrics that can be used to improve the course content and reflect on the most common issues the students are facing. In this paper, we explore Learning Analytics approaches that can be used with GitLab and similar tools, and discuss the challenges raised when applying those approaches in Software Engineering Education, with the objective of building a pipeline that supports the full Learning Analytics cycle, from data extraction to data analysis. We focus in particular on the data anonymisation step of the proposed pipeline to explore the available alternatives to satisfy the data protection requirements when handling personal information in academic environments for research purposes.},
author = {{Cortes Rios}, Julio Cesar and Kopec-Harding, Kamilla and Eraslan, Sukru and Page, Christopher and Haines, Robert and Jay, Caroline and Embury, Suzanne M.},
doi = {10.1109/chase.2019.00009},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cortes Rios et al. - 2019 - A Methodology for Using GitLab for Software Engineering Learning Analytics.pdf:pdf},
isbn = {9781728122397},
month = {aug},
pages = {3--6},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{A Methodology for Using GitLab for Software Engineering Learning Analytics}},
year = {2019}
}
@inproceedings{webb2006integrationovermoulding,
author = {Webb, D P and Hutt, D A and Hopkinson, N and Palmer, P J and Conway, P P},
booktitle = {Proceedings of the 1st Electronics Systemintegration Technology Conference},
organization = {Dresden, Germany},
pages = {567--574},
title = {{Integration and packaging of microsystems by polymer overmoulding}},
year = {2006}
}
@book{Beck2004,
address = {Boston, MA},
annote = {Rated highly.},
author = {Beck, K and Andres, C},
edition = {2nd},
isbn = {978-0321278654},
keywords = {software},
publisher = {Addison-Wesley},
title = {{Extreme Programming explained:  embrace change}},
year = {2004}
}
@techreport{Abowd2017,
abstract = {Based on NBER Summer Institute Methods Lecture Originally delivered July 27, 2017},
author = {Abowd, JM},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Abowd - 2017 - Large-scale Data Linkage from Multiple Sources Methodology and Research Challenges.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {64},
title = {{Large-scale Data Linkage from Multiple Sources: Methodology and Research Challenges}},
url = {https://ecommons.cornell.edu/bitstream/handle/1813/53361/Large-scale Data Linkage from Multiple Sources Methodology and Research Challenges-20171026.pdf?sequence=3},
year = {2017}
}
@article{Peritz1990,
abstract = {The fit of bradford's law to bibliometrics - a field which is both$\backslash$ninterdisciplinary and relatively new was investigated. It is found$\backslash$nthat, contrary to expectations, the data fit bradford's law very$\backslash$nwell, particularly in the more recent period, 1979-1983. there are,$\backslash$nin both periods studied, seven core journals with about 30{\%} of the$\backslash$npapers; most of these journals are specialized in information science$\backslash$nor documentation. no "falling away" from bradford's distribution$\backslash$ntowards the right-hand end of the bibliography was observed.},
author = {Peritz, Bluma C.},
doi = {10.1007/BF02020148},
isbn = {0138-9130},
issn = {01389130},
journal = {Scientometrics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {may},
number = {5-6},
pages = {323--329},
title = {{A bradford distribution for bibliometrics}},
url = {http://link.springer.com/10.1007/BF02020148},
volume = {18},
year = {1990}
}
@article{hyslop2005aimages,
annote = {The tool described in this paper remains unique as far as the authors know. The knowledge and understand has been incorporated in other projects and short courses as a way of quantifying the limits to integration and size reduction.},
author = {Hyslop, S M and Palmer, P J and Whalley, D C},
doi = {10.1108/03056120510585018},
issn = {0305-6120},
journal = {Circuit World},
number = {(3)},
pages = {10--16},
title = {{A software tool for estimation of PCB substrate utilisation efficiency statistics from scanned images}},
url = {http://dx.doi.org/10.1108/03056120510585018},
volume = {31},
year = {2005}
}
@article{Wickham2014,
abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
author = {Wickham, Hadley},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wickham - 2014 - Tidy Data.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {10},
pages = {1--22},
publisher = {[UCLA Statistics]},
title = {{Tidy Data}},
url = {http://apps.webofknowledge.com/Search.do?product=WOS{\&}SID=D5amRlmtrNwhg76w8QF{\&}search{\_}mode=GeneralSearch{\&}prID=64653d37-195b-4960-b053-28093eba42d4},
volume = {59},
year = {2014}
}
@book{MacKenzie,
abstract = {Second edition. Occupancy Estimation and Modeling: Inferring Patterns and Dynamics of Species Occurrence, Second Edition, provides a synthesis of model-based approaches for analyzing presence-absence data, allowing for imperfect detection. Beginning from the relatively simple case of estimating the proportion of area or sampling units occupied at the time of surveying, the authors describe a wide variety of extensions that have been developed since the early 2000s. This provides anï{\AA}½improved insight about species and community ecology, including, detection heterogeneity; correlated detections; spatial autocorrelation; multiple states or classes of occupancy; changes in occupancy over time; species co-occurrence; community-level modeling, and more. Occupancy Estimation and Modeling: Inferring Patterns and Dynamics of Species Occurrence, Second Edition has been greatly expanded and detail is providedï{\AA}½regarding the estimation methods and examples of their application are given. Important study design recommendations are also covered to give a well rounded view of modeling.},
annote = {Not in lboro library.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mackenzie, Darryl I and Nichols, James D and Royle, J. Andrew and Pollock, Kenneth H and Bailey, Larissa L and Hines, James E.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
pages = {343},
pmid = {25246403},
publisher = {Elsevier Academic Press},
title = {{Occupancy estimation and modeling. Inferring patterns and dynamics of species occurrence.}},
year = {2006}
}
@techreport{CPSoS-152015,
abstract = {The CPSoS project is developing a roadmap for future research activities in cyber-physical systems of systems. To support this process, the project has set up three Working Groups to capture the views of industry and academia:    •  Systems of Systems in Transportation and Logistics, •  Physically Connected Systems of Systems, and, •  Tools for Systems of Systems Engineering and Management.  The Working Groups currently comprise of 36 members, leading specialists from industry and academia, and include delegates from ongoing EU-funded projects in the area of Systems of Systems to ensure that as many views as possible are represented. A list of the members of the Working Groups can be found in the appendix.   This document presents preliminary findings and proposals that are put forward as a result of internal discussions in the consortium, in the three Working Groups (WGs) and at public meetings of the Working Groups. A number of meetings have been held starting with a joint meeting in January 2014 where the scope of cyber-physical systems of systems research and innovation was discussed and first proposals for research and innovation topics were collected and prioritized. A much wider consultation was then held via performance of a large number of interviews with representatives of industry in the domains of transportation and logistics, electrical grid management, the process industries and smart buildings.    The domain of cyber-physical systems of systems and the key research and innovation challenges in CPSoS were presented by the Coordinator and discussed at the HYCON II Final Workshop at the European Control Conference in June 2014 and in a Panel Discussion at the IFAC World Congress in August 2014. Interestingly, the views of leading experts from industry, academia and funding institutions from Europe, Japan and the US largely coincided in stressing the need to address system-wide issues in present-day socio-technical systems.    A second round of Working Group meetings with wider external participation of key actors from their domains was organized in September/October 2014. Here the initial findings were presented and further additional input on research and innovation priorities was gathered.    On the basis of the input obtained by the interviews, the discussions in the WGs, and further discussions in the consortium, this first draft of a roadmap document for CPSoS has been produced identifying three key challenges:   •  Distributed, reliable and efficient management of cyber-physical systems of systems •  Engineering support for the design-operation continuum of cyber-physical systems of systems •  Cognitive cyber-physical systems of systems.  This report firstly gives a brief overview of cyber-physical systems of systems, their specific features and challenges in operation and design. Building upon this analysis, the three main areas that have been identified as key for future research and innovation are then outlined.   This initial document will be posted for public consultation in February 2015. After receiving feedback from the WG members and from the public, a matured roadmap document will be prepared and sent to the Commission by April 2015.   },
address = {Eindhoven, NL},
annote = {Paper in Road2CPS. 

See Challenges below.Partial Autonomy:
"Partial autonomy of the subsystems both in terms of their independent ability to provide certain services and of partial autonomy of their control and management systems is essential in the definition of CPSoS. Often, the sub-systems can exhibit “selfish” behaviour with local management, goals, and preferences. The autonomy can in particular result from human users or supervisors taking or influencing the local decisions. The decision structures of the overall system can vary largely, from a (possibly multi-layered) hierarchy, where goals for the sub-systems are set but the sub-systems have degrees of freedom how to reach their goals, to a fully decentralized structure where only technical constraints and economic incentives provide the “glue” between the sub-systems.

Autonomy is understood in this context as the presence of local goals that cannot be fully controlled on the system of systems level. Rather, incentives or constraints are given to the subsystem control in order to make it contribute to the global system targets. An example is the operation of units of a chemical plant that consume and produce steam as a necessary resource or by-product of their main task. Their operators or managers run their processes autonomously to achieve local goals and meet local targets. The site owner/operator sets mechanisms to negotiate about the steam generation/consumption in order to balance the steam network and in addition should provide suitable incentives so that the global profit of the site is maximized. 

If a subsystem is controlled by humans, there always is a certain degree of autonomy. As humans act following reasons, preferences, and emotions which are external to the technical system and are not controllable, their actions are not fully predictable.

Besides the possible selfishness of autonomous sub-systems, autonomy also describes the ability of a sub-system to cope with certain tasks, disturbances, faults, on its own, without intervention from the system of systems level. The autonomous sub-systems can absorb variability and to the outside show a more predictable behaviour than what would result without their ability to regulate, react, and compensate disturbances. Ideally, a subsystem is given a task and fulfils the task (or “contract”) under any circumstance, and the higher level can ignore the details of the behaviour of the autonomously controlled subsystem. 

Emergence:
"Emerging behaviour should be distinguished from cascades of failures, like if a traffic jam on one motorway leads to one on the alternative route. However, if faults lead to instabilities and possible breakdowns of a large system due to “long-range interactions” in the system, like in power blackouts, then this can be called emerging behaviour. In technical systems, emerging behaviours usually are seen as problematic as a predictable behaviour of the system is preferred. On the other hand, in large systems with subsystems that show significant diversity in their behaviours, the formation of stable structures on a higher level due to the interactions between the subsystems despite their local diversity is very important and enables the design and management of the overall system without precise knowledge of all its elements. Emerging behaviour should be addressed both from the side of system analysis – under which conditions does emerging behaviour occur – and from the side of systems design – how can sufficient resiliency be built into the system such that local variations, faults, and problems can be absorbed by the system or be confined to the subsystem affected and its neighbours and do not trigger cascades or waves of problems in the overall system. Formal verification (e.g. assume/guarantee reasoning) as well as dynamic stability analysis for large-scale systems are possible approaches to prove the non-existence of unwanted emerging behaviours. 

Cyber-physical systems of systems cannot be designed and managed using theories and tools from only one single domain. The behaviour of the large coupled physical part of the system must be modelled, simulated and analysed using methods from continuous systems theory, e.g. large-scale simulation, stability analysis, and design of stabilizing control laws. Also methods and tools from computer science for the modelling of distributed discrete systems, for verification and testing, assume-guarantee methods, contract-based assertions etc. are indispensable to capture both the behaviour on the low level (discrete control logic, communication, effects of distributed computing) and global effects, in the latter case based on abstract models of complete subsystems. Logistic models as well as models and tools for performance analysis of discrete systems will be useful for system-wide performance analysis. Finally, theories from physics, e.g. structure formation in large systems, and from economics and social science (market mechanisms, evolution of beliefs and activity in large groups) may also prove to be useful."

CHALLENGE 1: Distributed, reliable and efficient management of cyber-physical systems of systems 
• Decision structures and system architectures [including dynamic membership]

• Self-organization, structure formation, and emergent behaviour in technical systems of systems ['Good' emergence, system stability, no normal accidents]

• Real-time monitoring, exception handling, fault detection and mitigation of faults and degradation. Due to the large scale and the complexity of systems of systems, the occurrence of failures is the norm in CPSoS. Hence there is a strong need for mechanisms for the detection of abnormal states and for fail-soft mechanisms and fault tolerance by suitable mechanisms at the systems level. Advanced monitoring of the state of the system and triggering of preventive maintenance based on its results can make a major contribution to the reduction of the number of unexpected faults and to the reduction of maintenance costs and downtimes. Faults may propagate over the different layers of the management and automation hierarchy. Many real-world SoS experience cascading effects of failures of components. These abnormal events must therefore be handled across the layers. 

• Adaptation and integration of new components Cyber-physical systems of systems are operated and continuously improved over long periods of time. New functionalities or improved performance have to be realized with only limited changes of many parts of the overall system. Components are modified and added, the scope of the system may be extended or its specifications may be changed. So engineering to a large extent has to be performed at runtime. Additions and modifications of system components are much facilitated by plug-and-play capabilities of components that are equipped with their own management and control systems (“decentralized intelligence”). 

• Humans in the loop and collaborative decision making HMI concepts, i.e. filtering and appropriate presentation of information to human users and operators are crucial for the acceptance of advanced computer-based solutions. Human interventions introduce an additional nonlinearity and uncertainty in the system. Important research issues are the human capacity of attention and how to provide motivation for sufficient attention and consistent decision making. It must be investigated how the capabilities of humans and machines in real-time monitoring and decision making can be combined optimally. Future research on the monitoring of the actions of the users and anticipating their behaviours and modelling their situation awareness is needed. Social phenomena (e.g. the dynamics of user groups) must also be taken into account. 

• Trust in large distributed systems. Cyber-security is a very important element in cyber-physical systems of systems. A specific CPSoS challenge is the recognition of obstructive injections of signals or takeovers of components in order to cause malfunctions, suboptimal performance, shutdowns or accidents, e.g. power outages. The detection of such attacks requires taking into account both the behaviour of the physical elements and the computerized monitoring, control and management systems. In the case of the detection of unsecure states, suitable isolation procedures and soft (partial) shut-down strategies must be designed. [They haven't got this well at all.]CHALLENGE 2: Engineering support for the design-operation continuum of cyber-physical systems of systems 
"• CPSoS are continuously evolving which softens, or even completely removes, the traditional separation between the engineering/design phases and the operational stages,
• The high degree of heterogeneity and partial autonomy of CPSoS requires new, fully integrated approaches for their design, validation, and operation,
• CPSoS are highly flexible and thus subject to frequent, dynamic reconfiguration, which must be supported by design support tools to enable efficient engineering,
• Failures, abnormal states, and unexpected/emerging behaviours are the norm in CPSoS, and
• CPSoS are socio-technical systems in which machines and humans interact closely.

The efficient design and operation of such systems requires new design support methodologies and software tools in the following areas: 

• Integrated engineering of CPSoS over their full life-cycle, The disappearance of the separation between the design and engineering phases and the operational stage necessitates new engineering frameworks that support the specification, adaptation, evolution, and maintenance of requirements, structural and behavioural models, and realizations not only during design, but over their complete life cycle. The challenges in rolling out systems of systems are the asynchronous lifecycles of the constituent parts and also the fact that many components are developed independently and that legacy systems may only be described insufficiently. ... CPSoS usually are not designed and maintained by a single company, but instead many providers of tools and hardware may be involved. Thus, collaborative engineering and runtime environments are essential that enable providers to jointly work on aspects of the CPSoS while competing on others. Integration must be based on open, easy-to-test interfaces and platforms that can be accessed by all component providers. Methods and software tools must provide semantic integration to simplify the interactions of existing systems as well as the deployment of new systems. 

• Modelling, simulation, and optimization of CPSoS, Challenges in modelling and simulation are the high cost for building and maintaining models, modelling of human users and operators, simulation and analysis of stochastic behaviour, and setting up models that include failure states and the reaction to abnormal situations for validation and verification purposes. Key for the adaptation of models during the life-cycle of a system and for reduced modelling cost are methodologies and software tools for model management and for the integration of models from different domains. Such model management requires meta-models. ... The model-based development of systems of systems necessitates collaborative environments for competing companies and the integration of legacy systems simulation as well as open approaches for tight and efficient integration and consolidation of data, models, engineering tools, and other information across different platforms. New business models may lead to a situation where for potential system components simulation models are delivered such that the overall system can be designed based on these models. 

• Establishing system-wide and key properties of CPSoS, Establishment, validation, and verification of key properties of CPSoS is an important challenge. New approaches are needed for dynamic requirements management during the continuous evolution of a cyber-physical system of systems, ensuring correctness by design during its evolution, and for verification especially on the system of systems level. New algorithms and tools should enable the automatic analysis of complete, large-scale, dynamically varying and evolving CPSoS. This includes formal languages and verification techniques for heterogeneous distributed hybrid systems including communication systems, theory for successive refinements and abstractions of continuous and discrete systems so that validation and verification at different levels of abstraction are correlated, and the joint use of assume-guarantee reasoning and simulation-based (Monte Carlo) and exhaustive (model checking) verification techniques. 

CHALLENGE 3: Cognitive cyber-physical systems of systems"Systems of Systems (SoS) by their very nature are large, distributed and extremely complex presenting a myriad of operational challenges. To cope with these challenges there is a need for improved situational awareness. Gaining an overview of the entire SoS is inherently complicated by the presence of decentralized management and control. The introduction of cognitive features to aid both operators and users of complex cyber-physical systems of systems is seen as a key requirement for the future to reduce the complexity management burden from increased interconnectivity and the data deluge presented by increasing levels of data acquisition. This requires research in a number of supporting areas to allow vertical integration from the sensor level to supporting algorithms for information extraction, decision support, automated and self-learning control, dynamic reconfiguration features and consideration of the sociotechnical interactions with operators and users. The following key subtopics have been identified as being necessary to support a move to Cognitive CPSoS. 

• Situation awareness in large distributed systems with decentralized management and control

• Handling large amounts of data in real time to monitor the system performance and to detect faults and degradation A challenge for the future will be the physical system integration of highly complex data acquisition systems and the management of the data deluge from the plethora of installed sensors and the fusion of this with other information sources. This will require analysis of large amounts of data in real time to monitor system performance and to detect faults or degradation. Here there is a need for visualization tools to manage the complexity of the data produced allowing managers to understand the “real world in real time”, manage risk and make informed decisions on how to control and optimize the system. 

• Learning good operation patterns from past examples, auto-reconfiguration and adaptation There is a great opportunity to aid system operators by incorporating learning capabilities within decision support tools to identify good operational patterns from past examples. Additionally, to deal with the complexity of managing system faults, which is a major burden for CPSoS operators, auto-reconfiguration and adaptation features can be built into the system. 

• Analysis of user behaviour and detection of needs and anomalies "Finally, it must be remembered that CPSoS are socio-technical systems and as such humans are an integral element of the system. Systems of systems thus need to be resilient to the effects of the natural unpredictable behaviour of humans. There is thus a need to continuously analyse user behaviour and its impact upon the system to ensure that this does not result in system disruption. 

The end result of combining real world, real-time information for decision support with autonomous control and learning features will be to provide Cognitive Cyber-Physical Systems of Systems that will support both users and operators, providing situational awareness and automated features to manage complexity that will allow them to meet the challenges of the future."},
author = {CPSoS-15},
keywords = {BPR,Cyber-physical systems,Governance,IoT,STS,affordances,agents,agility,allocation of functions,automation,business process re-engineering,complexity,control,corporate governance,decision-making,design process,ergonomics,errors,ethics,federated control,holons,job design,legal,maintenance,models,network-centric warfare,networks,org. trust,organisational design,quotes,reliability,resilience,robots,robustness,security,simulation,socio-tech,software,staffing,strategy,supply chain,sustainability,systems,training,trust},
publisher = {CPSoS Project, G.A. 611115},
title = {{Cyber-Physical Systems of Systems: research and innovation priorities }},
url = {http://www.cpsos.eu/wp-content/uploads/2015/02/CPSoS-Provisional-Roadmap-Paper-for-public-consultation{\_}web.pdf},
year = {2015}
}
@article{Mooney2012,
abstract = {INTRODUCTION Data citation should be a necessary corollary of data publication and reuse. Many researchers are reluctant to share their data, yet they are increasingly encouraged to do just that. Reward structures must be in place to encourage data publication, and citation is the appropriate tool for scholarly acknowledgment. Data citation also allows for the identification, retrieval, replication, and verification of data underlying published studies. METHODS This study examines author behavior and sources of instruction in disciplinary and cultural norms for writing style and citation via a content analysis of journal articles, author instructions, style manuals, and data publishers. Instances of data citation are benchmarked against a Data Citation Adequacy Index. RESULTS Roughly half of journals point toward a style manual that addresses data citation, but the majority of journal articles failed to include an adequate citation to data used in secondary analysis studies. DISCUSSION Full citation of data is not currently a normative behavior in scholarly writing. Multiplicity of data types and lack of awareness regarding existing standards contribute to the problem. CONCLUSION Citations for data must be promoted as an essential component of data publication, sharing, and reuse. Despite confounding factors, librarians and information professionals are well-positioned and should persist in advancing data citation as a normative practice across domains. Doing so promotes a value proposition for data sharing and secondary research broadly, thereby accelerating the pace of scientific research.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Mooney, Hailey and Newton, Mark},
doi = {10.7710/2162-3309.1035},
eprint = {0521865719 9780521865715},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mooney, Newton - 2012 - The Anatomy of a Data Citation Discovery, Reuse, and Credit.pdf:pdf},
isbn = {2162-3309},
issn = {21623309},
journal = {Journal of Librarianship and Scholarly Communication},
number = {1},
pages = {eP1035},
pmid = {7733545487724445170},
title = {{The Anatomy of a Data Citation: Discovery, Reuse, and Credit}},
url = {http://jlsc-pub.org/articles/10.7710/2162-3309.1035},
volume = {1},
year = {2012}
}
@article{Ofcom2015,
abstract = {Smartphones have overtaken laptops as the most popular device for getting online, Ofcom research has revealed, with record ownership and use transforming the way we communicate. Two thirds of people now own a smartphone, using it for nearly two hours every day to browse the internet, access social media, bank and shop online. Ofcom's 2015 Communications Market Report finds that a third (33{\%}) of internet users see their smartphone as the most important device for going online, compared to 30{\%} who are still sticking with their laptop. The rise in smartphone surfing marks a clear shift since 2014, when just 22{\%} turned to their phone first, and 40{\%} preferred their laptop. Smartphones have become the hub of our daily lives and are now in the pockets of two thirds (66{\%}) of UK adults, up from 39{\%} in 2012. The vast majority (90{\%}) of 16-24 year olds own one; but 55-64 year olds are also joining the smartphone revolution, with ownership in this age group more than doubling since 2012, from 19{\%} to 50{\%}. The surge is being driven by the increasing take-up of 4G mobile broadband, providing faster online access. During 2014, 4G subscriptions have leapt from 2.7 million to 23.6 million by the end of 2014. We now spend almost twice as long online with our smartphones than on laptops and personal computers. On average, adult mobile users spent nearly two hours online each day using a smartphone in March 2015 (1 hour and 54 minutes), compared to just over an hour on laptops and PCs (1 hour and nine minutes). But this is still only half of the 3 hours and 40 minutes we spend in front of the TV each day.},
author = {Ofcom},
journal = {05/08/2015},
keywords = {AHI 2017},
mendeley-tags = {AHI 2017},
number = {June 2015},
pages = {2--5},
title = {{The UK is now a smartphone society | Ofcom}},
url = {https://www.ofcom.org.uk/about-ofcom/latest/media/media-releases/2015/cmr-uk-2015 http://media.ofcom.org.uk/news/2015/cmr-uk-2015/},
volume = {2003},
year = {2015}
}
@techreport{Feiler2006,
abstract = {Executive SummaryThe Software Challenge of the Future The U. S. Department of Defense (DoD) has a goal of information dominance—to achieve and exploit superior collection, fusion, analysis, and use of information to meet mission objectives. This goal depends on increasingly complex systems characterized by thousands of platforms, sensors, decision nodes, weapons, and warfighters connected through heterogeneous wired and wireless networks. These systems will push far beyond the size of today's systems and systems of systems by every measure: number of lines of code; number of people employing the system for different purposes; amount of data stored, accessed, manipulated, and refined; number of connections and interdependencies among software components; and number of hardware elements. They will be ultra-largescale (ULS) systems.  The sheer scale of ULS systems will change everything. ULS systems will necessarily be decentralized in a variety of ways, developed and used by a wide variety of stakeholders with conflicting needs, evolving continuously, and constructed from heterogeneous parts. People will not just be users of a ULS system; they will be elements of the system. Software and hardware failures will be the norm rather than the exception. The acquisition of a ULS system will be simultaneous with its operation and will require new methods for control. These characteristics are beginning to emerge in today's DoD systems of systems; in ULS systems they will dominate. Consequently, ULS systems will place unprecedented demands on software acquisition, production, deployment, management, documentation, usage, and evolution practices.  Fundamental gaps in our current understanding of software and software development at the scale of ULS systems present profound impediments to the technically and economically effective achievement of the DoD goal of deterrence and dominance based on information superiority. These gaps are strategic, not tactical. They are unlikely to be addressed adequately by incremental research within established categories. Rather, we require a broad new conception of both the nature of such systems and new ideas for how to develop them. We will need to look at them differently, not just as systems or systems of systems, but as socio-technical ecosystems. We will face fundamental challenges in the design and evolution, orchestration and control, and monitoring and assessment of ULS systems. These challenges require breakthrough research.  We propose a ULS systems research agenda for an interdisciplinary portfolio of research in at least the following areas:  • Human Interaction: involves anthropologists, sociologists, and social scientists conducting detailed socio-technical analyses of user interactions in the field, with the goal of understanding how to construct and evolve such socio-technical systems effectively. • Computational Emergence: explores the use of methods and tools based on economics and game theory (e.g., mechanism design) to ensure globally optimal ULS system behavior and explores metaheuristics and digital evolution to augment the cognitive limits of human designers. • Design: broadens the traditional technology-centric definition of design to include people and organizations; social, cognitive, and economic considerations; and design structures such as design rules and government policies. • Computational Engineering: focuses on evolving the expressiveness of representations to accommodate the semantic diversity of many languages and focuses on providing automated support for computing the evolving behavior of components and their compositions. • Adaptive System Infrastructure: investigates integrated development environments and runtime platforms that will support the decentralized nature of ULS systems as well as technologies, methods, and theories that will enable ULS systems to be developed in their deployment environments. • Adaptable and Predictable System Quality: focuses on how to maintain quality in a ULS system in the face of continuous change, ongoing failures, and attacks and focuses on how to identify, predict, and control new indicators of system health (akin to the U. S. gross domestic product) that are needed because of the scale of ULS systems. • Policy, Acquisition, and Management: focuses on transforming acquisition policies and processes to accommodate the rapid and continuous evolution of ULS systems by treating suppliers and supply chains as intrinsic and essential components of a ULS system.  The proposed research does not supplant current, important software research but rather significantly expands its horizons. Moreover, because we are focused on systems of the future, we have purposely avoided couching our descriptions in terms of today's technology. The envisioned outcome of the proposed research is a spectrum of technologies and methods for developing these systems of the future, with national-security, economic, and societal benefits that extend far beyond ULS systems themselves.  Though our research agenda does not prescribe a single, definitive roadmap, we offer three structures that suggest ways to cluster and prioritize groups of research areas mapping the research areas and topics to (1) specific DoD missions and required capabilities, (2) DoD research funding types required to support them, and (3) estimates of the relative starting points of the research. These structures can then be used to define one or more roadmaps that could lead to one or more ULS systems research programs or projects. As a first step, we recommend the funding and establishment of a ULS System Research Startup Initiative, which over the course of the next two years would, among other things,  • work with others to conduct new basic research in key areas; • foster the growth of a community of informed stakeholders and researchers; and • formulate and issue an initial Broad Agency Announcement (BAA) to attract researchers with proven expertise in the diverse set of disciplines (e.g., software engineering, economics, human factors, cognitive psychology, sociology, systems engineering, and business policy) that are collectively required to meet the challenge of ULS systems.  The United States needs a program that will fund the software research required to sustain ongoing transformations in national defense and achieve the DoD goal of information dominance. The key challenge is the decision to move forward. The ULS System Research Agenda presented in this report provides the starting point for the path ahead." },
annote = {Document in T-AREA-SoS. Given their origin, it is natural that they should see the solution as software, not systems, though the do a recognisable job on understanding the systems aspects of the problem.


p.ix Quotation of fundamental research needs:
"Fundamental gaps in our current understanding of software and software development at the scale of ULS systems present profound impediments to the technically and economically effective achievement of the DoD goal of deterrence and dominance based on information superiority. These gaps are strategic, not tactical. They are unlikely to be addressed adequately by incremental research within established categories. Rather, we require a broad new conception of both the nature of such systems and new ideas for how to develop them. We will need to look at them differently, not just as systems or systems of systems, but as socio-technical ecosystems. We will face fundamental challenges in the design and evolution, orchestration and control, and monitoring and assessment of ULS systems. These challenges require breakthrough research."

p.x Research needs:
"We propose a ULS systems research agenda for an interdisciplinary portfolio of research in at least the following areas:

• Human Interaction: involves anthropologists, sociologists, and social scientists conducting detailed socio-technical analyses of user interactions in the field, with the goal of understanding how to construct and evolve such socio-technical systems effectively.
• Computational Emergence: explores the use of methods and tools based on economics and game theory (e.g., mechanism design) to ensure globally optimal ULS system behavior and explores metaheuristics and digital evolution to augment the cognitive limits of human designers.
• Design: broadens the traditional technology-centric definition of design to include people and organizations; social, cognitive, and economic considerations; and design structures such as design rules and government policies.
• Computational Engineering: focuses on evolving the expressiveness of representations to accommodate the semantic diversity of many languages and focuses on providing automated support for computing the evolving behavior of components and their compositions.
• Adaptive System Infrastructure: investigates integrated development environments and runtime platforms that will support the decentralized nature of ULS systems as well as technologies, methods, and theories that will enable ULS systems to be developed in their deployment environments.
• Adaptable and Predictable System Quality: focuses on how to maintain quality in a ULS system in the face of continuous change, ongoing failures, and attacks and focuses on how to identify, predict, and control new indicators of system health (akin to the U. S. gross domestic product) that are needed because of the scale of ULS systems.
• Policy, Acquisition, and Management: focuses on transforming acquisition policies and processes to accommodate the rapid and continuous evolution of ULS systems by treating suppliers and supply chains as intrinsic and essential components of a ULS system."

p.xi The plan:
"Though our research agenda does not prescribe a single, definitive roadmap, we offer three structures that suggest ways to cluster and prioritize groups of research areas mapping the research areas and topics to (1) specific DoD missions and required capabilities, (2) DoD research funding types required to support them, and (3) estimates of the relative starting points of the research. These structures can then be used to define one or more roadmaps that could lead to one or more ULS systems research programs or projects.

As a first step, we recommend the funding and establishment of a ULS System Research Startup Initiative, which over the course of the next two years would, among other things,

• work with others to conduct new basic research in key areas;
• foster the growth of a community of informed stakeholders and researchers; and
• formulate and issue an initial Broad Agency Announcement (BAA) to attract researchers with proven expertise in the diverse set of disciplines (e.g., software engineering, economics, human factors, cognitive psychology, sociology, systems engineering, and business policy) that are collectively required to meet the challenge of ULS systems.

The United States needs a program that will fund the software research required to sustain ongoing transformations in national defense and achieve the DoD goal of information dominance."

p.5, 1.6 Alan Kay famously said that the right perspective is worth 80 IQ points. For
40 years, we have embraced the traditional engineering perspective. The basic
premise underlying the research agenda presented in this document is that
beyond certain complexity thresholds, a traditional centralized engineering
perspective is no longer adequate nor can it be the primary means by which
ultra-complex systems are made real. Electrical and water systems are
engineered, but cities are not—although their forms are regulated by both
natural and imposed constraints. Firms are engineered, but the overall
structure of the economy is not—although it is regulated. Ecosystems exhibit
high degrees of complexity and organization, but not through engineering.
The protocols on which the Internet is based were engineered, but the Web as
a whole was not engineered—although its form is constrained by both natural
and artificial regulations. In this report, we take the position that the advances
needed for ULS systems require a change in perspective, from the satisfaction
of requirements through traditional, rational, top-down engineering to their
satisfaction by the regulation of complex, decentralized systems.

p.6, 1.6.1 City metaphor - GOOD!:


Useful quotes:
“This goal depends on increasingly complex systems characterized by thousands of platforms, sensors, decision nodes, weapons, and warfighters connected through heterogeneous wired and wireless networks. These systems will push far beyond the size of today's systems and systems of systems by every measure: number of lines of code; number of people employing the system for different purposes; amount of data stored, accessed, manipulated, and refined; number of connections and interdependencies among software components; and number of hardware elements. They will be ultra-largescale (ULS) systems.

The sheer scale of ULS systems will change everything. ULS systems will necessarily be decentralized in a variety of ways, developed and used by a wide variety of stakeholders with conflicting needs, evolving continuously, and constructed from heterogeneous parts. People will not just be users of a ULS system; they will be elements of the system. Software and hardware failures will be the norm rather than the exception. The acquisition of a ULS system will be simultaneous with its operation and will require new methods for control. These characteristics are beginning to emerge in today's DoD systems of systems; in ULS systems they will dominate. Consequently, ULS systems will place unprecedented demands on software acquisition, production, deployment, management, documentation, usage, and evolution practices.

Fundamental gaps in our current understanding of software and software development at the scale of ULS systems present profound impediments to the technically and economically effective achievement of the DoD goal of deterrence and dominance based on information superiority. These gaps are strategic, not tactical. They are unlikely to be addressed adequately by incremental research within established categories. Rather, we require a broad new conception of both the nature of such systems and new ideas for how to develop them. We will need to look at them differently, not just as systems or systems of systems, but as socio-technical ecosystems. We will face fundamental challenges in the design and evolution, orchestration and control, and monitoring and assessment of ULS systems. These challenges require breakthrough research.

GOOD ANALOGY:
"1.6 From Engineering to Complex Systems
Alan Kay famously said that the right perspective is worth 80 IQ points. For 40 years, we have embraced the traditional engineering perspective. The basic premise underlying the research agenda presented in this document is that beyond certain complexity thresholds, a traditional centralized engineering perspective is no longer adequate nor can it be the primary means by which ultra-complex systems are made real. Electrical and water systems are engineered, but cities are not—although their forms are regulated by both natural and imposed constraints. Firms are engineered, but the overall structure of the economy is not—although it is regulated. Ecosystems exhibit high degrees of complexity and organization, but not through engineering. The protocols on which the Internet is based were engineered, but the Web as a whole was not engineered—although its form is constrained by both natural and artificial regulations. In this report, we take the position that the advances needed for ULS systems require a change in perspective, from the satisfaction of requirements through traditional, rational, top-down engineering to their satisfaction by the regulation of complex, decentralized systems.

1.6.1 From Buildings to Cities
One way to understand the difference in scale between traditional and ULS systems is to think about buildings, infrastructure systems, and cities. Designing and building most of today's large systems can be compared to designing and constructing a single, large building or a single infrastructure system (such as for power or water distribution). In contrast, ULS systems will operate at levels of complexity more similar to cities. At first it might seem that designing and building a city is simply a matter of designing and building a large number of buildings. However, cities are not conceived or built by individual organizations, but rather by the actions of many individuals acting locally over time. The form of a city is not defined in advance by specifying requirements; rather, a city emerges and changes over time through the loosely coordinated and regulated actions of many individuals. The factors that enable cities to be successful, then, include both extensive infrastructures not present in individual buildings as well as mechanisms that regulate local actions to maintain coherence without central control. These mechanisms include government organizations and policies, city planning, streets and transportation systems, communication and emergency services, and distribution of food and consumer goods, to name a few. Moreover, it is not feasible to design and build a city in one attempt. People, companies, communities, and organizations decide to build parts of cities for their own purposes. Cities grow and thrive based on cultural and economic necessities, and, although some aspects of a city are designed and constructed in a local context, most elements that make up the essence of a city arise from more global policies and mechanisms, such as zoning laws, building codes, and economic incentives designed to encourage certain sorts of growth and construction.

Closer examination of the two design and construction problems—buildings versus cities—reveals that, although it is necessary, skill in designing and constructing buildings does not help much in designing and constructing a city. To point out just one of the major differences: every day in every city, construction is going on, repairs are taking place, modifications are being made, and yet the cities continue to function. Like cities, ULS systems will not simply be bigger systems: they will be interdependent webs of software intensive systems, people, policies, cultures, and economics.
{\ldots}
In a ULS system, there will be competition for resources, such as bandwidth, storage capacity, sensors, and weapons. The system will enforce rules intended to encourage effective use of these resources to achieve mission objectives. There may be variations in service depending on how different commanders, planners, and automated subsystems attempt to apply the available resources to missions with different levels of importance and urgency. With appropriate incentives and rules enforced by the system, these resources will be optimized so that they are appropriately available. In addition, there may be an overall measure of the quality of service being provided to different parts of the system or for different purposes (e.g., quality-of-service requirements will change for some parts of the system during a mission). This measure can be used to determine if the incentives are working as intended. As system behavior changes in response to the incentives, the incentives may also need to be changed to ensure that key mission goals are accomplished."
{\ldots}.
[ULSS Characteristics, developed from Maier's list:]
"The characteristics of ULS systems that will arise because of their scale are much more revealing. These characteristics are as follows:

• decentralization: The scale of ULS systems means that they will necessarily be decentralized in a variety of ways -decentralized data, development, evolution, and operational control.
• inherently conflicting, unknowable, and diverse requirements: ULS systems will be developed and used by a wide variety of stakeholders with unavoidably different, conflicting, complex, and changing needs.
• continuous evolution and deployment: There will be an increasing need to integrate new capabilities into a ULS system while it is operating. New and different capabilities will be deployed, and unused capabilities will
be dropped; the system will be evolving not in phases, but continuously.
• heterogeneous, inconsistent, and changing elements: A ULS system will not be constructed from uniform parts: there will be some misfits, especially as the system is extended and repaired.
• erosion of the people/system boundary: People will not just be users of a ULS system; they will be elements of the system, affecting its overall emergent behavior.
• normal failures: Software and hardware failures will be the norm rather than the exception.
• new paradigms for acquisition and policy: The acquisition of a ULS system will be simultaneous with the operation of the system and require new methods for control.},
author = {Feiler, P and Gabriel, R P and Goodenough, J and Linger, R and Longstaff, T and Northrop, L and Kazman, R and Klein, M and Schmidt, D and Sullivan, K and Wallnau, K and {Software Engineering Institute}, Carnegie Mellon University},
doi = {ISBN 0-9786956-0-7},
keywords = {agents,automation,business process re-engineering,complexity,consequences,dashboard/battlespace,decision-making,information systems,methods,multi-niche marketing,network-centric warfare,networks,quality,software},
publisher = {Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA 15213-3890, USA},
title = {{Ultra-Large-Scale Systems -- the software challenge of the future}},
url = {http://www.cs.bham.ac.uk/{~}wbl/ftp/biblio/gp-html/Feiler{\_}book.html},
year = {2006}
}
@incollection{Asaro2012,
address = {Cambridge, MA},
annote = {Book on shelf. See also Bryson

Note, this is a paper based on US law. Says (p.180): "Criminal law is concerned with punishing wrongdoers, whereas civil law is concerned with compelling wrongdoers to compensate those harmed. {\ldots} crimes deserve to be pinished, regardless of and cmpensation to those directly harmed by the crime. {\ldots} the harmed party in a crime is the whole of society. Thus, the case is prosecuted by thestate."

Argues that Semi-autonomous robots (what other class is there?) are covered by product liability precedents and laws. Mentions Agency Law as a particular area. Gives examples

1 Child ingest foam part, chokes to death. Manufacturer may be liable to civil damages. If it can be demonstrated that the manufacturer was negligent in making the product available, then criminal liability is involved - failures to warn, or failures to take proper care in assessing the potential risks posed by the product. Proving the latter may be difficult, because it's a grey area. Usual defence is that the manufacturer acted in accordance with the stated or unstated standards of the industry of which it is a part. Note that recourse to ISO standards (or equivalent) and to Codes of Practice can help here.

Note that there is a difference between ignorance, negligence, and recklessness, but these are nuances in the issue of 'culpable intent' in the criminality.

In US, they have classifications of liability, all predicated on monetary recompense. You can be 10{\%} liable, where others get the remaining 90{\%}. They separate causal responsibility from legal liability - you may be 1-0{\%} responsible but be caught by 'joint and several liability' where you ability to pay is assessed, so you might windup paying 50+{\%}, in this example. Or you could be under 'strict liability' wher you pay thelot.

Because of this approach to liability, need to pay attention to FMEA analyses, where other OEMs deliver components which become faulty.

For robots, especially those which can learn, or have application interfaces that allow 3rd-party software apps to be added, the OEM may not be liable - instead, the user/operator becomes liable. This, too, is a grey area; note the separation above between causal responsibility and legal liability. Discussion needed here about what 'learn' means; extending the database about the environment (situation awareness) is different to reprogramming the control algorithms. [but even SA has its problems; robot may be taught an unsafe procedure as part of its operation in a given context (e.g. "we keep glass bottles here." - unsafe; an accident waiting to happen). But this might be a user responsibility; last person to give order. Furthermore, there is the problem of equifinality; can we expect the user to anticipate all the equifinal alternatives, and cover them in the instructions to the robot? Conclusion: No. We have to ensure safe behaviour by the robot in design.]
p.176 Vicarious liability, agents, and diminished responsibility
Discusses analogy between domestic robot and domestic animal (apparently widely discussed inrobotic circles). Distinguishes between 'safe' animals such as hamsters and 'dangerous' pets such as lions, some dogs, etc.). Owner has different standards of responsibility in relationto these, especially where the pet can act onits own. Suggests that the more sophisticated robots,especially thos that can 'learn', are more equivalent to the latter class.

Note also that for more dangerous products (e.g. cars) there are extra laws in place re skills, awareness, safety practice, need for a licence, etc.

US slave laws might be relevant here as well - special clauses regading the slave as a special class of property (e.g. slaves held liable for the decision to escape - does not apply to animals, nor to other products)

Note Lokhorst {\&} vandenHoven:
Qui facit per alium, facit per se - he who acts through another does the act himself. The robot cannot be held responsible because the robot has no choice. Also:
Respondeat superior - let the master answer. An employer, in most cases, is responsible for the actions of employees perorming within the course of their employment.

?But these only apply where there is some sort of contract? This situation doesn't really deal with the co-worker scenario, where you don't have full authority over the robot, as in a team situation.p.180 Rights, personhood and diminished responsibility

Basically about how the law restricts what minors can do. Can't sign contracts, get passport, etc. That's the role of the parent or legal guardian. [latter role is an interesting idea.] 

Note the child has legal rights - limits to punishment, behaviour, etc. about which the courts can be involved [ASBOs/CRIMBOs, etc.]

Same with diminished responsibility.

In other words, we have here 'Quasi-persons'

p.180 Crime, punishment and personhood in corporations and robots
Can't apply criminal law to robots; (1) criminal actions require a moral agent to commit them; (2) it is not clear that it is possible to punish a robot. In criminal law, moral agency implies an autonomous subject who has a culpable mind (mens rea), and who can be punished. Absent this, and there can be harm, but not guilt. Also, how would a robot appreciate that it is being punished?

p.1812 Corporations:
"The corporation is a nonhuman entity that has been effectively granted many of the legal rights and responsibilities of a person. Corporations can (throught the actions of their agents) own property, sign contracts, and be held liable for criminal activties, such as fraud, criminal negligence,and causing environmental damage. As a seventeenth-century Lord Chancellor of England put it, corporations have 'no soul to damn and no body to kick' (Coffee 1981), so how can they be expected to have a oral conscience? Of course, corporations exist to make money, for themselves or for stockholders, and as such can be given monetary punishments, and in certain cases, such as anti-trust violations, split apart, or dissolved altogether. Though they cannot be imprisoned in criminal cases, responsible agents within the corporation can be prosecuted for their individual actions. As a result of this, and other aspects of corporations being complex socio-technical systems in which there are may stakeholderswith different relations to te monetary wealth of a corporation, it can be difficult to assign a punishment that achieves retribution, reform and deterrence, while meeting other requirements of justice, such as fairness and proportionality"

Robots do have a body, but the same problem of punishment or deterrence remains. Until this is solved, in his opinion there is no way a robot could be given personhood.},
author = {Asaro, P M},
booktitle = {Robot ethics},
chapter = {11},
editor = {Lin, P and Abney, K and Bekey, G A},
isbn = {978-0-262-01666-7},
keywords = {Governance,agents,definitions,ethics,legislation,network-centric warfare,philosophy,robots,socio-tech},
pages = {169--186},
publisher = {MIT Press},
title = {{A body to kick, but no soul to damn:  perspectives on robotics}},
year = {2012}
}
@techreport{STOA162016,
abstract = {Cyber-physical systems (CPS) are intelligent robotics systems, linked with the Internet of Things, or technical systems of networked computers, robots and artificial intelligence that interact with the physical world. The project 'Ethical aspects of CPS' aims to provide insights into the potential ethical concerns and related unintended impacts of the possible evolution of CPS technology by 2050. The overarching purpose is to support the European Parliament, the parliamentary bodies, and the individual Members in their anticipation of possible future concerns regarding developments in CPS, robotics and artificial intelligence. The Scientific Foresight study was conducted in three phases: 1. A 'technical horizon scan', in the form of briefing papers describing the technical trends and their possible societal, ethical, economic, environmental, political/legal and demographic impacts, and this in seven application domains. 2. The 'soft impact and scenario phase', which analysed soft impacts of CPS, on the basis of the technical horizon scan, for pointing out possible future public concerns via an envisioning exercise and using exploratory scenarios. 3. The 'legal backcasting' phase, which resulted in a briefing for the European Parliament identifying the legal instruments that may need to be modified or reviewed, including — where appropriate — areas identified for anticipatory parliamentary work, in accordance with the conclusions reached within the project. The outcome of the study is a policy briefing for MEPs describing legal instruments to anticipate impacts of future developments in the area of cyber-physical systems, such as intelligent robotics systems, linked with the Internet of Things. It is important to note that not all impacts of CPS are easily translated into legislation, as it is often contested whether they are in effect harmful, who is to be held accountable, and to what extent these impacts constitute a public rather than a private concern.},
address = {Brussels},
annote = {Paper in Autonomy!!. Mike Henshaw contributed.

Good source of quotes; need to re-read this to find them. 


IMPORTANT SOURCE OF LEGAL ISSUES

Has a long section generated by EC sherpas on legal issues that need to be addressed, sorted by societal domain. There are long bullet-pointed lists. Includes:
Legal instruments/provisions that might need to be reviewed/updated (indicative list)
• European Charter of Fundamental Rights and the UN Convention on Disability Rights; 
• Directive 2011/24/EU of the European Parliament and of the Council of 9 March 2011 on the application of patients' rights in cross-border healthcare, OJ L 88, 4.4.2011, p. 45–65; 
• Communication of the European Commission, Taking forward the Strategic Implementation Plan of the European Innovation Partnership on Active and Healthy Ageing, COM(2012) 83 final;
• Council Declaration on the European Year for Active Ageing and Solidarity between Generations (2012): The Way Forward, Guiding Principles for Active Ageing and Solidarity between Generations, Brussels, 7 December 2012.Conclusions from document:
The overarching purpose of this study is to support the European Parliament's bodies, as well as the individual Members, to strengthen their anticipatory knowledge and develop insights regarding the dynamics of change, future long-term challenges and options, in view of the rapid developments in the field of robotics. The main outcome of this foresight study is a policy briefing aiming at translating the identified the technical trends in the area of robotics as well as the respective impacts and concerns into legal and regulatory terms.

During the first phase of this study, a 'technical horizon scan' has been conducted in seven different domains of possible cyber-physical system (CPS) application, including short- and longer-term trends and their societal impacts. These domains are:

1. Disabled people and daily life
2. Healthcare
3. Agriculture and food supply
4. Manufacturing
5. Energy and critical infrastructure
6. Logistics and transport
7. Security and safety

In each case, the key technical developments, the short- and long-term trends and reflections upon the most important social, technological, environmental, economic, political, ethical and demographic impacts identified have been highlighted.

The development and implementation of CPS for the disabled and the elderly may lead to higher risks to data protection and privacy and a shift in the focus of medicine from treatment to prevention. This shift may help relieve the burden on medical professionals, allowing for more time to focus on patient care, and lowering the cost of medicine.
CPS may create major changes in the healthcare sector. These changes will trigger discussions on patient privacy including medical professional secrecy, data ownership and patient acceptance of CPS, and on civil liability for in case of medical shortcomings.

In the area of farming and food, CPS may result in greater food safety and hygiene. Through a combination of the Internet of Things, autonomous robots and sensors, we may experience an improvement in working conditions in the agricultural sector, and achieve optimised harvests and increased production. These changes will require discussions, for instance, on liability issues and the terms of the relationship between farmers and machines.

As CPS continue to be developed and deployed in manufacturing, with smart factories, we will see a radical change in the way manufacturing occurs primarily through new business models and the customisation of products. The changes will have profound implications for the economy, therefore legal action concerning data ownership, privacy, certification and safety will be needed.

CPS will become a key component of future energy systems, and the critical infrastructure underpinning the energy grid. However it is important to discuss the areas of liability, data collection and the ownership of that data to ensure that the rollout of the new energy systems can achieve the positive benefits while mitigating and potentially eliminating the negative side effects.

CPS are already changing the transport and logistics sectors. In the future, these changes will profoundly impact the way we move both goods and people with major impacts on safety, emissions, and the mobility of older citizens. A discussion towards accommodating the implantation of CPS will be needed concerning appropriate regulatory policy actions and timing, standardisation of laws, liability and privacy.

CPS will not generally make the world a more or less safe or secure place. It may however create a more complex world in which we will need to improve our ability to predict and understand the machines and their effect on security and safety. We need to ensure that those coming into contact with these new technologies – whether bystanders or operators – are able to understand the risks to their safety and security. These changes will require discussions on liability, data protection and the impact CPS will have on employment.

The next step of the study was based on and inspired by the outcomes of the technical horizon scan. Within this frame, a series of potential soft impacts7 of CPS were taken into consideration along with some publicly expressed concerns and fears. Two workshops were organised to identify these soft impacts, to develop a set of possible future scenarios, and to identify areas of possible public or ethical concern. A list of possible societal impacts and concerns related to a future in which CPS would be integrated in society was the main outcome of this phase.

Examples of these outcomes cover both rather obvious and less likely robotics-related futures. Some of the examples below illustrate how entrenched meanings may be challenged by technological developments in the domain of CPS, raising subsequent questions about how to act in view of the following challenges/developments:

 Co-living and co-working between humans and robots, possibility of intelligent robots as a result of artificial intelligence developments
 The incorporation of smart technologies, which might raise issues as to where we will need to consider the borderlines between assistive technologies and human enhancement: will human enhancement become wide-spread? Will we consider the concept of 'disability' differently tomorrow? In this area, we are also confronted with issues such as self-determination and physical integrity.
 Normative conceptions of 'nature' will be constantly challenged, to an even greater extent than today is the case. For instance, do we perceive precision farming, using CPS, as conflicting with the concepts of 'nature'? Opinions already differ if organic farming compatible with the use of agricultural robots.
 CPS in farming may allow farmers to work further from the land they are farming. The image of farmers might change drastically. Will CPS in farming make farming more or less attractive to young farmers?
 Currently, security as a value is most often applied to questions regarding physical health and safety, and to a lesser extent to employment and finances. With CPS and the Internet of Things, data streams become ubiquitous, making all domains of life possible candidates for security risks. As a possible consequence, it is foreseen that debates will occur about how much security it is reasonable to expect or claim in any given domain, and what is seen as (ir)responsible behaviour.
 Where the border is drawn between public and private is of the utmost relevance for policy- making. However, CPS technology is bound to bring these borders constantly into question, because it shifts responsibility from the collective to the individual and back again. For example: is human enhancement a private or public responsibility?

There are a number of examples where CPS may invite novel behaviour more directly, for instance:

 If technologies become easier to use, more people will use them. One example of this mechanism is drones. If you are the only one to have them, that gives you a competitive edge. However, other parties are bound to acquire the technology too, and then what seemed 'smart for one' may become 'dumb for all'.
 If things or services gets cheaper, more people buy and use them. If we make energy cheaper through the use of CPS, this may well lead to a net increase in energy use, with for example devastating effects for global warming. Or, more in the domain of soft impacts: might scarcity be a good thing? If energy is cheap and clean, do we still need to be conscientious about using it? Are there other positive practices that we have developed in the light of scarcity?

For this analysis of possible future impacts and concerns, the study made use of four exploratory scenarios in which the first identified possible futures were considered. These were developed after the envisioning meeting during which a working group with technical experts, social scientists and some stakeholders brainstormed on the possible future impact of CPS. They covered:

 CPS and health and disability
 CPS and farming
 CPS and manufacturing and security
 CPS and energy and security

Each of these scenarios is an imagined account of a future in which CPS has developed and matured in various aspects of our lives. Based upon these exploratory scenarios, the study identified future concerns regarding CPS that can be considered for anticipatory action by the European policymaker.

While the scenarios are speculative and ultimately fictional, it is important to note that they are systematically based upon concrete research conducted by top experts in the field, including technical trend analysis, horizon scanning and expert workshops. As such, every detail of the scenarios presented — from the habits, hopes, fears and values of the characters to the social, legal and ethical tensions evident in their lives — is based upon substantial research and analysis.

The scenarios do not aim to predict the future, but to highlight how technology development might affect society and the public and private lives of EU citizens. These imagined scenarios (published in Annex 2 to this report) are meant to provide an accessible means for the reader to understand the social, ethical and legal tensions that were identified in the research process. They were designed to support committees and individual MEPs in exploring, anticipating and responding to potential CPS development paths and their associated impacts, and to aid reflection on anticipatory policy and agenda setting at the European Parliament.

The final step of the foresight process, called the 'legal backcasting' phase, was performed entirely in- house and aimed at translating the findings of the foresight phase into legal terms so as to pave the way for possible parliamentary reflection and work. During this phase, the outcomes of the previous steps were taken into account and were legally translated into a forward looking instrument for the European Parliament, the parliamentary committees and the Members of the European Parliament.

The analysis consisted of the following phases:

1. Identification and analysis of areas of possible future concern regarding CPS that may trigger EU legal interest
2. Identification of those relevant EP committees and intergroups8 that may have a stake or interest in these areas
3. Identification of those legal instruments that may need to be reviewed, modified or further specified
4. Identification of possible horizontal issues of a legal nature (not committee-specific, wider questions to think about)

The legal backcasting covered the following areas:

 Transport
 Trade (dual-use / misuse)
 Data protection
 Safety (including risk assessment, etc.)
 Health (clinical trials/medical devices/E-health devices)
 Energy and environment
 Horizontal legal issues (cross-committee considerations).

The analysis looked at all stages of contact between robots, AI and humans. In this process, special emphasis was given to human safety, privacy, integrity, dignity, autonomy, data ownership and the need to provide a clear and predictable legal framework of an anticipatory nature. Special attention was given to the legal framework for data protection owing to the (expected massive) flow of data arising from the use of robotics and AI. Moreover, consumer concerns over safety and security concerning the use of robots and AI were discussed. The analysis shed light on legal concerns arising during the testing and development of robots including the risks associated with the terms of interaction with robots given their potential to profoundly impact physical and moral relations in our society.

Beyond the identification of the main areas of potential legal concern and the associated challenges as well as the respective pieces of EU legislation that may need to be reviewed or considered, the analysis leads to several, rather conceptual conclusions of a structural nature. Firstly, that every attempt to conceive and tackle the legal challenges associated with such a multifaceted technology needs to be designed in a reflective manner in order to help with making individual adjustments on a case-by-case basis. Moreover, special emphasis should be placed on the need for a clear definition of CPS and more specifically of smart autonomous robots for reasons of legal certainty at least at EU level. Such a definition should be subject to future modifications in the form of delegated acts.

Beyond the identified points of legal reflection, a risk analysis strategy should be devised in order to provide a plausible instrument of regulatory importance that will have a horizontal and technology- driven perspective.
Last but not least, the attempt to regulate emerging technology of this kind should be accompanied by ethical standards and procedures that will address the needs and ethical dilemmas of researchers, practitioners, users and designers alike. Such an ethical framework does not need to take a legally binding form but would be better established as an EU code of conduct. Finally, it should be emphasised that the fact is that not all the concerns identified in the previous steps can be 'translated' into legislative terms. Following such an extensive backcasting analysis and looking simultaneously at the rapid technological trends and developments at the same time, the regulatory and protective limits of law become rather evident.
In the light of some of these foresight scenarios, laws appears to be significantly inept at fulfilling their protective or even precautionary function. When carrying out these forward-thinking technological reflections, the fragility of traditional legal instruments and the limits of law and legal optimism become rather clear. At the same time, however, multidisciplinary exercises of this kind can facilitate the technological embodiment of law and help to shape a pluralist conception of law and technology.

To illustrate this, below we list some examples of concerns which, given the current EU legislative acquis and the particular status of EU competences but also the nature of some of these challenges, cannot fall within the scope of law in general or EU law in particular.

 The affordability of CPS services
 The possible digital divide between those using CPS and those not doing so
 The terms of interface between the authority of the doctor and the patient with AI-authority
 Avoiding data concentration
 The shortage of skills required for working with robots (e.g. as a person with a disability, as a user of an autonomous vehicle or as a farmer)
 Empathy with robots
 Control of super smart, quick, strong cyborgs
Outcomes of the study:

Unsurprising outcomes
Some concerns such as privacy/data protection and (un)employment frequently emerge when discussing data-intensive, ubiquitous technological systems like CPS that are meant to take over some of the tasks currently performed by human workers. These appeared repeatedly in the study.

Thought-provoking outcomes
Several thought-provoking ideas suggested how CPS development may destabilise the current meanings of the normative concepts that guide our practices, policies and laws. Some of these included:
 Human: when collaborating and living together with robots that are increasingly intelligent and, perhaps, more morally aware, the question of what makes humans truly human becomes unavoidable. Especially if artificial intelligence (AI) develops substantially, it is to be expected that the answers to questions of human identity will co-evolve with CPS technology.
 Body: as we start incorporating more and more smart technologies, the question of where the body ends will become increasingly important, and unanswerable in simple a priori terms. For example, is a prosthesis part of the body, or is it a device separate from the body? It will become increasingly impossible to define the body in a 'naturalistic' way. This question has deep ramifications, for example, concerning ideas about self-determination, physical integrity, and property rights.
 Dis/abled: connected to the previous issue, our conceptions of what it means to be able or disabled, will prove to be relative to our increasingly technological environment. Assuming a future of widespread human enhancement, what we consider to be able now may be considered dis-abled tomorrow.
 Nature: Underlying the previous instances of meaning-destabilisation, is the idea that normative conceptions of 'nature' will be constantly challenged, to an even greater extent than the case that is today. However, also in the domain of agriculture, prevailing conceptions of nature as organic and interdependent may, under the influence of 'precision farming', give way to more reductionist, molecular conceptions of 'nature'.
 Food: Examples of destabilised meaning are found in the current discussion about what constitutes 'organic' in relation to food. Currently, the concept only denotes that farming should be devoid of chemicals. But is organic farming compatible with the use of agricultural robots? Opinions differ already.
 Farming/Farmer: Currently, farmers are typically close (emotional) in distance to the land they are farming. However, CPS may enable long-distance farming, which would allow the farmer to live most of the time in a city environment. Also, farming will be increasingly high-tech. Due to these changes, popular images of farmers/farming are bound to change, which could make it more or less attractive for young people.
 Security: Currently, this value is most often applied to questions regarding physical health and safety, and to a lesser extent to employment and finances. With CPS and the Internet of Things, data streams become ubiquitous, making all domains of life a possible candidate for security risks. As a possible consequence, it is foreseen that debates will occur about how much security in which domain it is reasonable to expect or claim, and what is seen as (ir)responsible behaviour.
 Public-private: Where the border is drawn between these two domains is of utmost relevance to policy-making. However, CPS technology is bound to bring these borders constantly into question, because it shifts responsibility from the collective to the individual and back again. For example: is human enhancement a private or a public responsibility?

These examples illustrate how entrenched meanings may be challenged by technological developments in the domain of CPS, raising subsequent questions about how to act.
More novel outcomes},
author = {STOA'16},
doi = {10.2861/68949},
institution = {European Parliament},
isbn = {PE 563.501, ISBN 978-92-823-9428-1},
keywords = {Consent (informed):,Cyber-physical systems,agents,allocation of functions,automation,competitive challenges,complexity,culture,decision-making,design process,ergonomics,ethics,evaluation,federated control,interface design,legal,legislation,resilience,robots,smart city,socio-tech,trust},
publisher = {Scientific Foresight Unit (STOA), European Parliamentary Research Service, European Parliament.},
title = {{Ethical aspects of Cyber-Physical Systems}},
year = {2016}
}
@techreport{Olofsson2015,
abstract = {Many of us have encountered the discussion about which interface is better for working with Git, command-line or graphical. This thesis is an attempt to find out which user interface new Git users ...},
author = {Olofsson, Robin and Hultstrand, Sebastian},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Olofsson, Hultstrand - 2015 - Git - CLI or GUI Which is most widely used and why.pdf:pdf},
institution = {Blekinge Institute of Technology},
keywords = {Programvaruteknik,Software Engineering},
pages = {55},
title = {{Git - CLI or GUI Which is most widely used and why ?}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2{\%}3A852747{\&}dswid=-5566},
year = {2015}
}
@article{Bakka2018,
abstract = {Coming up with Bayesian models for spatial data is easy, but performing inference with them can be challenging. Writing fast inference code for a complex spatial model with realistically-sized datasets from scratch is time-consuming, and if changes are made to the model, there is little guarantee that the code performs well. The key advantages of R-INLA are the ease with which complex models can be created and modified, without the need to write complex code, and the speed at which inference can be done even for spatial problems with hundreds of thousands of observations. R-INLA handles latent Gaussian models, where fixed effects, structured and unstructured Gaussian random effects are combined linearly in a linear predictor, and the elements of the linear predictor are observed through one or more likelihoods. The structured random effects can be both standard areal model such as the Besag and the BYM models, and geostatistical models from a subset of the Mat$\backslash$'ern Gaussian random fields. In this review, we discuss the large success of spatial modelling with R-INLA and the types of spatial models that can be fitted, we give an overview of recent developments for areal models, and we give an overview of the stochastic partial differential equation (SPDE) approach and some of the ways it can be extended beyond the assumptions of isotropy and separability. In particular, we describe how slight changes to the SPDE approach leads to straight-forward approaches for non-stationary spatial models and non-separable space-time models.},
archivePrefix = {arXiv},
arxivId = {1802.06350},
author = {Bakka, Haakon and Rue, H{\aa}vard and Fuglstad, Geir-Arne and Riebler, Andrea and Bolin, David and Krainski, Elias and Simpson, Daniel and Lindgren, Finn},
eprint = {1802.06350},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bakka et al. - 2018 - Spatial modelling with R-INLA A review.pdf:pdf},
journal = {arXiv:1802.06350 [stat.ME]},
month = {feb},
pages = {33},
title = {{Spatial modelling with R-INLA: A review}},
url = {http://arxiv.org/abs/1802.06350},
year = {2018}
}
@misc{Berners-Lee2006,
author = {Berners-Lee, Tim},
booktitle = {World Wide Web Consortium},
pages = {1},
title = {{Linked Data - Design Issues}},
url = {https://www.w3.org/DesignIssues/LinkedData.html http://www.w3.org/DesignIssues/LinkedData.html},
urldate = {2018-07-10},
year = {2006}
}
@techreport{CERP-IoT2009,
address = {Brussels},
annote = {Document in Autonomy!! See also Petrissans. CERP = 'Cluster of European Research Projects'

Currenty seems to be seen as an exercise in software. Silly.

EU-funced exercise, bringing together a number of projects into a cluster: EU-funded projects in CERP-IoT: ASPIRE, BRIDGE, CASAGRAS , CASCADAS, CONFIDENCE, CuteLoop, ETP EPoSS, EU-IFM, EURIDICE, GRIFS, HYDRA, IMS2020, Indisputable Key, iSURF, LEAPFROG, PEARS Feasibility, PrimeLife, RACE networkRFID, SMART, StoLPaN, SToP, TraSer, WALTER CERP-IoT collaborate with FInES and FIA/RWI Stakeholders of closed projects AITPL, AMI-4-SME, CE-RFID, CoBIS, Dynamite, PRIME, PROMISE and SMMART stay active in the Cluster.},
author = {CERP-IoT},
keywords = {Governance,agents,agility,architecture,automation,complexity,federated control,networks,resilience,robots,robustness,safety,security,software,strategy,supply chain},
publisher = {European Commission - Information Society and Media DG},
title = {{Internet of Things - Strategic Research Roadmap}},
year = {2009}
}
@book{Pearl2018,
abstract = {'Correlation does not imply causation.' This mantra was invoked by scientists for decades in order to avoid taking positions as to whether one thing caused another, such as smoking and cancer and carbon dioxide and global warming. But today, that taboo is dead. The causal revolution, sparked by world-renowned computer scientist Judea Pearl and his colleagues, has cut through a century of confusion and placed cause and effect on a firm scientific basis. Now, Pearl and science journalist Dana Mackenzie explain causal thinking to general readers for the first time, showing how it allows us to explore the world that is and the worlds that could have been. It is the essence of human and artificial intelligence. And just as Pearl's discoveries have enabled machines to think better, The Book of Why explains how we can think better},
author = {Pearl, Judea and Mackenzie, Dana},
isbn = {978-0241242636},
pages = {432},
publisher = {Allen Lane},
title = {{The book of why : the new science of cause and effect}},
year = {2018}
}
@techreport{Madsen2012,
abstract = {The main idea behind this document is to demonstrate various either contributed or inspired chapter styles for the memoir class.},
author = {Madsen, Lars},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Madsen - 2012 - Various chapter styles for the memoir class ∗.pdf:pdf},
pages = {1--51},
title = {{Various chapter styles for the memoir class ∗}},
year = {2012}
}
@article{Avison2014,
abstract = {This paper suggests that there is too much emphasis on the requirement for theory use and theory building in qualitative research published in our leading journals. We discuss six concerns that we have that relate to this high status of theory in such papers. We argue for what we refer to as 'theory light' papers where theory plays no significant part in the paper and the contribution lies elsewhere, for example, new arguments, facts, patterns or relationships. Some examples of theory light papers (and research) are provided from other disciplines and one exemplar information systems paper is studied in depth. We see these papers as equally worthy as those which demonstrate the applicability and predictive qualities of theory use as well as the potential of theory building. We propose a list of 10 questions that authors and reviewers might ask themselves when writing or reviewing such theory light papers. The more demanding role of the reader is also discussed along with the requirement for editorial teams to adapt. We suggest that the requirement for a contribution to theory would be replaced with the requirement that any journal paper has a high potential for stimulating research that will impact on information systems theory and/or practice.},
author = {Avison, David and Malaurent, Julien},
doi = {10.1057/jit.2014.8},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Avison, Malaurent - 2014 - Is theory king Questioning the theory fetish in information systems.pdf:pdf},
issn = {14664437},
journal = {Journal of Information Technology},
keywords = {author,editor,journals,qualitative research,reader,theory,theory light},
month = {dec},
number = {4},
pages = {327--336},
publisher = {Palgrave Macmillan Ltd.},
title = {{Is theory king?: Questioning the theory fetish in information systems}},
volume = {29},
year = {2014}
}
@article{Schneble2018,
abstract = {In recent years, the Internet has become an essential source of data for research. A vast array of information can be collected via platforms, such as Amazon Mechanical Turk [1] and Survey Tools for specific research questions, or from harvesting social networks such as Twitter or Facebook [2]. Questions about data protection, consent and confidentiality will therefore become increasingly important [3], not only for users, but also for researchers and providers of such research and social media services. The European General Data Protection Regulation (GDPR) [4], with its paradigm of security and privacy by default, is a step in the right direction. The recent scandal surrounding Facebook and Cambridge Analytica [5] shows that these aspects of security and privacy are often not taken into account. Cambridge Analytica, a British consulting firm, was able to collect data from as many as 87 million Facebook users without their consent. The company gained access to 320,000 user profiles and their friends' data through the “thisisyourdigitallife” app developed by psychologist Alexandr Kogan of Cambridge University, UK, when he sold it to the company.},
author = {Schneble, Christophe Olivier and Elger, Bernice Simone and Shaw, David},
doi = {10.15252/embr.201846579},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Schneble, Elger, Shaw - 2018 - The Cambridge Analytica affair and Internet‐mediated research.pdf:pdf},
issn = {1469-221X},
journal = {EMBO reports},
pages = {e46579},
title = {{The Cambridge Analytica affair and Internet‐mediated research}},
url = {https://onlinelibrary.wiley.com/doi/pdf/10.15252/embr.201846579 http://embor.embopress.org/lookup/doi/10.15252/embr.201846579},
year = {2018}
}
@techreport{Schwarber2016,
abstract = {The SCRUM guide, on rapid program development},
author = {Schwarber},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Schwarber - 2016 - The Scrum Guide.pdf:pdf},
pages = {17},
title = {{The Scrum Guide}},
year = {2016}
}
@article{Hampton2013,
abstract = {The need for sound ecological science has escalated alongside the rise of the information age and “big data” across all sectors of society. Big data generally refer to massive volumes of data not readily handled by the usual data tools and practices and present unprecedented opportunities for advancing science and inform- ing resource management through data-intensive approaches. The era of big data need not be propelled only by “big science” – the term used to describe large-scale efforts that have had mixed success in the individual-driven culture of ecology. Collectively, ecologists already have big data to bolster the scientific effort – a large volume of distributed, high-value information – but many simply fail to contribute. We encourage ecologists to join the larger scientific community in global initiatives to address major scientific and societal problems by bringing their distributed data to the table and harnessing its collective power. The scientists who contribute such information will be at the forefront of socially relevant science – but will they be ecologists?},
author = {Hampton, Stephanie E and Strasser, Carly A and Tewksbury, Joshua J and Gram, Wendy K and Budden, Amber E and Batcheller, Archer L and Duke, Clifford S and Porter, John H},
doi = {10.1890/120103},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hampton et al. - 2013 - Big data and the future of ecology.pdf:pdf;:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hampton et al. - 2013 - Big data and the future of ecology(2).pdf:pdf},
isbn = {1540-9295},
issn = {15409295},
journal = {Frontiers in Ecology and the Environment},
month = {apr},
number = {3},
pages = {156--162},
publisher = {Wiley-Blackwell},
title = {{Big data and the future of ecology}},
url = {http://doi.wiley.com/10.1890/120103},
volume = {11},
year = {2013}
}
@book{Xie2015,
abstract = {Second edition. "Suitable for both beginners and advanced users, Dynamic Documents with R and knitr, Second Edition makes writing statistical reports easier by integrating computing directly with reporting. Reports range from homework, projects, exams, books, blogs, and web pages to virtually any documents related to statistical graphics, computing, and data analysis. The book covers basic applications for beginners while guiding power users in understanding the extensibility of the knitr package, "--Amazon.com. Preface -- Author -- List of Figures -- List of Tables -- Introduction -- Reproducible Research -- A First Look -- Editors -- Document Formats -- Text Output -- Graphics -- Cache -- Cross Reference -- Hooks -- Language Engines -- Tricks and Solutions -- Publishing Reports -- R Markdown -- Applications -- Other Tools -- Appendix -- Internals -- Bibliography -- Index.},
address = {London, UK},
author = {Xie, Yihui},
edition = {2nd},
isbn = {9781498716963},
pages = {266},
publisher = {Chapman and Hall/CRC},
title = {{Dynamic documents with R and knitr}},
year = {2015}
}
@inproceedings{teh2000embeddingtechnique,
author = {Teh, N J and Conway, P P and Palmer, P J and Kioul, A and Prosser, S},
booktitle = {Proceedings from the 26th IEMT Symposium},
isbn = {0-7803-6482-1},
month = {oct},
organization = {Santa Clara, CA, USA},
pages = {10--18},
title = {{Embedding of Electronics within Thermoplastic Polymers using Injection Moulding Technique}},
year = {2000}
}
@techreport{Acatech112011,
abstract = {SUMMARY Embedded hardware and software systems are decisive driving forces for innovation in the export and growth markets of German industry. They crucially expand the functionality and, as a result, the practical value and competitiveness of vehicles, aircraft, medical equipment, production plants and household appliances. Today, about 98 percent of micro processors are embedded, connected with the outside world through sensors and actuators. They are increasingly connected with one another and the internet. The physical world and the virtual world – or cyberspace – are merging. Cyber-physical systems (CPS) are being developed that are part of a globally networked future world, in which products, equipment and objects interact with embedded hard-ware and software beyond the limits of single applications. With the help of sensors, these systems process data from the physical world and make it available for network-based services, which in turn can have a direct effect on processes in the physical world using actuators. Through cyber-physical systems, the physical world is linked with the virtual world to form an Internet of Things, Data and Services. Some examples of early cyber-physical systems – such as networked navigation software – already exist today. With the help of mobile communication data, the software de-duces information on traffic jams from current movement profiles for improved route guidance. Further examples in-clude assistance and traffic control systems from the fields of avionics and railway transportation. In these cases, the systems actively exercise control. Future cyber-physical systems will contribute to safety, efficiency, comfort and human health like never before. They will therefore contribute to solving key challenges of our society, such as the aging population, limited resources, mobility, or the shift towards renewable energies, to name but a few fundamental fields of application. As part of a smart grid, cyber-physical systems will control the future energy network consisting of a multitude of renewable en-ergy producers. In the future, they will make transport safer through coordination, and they will reduce CO2 emissions. Modern smart-health systems will network patients and doc-tors, facilitate remote diagnoses, and provide medical care at home. Internet-based systems for remote monitoring of autonomous production systems are being developed for manufacturing, logistics and transportation. One of the next steps is self-organization. Machines will autonomously control their maintenance and repair strategy depending on the degree of workload, and ensure backup capacities to maintain production in the case of maintenance-related interruptions.Cyber-physical systems have a highly disruptive effect on market structures. They will fundamentally change business models and the competitive field of play. New suppliers of services based on cyber-physical systems are penetrating the markets. Revolutionary applications will facilitate new value chains, transforming the classic industries such as the automobile industry, the energy sector and production engineering. Cyber-physical systems will pose new challenges to science and research. How should heterogeneously networked structures be handled that require an integral systemic view and interdisciplinary cooperation between mechanical engineering, electrical engineering and computer science? How can cyber-physical systems be mastered technically? And how should they be built, operated, monitored and maintained? In terms of embedded systems, Germany is a world leader and also occupies a leading position in the market for secu-rity solutions and corporate software. In addition, Germany traditionally has a high level of engineering competence in the development of complex system solutions and has extensive research knowledge in semantic technologies and embedded systems. Despite this favourable starting position, Germany must also consider its weaknesses with regard to the development of cyber-physical systems. Germany has to do some catching up in internet competence – in research and applications, development platforms and operator models as well as with innovative solutions for user-centred human-machine interfaces. On the other hand, the US National Science Foundation has been promoting the subject of cyber-physical systems since 2006 with numerous projects and programmes. [see NSF 2011] If Germany wants to secure a position as world leader in the use of innovative cyber-physical systems, rapid action by politicians together with stakeholders from science, the economy and society is required due to the tight time frame. The objective should be to master technology and its eco-nomic use and to focus on the social acceptance of cyber-physical systems. Taking into account the National Road-map Embedded Systems (NRMES) 20092 for the further development of embedded systems, in order to overcome the technical, economic, social and political challenges con-nected with cyber-physical systems, acatech recommends: 1 As technical prerequisites for cyber-physical systems, mobile internet access and access to the physical infra-structure need to be promoted. This includes engineer-ing of sensors and actuators, algorithms for the adap-tive behaviour of networked systems and ontologies to interlink such autonomous systems. Development and operator platforms should be set up and expanded. 2 Interoperability standards need to be developed, and standardization activities need to be supported on in-ternational committees. 3 Human-machine interaction needs to be further de-veloped in the fields of research, training and practical implementation. Human factors, such as the tailored logic of workflow, situational adequacy, usability of equipment and ergonomics issues, need to be explored integrally. 4 The existing legal situation with regard to the security and safety of cyber-physical systems needs to be adapted, especially in terms of privacy protection. A working group consisting of academics, lawyers and politicians is to be created to develop a concept for handling per-sonal data in cyber-physical systems. 5 A dialogue about the benefits of social innovations created by cyber-physical systems needs to be initiated. It is necessary to involve the general public in the development of cyber-physical systems and to inform them on security and safety issues. 6 Specific platforms need to be established to explore new business models for cyber-physical systems. These business models need to be analyzed as part of a secondary research project. 7 Platforms and joint research projects specifically involving SMEs have to be created for the promotion of cyber-physical systems. SMEs should get simplified ac-cess to research projects. Spin-offs, particularly from uni-versities, should be promoted. 8 A central national research and competence centre for the Internet of Things, Data and Services and the World Wide Web, which deals with all the issues in the field of global networks, has to be set up. 9 Existing studies and training courses (computer science, engineering, business management) need to be adapted to the requirements of cyber-physical systems. New interdisciplinary courses about cyber-physical systems need to be created. 10 German science should dedicate itself particularly to interdisciplinary projects on cyber-physical systems. Integrated and interdisciplinary fields of research on cyber-physical systems should be promoted specifically in innovation alliances made up of industry and research participants. 11 The establishment of relevant CPS showcases for pilot applications and other efficient forms of mediation (such as Living Labs) can contribute to raising aware-ness of the subject early on, within the relevant export groups (particularly in SMEs) as well as the general public. The acceptance of these new technologies by society is decisive for the success of cyber-physical systems. Cyber-physical systems elevate the requirements of privacy and information security to a new level. In the future, immense volumes of highly important data will flow through the networks. The confidence of the general public in this new technology also depends on the security and the transpar-ency of such flows of data. Cyber-physical systems have major significance for a multitude of key issues in the future. For this reason, it is essential that the German government takes cyber-physical sys-tems into account in its strategies for energy and resources, as well as in its high-tech and ICT strategy. And ultimately, the subject of transition to renewable energies also has to become part of an overall cyber-physical systems strategy.},
address = {Berlin},
annote = {Document in Road2CPS. Good quality document; likely to be useful for the SoA, and the Sustainability paper.

WRT the latter, see comments below and last section, which is effectively a gap analysis.. 

Mainly quotes in the body below; Abstract gives most of the important messages.

Note: "Bottom-up economy", "open production", "the machine follows the human workcycle"

Ch. 3 predicts the future for Germany up to 2025 for energy, mobility, health, industry. Nothing spectacular.

Ch. 4 likewise lists challenges.

p.15 DEFINITION OF CPS:
"Cyber-physical systems are systems with embedded soft-ware (as part of devices, buildings, means of transport, transport routes, production systems, medical processes, logistic processes, coordination processes and management processes), which:
— directly record physical data using sensors and affect physical processes using actuators;
— evaluate and save recorded data, and actively or re-actively interact both with the physical and digital world;
— are connected with one another and in global networks via digital communication facilities (wireless and/or wired, local and/or global);
— use globally available data and services;
— have a series of dedicated, multimodal human-machine interfaces."

"Medical engineering is one of the greatest fields of growth in the world. Investment in research and develop-ment in the industry makes up about eight percent of the turnover – about twice the industrial average.7 It is esti-mated that the turnover in medical engineering in Germany will increase by about eight percent per year up to 2020. Besides telemedical patient monitoring, equipment net-working and the expansion of the functionality of existing devices, cyber-physical systems offer a multitude of oppor-tunities, for example to optimize emergency deployments and increase efficiency in hospitals."

"The energy system needs to adapt to the volatile availability of electricity from renewable sources and the decentralization of energy pro-duction. Cyber-physical systems here play a decisive role as a fundamental component of intelligent power networks, or so-called smart grids: network management, consumption optimization and production planning can only be imple-mented through networked systems."

"Agriculture, which is already optimizing processes with the help of information technology, is another field for the use of cyber-physical systems. Comprehensive intelligent systems link GPS position location, monitoring technol-ogy and sensor networks to determine the current state of agricultural land and support agricultural providers in the optimized fertilization of fields. As a result, the efficiency of agricultural processes is increased and soil can be used with an increased focus on ecologic responsibility."


Driving forces for development of CPS
Three trends:
1 Smart embedded systems, mobile services, and "ubiquitous" computing
2 Internet-based business processes:
(a) intelligent networked objects; (b) cloud services
3 Social networks and conmmunities:
(c) people as customers, people as social animals living and interacting together; (d) groups of developers or specialists working together for a purpose.

Development of CPS:
"For all sectors of system design and control, cyber-physical systems require interdisciplinary, cooperative work in net-works and clusters which are dedicated to innovation. This concerns:
— development, production and exploitation;
— operation and maintenance;
— services, consulting, adjustment and extension;
— medium-term and long-term projects relating to strategy 
development and evolution; and
— comprehensive engineering of systems by corporate clusters sharing strategy and platform cooperation as part of a corporate network, i.e. an economic ecosystem."
- recognition and incorporation of societal values, cultures, behaviours, both for groups and for individuals."The decisive factor for networking cyber-physical systems beyond application boundaries is that information from different applications must be semantically compatible. This “semantic interoperability” ultimately enables the interplay of applications."

Ch.4 Challenges (selected):
"Ultimately, Cyber-physical systems can only be developed efficiently with the help of new models and design methods for networked technical systems (multi-level systems). It is typical for such systems that it is not the optimiza-tion of these systems which plays a fundamental role, but the overcoming of their complexity and the inclusion of new functionalities such as the adaptivity of the systems, learning of functions, self-organization and more. To put it boldly, the different branches of science need to be net-worked with one another in the same way the technical systems are networked through cyber-physical systems. For example, the networking of antilock braking systems (ABS) and supported steering systems (Electric Power Steering/EPS) is impossible without the interdisciplinary linking of methods of mechanical engineering, communi-cation technology and computer science.

As a key challenge, computer scientists need to find a way for applications with precise real time requirements to work via communication networks whose behaviour is only randomly representable, i.e. under the assumption of probabilities.

Relevant IT skills – as an essential part of professional qualifications – become the key to being able to develop cyber-physical systems in Germany and export them from here. This requires new ways of thinking in terms of open-ing up and creating closer links particularly between both engineering and computer science and other disciplines, for example business management or cognitive sciences. It is also important to ascribe greater value to interdisciplinary projects in terms of scientific reputation. 

At the moment, our education and training systems at schools, colleges and universities, as well as our develop-ment processes and methods are only suitable for manag-ing cyber-physical systems to a limited degree.

4.5 Social Challenges:
"Social willingness to accept this new technology, use it and further develop it is decisive for the success of cyber-physi-cal systems. Acceptance by users is a crucial prerequisite for the use of cyber-physical systems. Acceptance means that users perceive technologically designed systems posi-tively, accept them and are willing to use them. The past has shown that it is extraordinarily difficult to predict ac-ceptance. At the same time, acceptance is very closely dependent on well-designed human-machine interaction. For this reason, issues of acceptance need to be addressed extensively from the very beginning during the design of cyber-physical systems. In this context, privacy, the determi-nation of boundaries for systems and socially desired and legitimated restrictions of the functionality of cyber-physi-cal systems are of central importance. [See the article “Gesellschaftliche Relevanz Intelligenter Objekte” [Social relevance of intelligent objects] in Herzog/Schildhauer 2009.]

Against this background, it appears to be essential to initi-ate a more robust social discourse, will deal with a series of fundamental issues relating to cyber-physical systems. Examples of such questions include the forms of depend-ency of people on autonomously deciding systems, legal consequences, values and value systems of people with regard to cyber-physical systems, the question of how inter-personal communication develops under the influence of cyber- physical systems and to what extend it is sensible and responsible to set up large sections of critical infrastructure based on cyber-physical systems. The issue of what mea-sures are needed to limit risk also has to be considered."

Theses (selected):
3 Cyber-physical systems are part of socio-technical systems: As cyber-physical systems intervene in work and everyday life in an as yet unprecedented degree in many fields of application, for example in the health sector, public acceptance and acceptance by users is es-sential for the successful introduction of cyber-physical systems. For this reason, the development of ethically sustainable and legally permitted solutions is a key is-sue for the scientific and technical communities.

(The field of human-machine interaction needs to be further developed in terms of research, training and practical imple-mentation in order to achieve sustainable acceptance. The same applies to so-called “human factors”, from the mental models of the user, and the appeal and usability of cyber-physical systems to the user-specific ability to understand information, solutions and their implications. Besides usability, safety, security and reliability are further prerequisites for the acceptance of cyber-physical systems.)

4 New business models as a result of cyber-physical systems: As cyber-physical systems work collaboratively and interactively, those companies which specialise in roles conforming to their relevant core competences in corporate networks will be particularly successful, and will develop these roles in such a way that they are aligned to the infrastructure of the overall solution designed for cyber-physical systems.

(The economic environment should be taken into account when considering secondary research projects for all key projects relating to cyber-physical systems. The focal points are “business models for new products and product-service systems”, “services of cyber-physical systems” and “corpo-rate software for cyber-physical systems”.)

6 Significance of human-machine interaction: Technol-ogy and applications for cyber-physical systems need to consider user requirements and ensure simple, intuitive operability. The principles for user-friendly and accept-able solutions can be created as early as the technical development process of cyber-physical systems.

(Human-machine interaction is also of central importance from an economic point of view. In particular, the specific German phenomenon of “overengineering” – the creation of a product or service of a higher quality or at greater ex-pense than actually necessary – can be a crucial factor in the development of cyber-physical systems.

Human factors in connection with cyber-physical systems need to be comprehensively researched, from classic is-sues of ergonomics traceability, the integration of adaptive and adaptable cyber-physical systems into work processes and its effects, up to the issue of potential adjustments in social behaviour under the influence of the use of cyber-physical systems.

For cyber-physical systems, consistent customer focus and, thus, user-friendliness and intuitive usability are the key to success.

Scientific foundation:
The modelling of cyber-physical systems requires the inter-play of various disciplines – physics, mechanical engineer-ing, electrical engineering and computer science. However, the principles of cognitive psychology and sociology are also essential; their relevance ranges from models of perception, interaction, knowledge, thought processes and problem solving to system and network models in tech-nological sociology. The focus is on the development of a new discipline concerning the engineering of cyber-physical systems with an integrated perception of the modelling of relevant systems. Models from computer science, electrical engineering and mechanical engineering are merged into an integrated modelling approach on the basis of existing physical models and drawing heavily on control theory. In detail, this means:

acatech gap analysis (appendix - see also Table 7):
1 Gradual set-up of reference architectures, domain models and application platforms as a prerequisite for cor-rect situation and context perception, interpretation, process integration and reliable handling/management of the systems. This includes:
— models of the physical environment, its architecture, participants, tasks, roles and (interaction) relationships, etc.;
— requirement models (functional and non-functional) of direct or indirect participants (stakeholders, sys-tems, components);
— application/reference architectures: process models, function/service architectures and interaction tem-plates, as well as realization architectures (logical architectures, for example, to realize specific secu-rity or performance requirements; hardware and software architectures, or even specific platform and communication architectures), organizational conditions and standards, etc.;
— quality models as well as models for domain or business rules, target models or company-specific models to inspect and validate CPS services and applications.
— Specific norms and standards for the qualified development and certification of systems.

Besides the differences in the dynamics and culture of the involved application fields, sys-tems, participants and disciplines, the challenges are:
— the increasing loss of control in open (social) environments with networked and sometimes autonomously interacting systems and participants, and the questions, methods and safeguarding concepts connected with this;
— the reliability of the systems with regard to safety, IT security and privacy as well as other non-functional requirements, for example performance and energy efficiency;
— the protection of (business-) knowledge in open value networks (CPS eco systems);
— the uncertain and distributed risks accompanying cyber-physical systems as well as their assessment and evalu-ation by individual systems and participants. Risk as-sessment is virtually impossible in terms of quantity and only subjectively possible in terms of quality;
— cyber-physical systems acting as representatives (agents) of social and economy participants (humans, groups) and being required to conduct appropriate and fair negotiation and the resolution of any arising conflicts of objectives;
— regulations for the (partially) autonomous actions and decision-making on the part of the systems;
— the required conditions [For example, the necessary CPS infrastructure, its safety and quality, standardisation, standards to be complied with and legal conditions etc.] and the domain/quality models, rules and policies (compliance regulations) to be negotiated in a legally binding manner.},
author = {Acatech},
booktitle = {acatech Position Paper},
keywords = {BPR,Cyber-physical systems,IoT,agents,allocation of functions,architecture,business process re-engineering,circular economy,competitive challenges,control,dashboard/battlespace,decision-making,definitions,federated control,interface design,knowledge,legal,models,network-centric warfare,networks,outsourcing,quality,reference architectures,situation awareness,standards,strategic planning,sustainability},
publisher = {acatech},
title = {{Cyber-Physical Systems: driving force for innovaton in mobility, health, energy, and production}},
url = {http://www.acatech.de/fileadmin/user{\_}upload/Baumstruktur{\_}nach{\_}Website/Acatech/root/de/Publikationen/Stellungnahmen/acatech{\_}POSITION{\_}CPS{\_}Englisch{\_}WEB.pdf},
year = {2011}
}
@inproceedings{HoppnerNieder-RamstadterStr2005,
abstract = {This talk presents strategies for including graphics into L A T E X documents. It shows the usage of the standard graphics packages of L A T E X as well as an introduction to different graphics formats. Some external tools for converting graphics formats are discussed.},
address = {Chapel Hill, NC, USA},
author = {{H{\"{o}}ppner Nieder-Ramst{\"{a}}dter Str}, Klaus},
booktitle = {Proceedings of the Practical TEX Conference},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/H{\"{o}}ppner Nieder-Ramst{\"{a}}dter Str - 2005 - Strategies for including graphics in LATEX documents.pdf:pdf},
pages = {59--62},
publisher = {TEX User Group},
title = {{Strategies for including graphics in LATEX documents}},
url = {https://tug.org/TUGboat/tb26-1/hoeppner.pdf},
year = {2005}
}
@article{Andor1985,
abstract = {The Galois-lattice is a graphic method of representing knowledge structures. The nodes of G-lattices represent all the possible concepts in a given body of knowledge in the sense that a notion defines a set of individuals or properties with no exceptions or idiosyncrasies. A G-lattice provides a tool to represent all the possible developmental phases to reach a given total knowledge via different partial knowledge structures. G-lattices are pure algebraic structures and may contain contingent features caused by irregular and random data. In this chapter some algorithms are proposed to develop the G-lattice method so that statistical analyses can be incorporated. These algorithms are based on the possible omissions of individuals or properties from the data so that the resulting G-lattice be reduced as much as possible. The greatly simplified structure represented by the resulting G-lattice is still valid for a large percentage of individuals or properties. This is an alternative approach to statistical significance as this approach refers to strict logical relations. A comparison with the Rasch model is given. {\textcopyright} 1985.},
author = {Andor, Csaba and Jo{\'{o}}, Andr{\'{a}}s and M{\'{e}}r{\"{o}}, L{\'{a}}szl{\'{o}}},
doi = {10.1016/0191-765X(85)90015-1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Andor, Jo{\'{o}}, M{\'{e}}r{\"{o}} - 1985 - Galois-lattices A possible representation of knowledge structures.pdf:pdf},
issn = {0191765X},
journal = {Evaluation in Education},
month = {jan},
number = {2},
pages = {207--215},
publisher = {Pergamon},
title = {{Galois-lattices: A possible representation of knowledge structures}},
url = {https://www.sciencedirect.com/science/article/pii/0191765X85900151},
volume = {9},
year = {1985}
}
@article{Fagerholm2012,
abstract = {The evaluation of landscape services essentially deals with the complex and dynamic relationships between humans and their environment. When it comes to landscape management and the evaluation of the benefits these services provide for our well-being, there is a limited representation of stakeholder and intangible values on the land. Stakeholder knowledge is essential, since disciplinary expert evaluations and existing proxy data on landscape services can reveal little of the landscape benefits to the local stakeholders. This paper aims at evaluating the potential of using local stakeholders as key informants in the spatial assessment of landscape service indicators. A methodological approach is applied in the context of a rural village environment in Tanzania, Zanzibar, where local, spatially sensitive stakeholder knowledge is crucial in solving land management challenges as the resources are used extensively for supporting community livelihoods and are threatened by economic uses and agricultural expansion. A typology of 19 different material and non-material, cultural landscape service indicators is established and, in semi-structured interviews, community stakeholders map these indicators individually on an aerial image. The landscape service indicators are described and spatially analysed in order to establish an understanding of landscape level service structures, patterns and relationships. The results show that community involvement and participatory mapping enhance the assessment of landscape services. These benefits from nature demonstrate spatial clustering and co-existence, but simultaneously also a tendency for spatial dispersion, and suggest that there is far more heterogeneity and sensitivity in the ways the benefits are distributed in relation to actual land resources. Many material landscape service indicators are individually based and spatially scattered in the landscape. However, the well-being of communities is also dependent on the non-material services, pointing out shared places of social interaction and cultural traditions. Both material and non-material services are preferred closest to settlements where the highest intensity, richness and diversity are found. Based on the results, the paper discusses the role of local stakeholders as experts in landscape service assessments and implications for local level management processes. It can be pointed out that the integration of participatory mapping methods in landscape service assessments is crucial for true collaborative, bottom-up landscape management. It is also necessary in order to capture the non-utilitarian value of landscapes and sensitivity to cultural landscape services, which many expert evaluations of landscape or ecosystem services fail to do justice. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Fagerholm, Nora and K{\"{a}}yhk{\"{o}}, Niina and Ndumbaro, Festo and Khamis, Miza},
doi = {10.1016/j.ecolind.2011.12.004},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fagerholm et al. - 2012 - Community stakeholders' knowledge in landscape assessments - Mapping indicators for landscape services.pdf:pdf},
issn = {1470160X},
journal = {Ecological Indicators},
keywords = {Ecosystem services,Landscape functions,Landscape management,Landscape values,Participation,Participatory GIS},
month = {jul},
pages = {421--433},
title = {{Community stakeholders' knowledge in landscape assessments - Mapping indicators for landscape services}},
volume = {18},
year = {2012}
}
@misc{WinPure,
abstract = {Commercial data cleaning software},
author = {WinPure},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{Best Data Cleansing Software | Best Data Matching Software https://winpure.com/cleanmatch.html}},
url = {https://winpure.com/cleanmatch.html},
urldate = {2018-03-12},
year = {2018}
}
@inproceedings{Simmhan2006,
abstract = {The increasing ability for the earth sciences to sense the world around us is resulting in a growing need for datadriven applications that are under the control of data-centric workflows composed of grid- and web- services. The focus of our work is on provenance collection for these workflows, necessary to validate the workflow and to determine quality of generated data products. The challenge we address is to record uniform and usable provenance metadata that meets the domain needs while minimizing the modification burden on the service authors and the performance overhead on the workflow engine and the services. The framework, based on a loosely-coupled publish-subscribe architecture for propagating provenance activities, satisfies the needs of detailed provenance collection while a performance evaluation of a prototype finds a minimal performance overhead (in the range of 1{\%} for an eight service workflow using 271 data products).},
annote = {My own work has the potential to incorporate some information about provenance inside the data.},
author = {Simmhan, Yogesh L. and Plale, Beth and Gannon, Dennis},
booktitle = {Proceedings - ICWS 2006: 2006 IEEE International Conference on Web Services},
doi = {10.1109/ICWS.2006.5},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Simmhan, Plale, Gannon - 2006 - A framework for collecting provenance in data-centric scientific workflows.pdf:pdf},
isbn = {0769526691},
pages = {427--434},
publisher = {IEEE},
title = {{A framework for collecting provenance in data-centric scientific workflows}},
url = {http://ieeexplore.ieee.org/document/4032054/},
year = {2006}
}
@article{Lohse2010,
abstract = {The wide application of high-throughput transcriptomics using microarrays has generated a plethora of technical platforms, data repositories, and sophisticated statistical analysis methods, leaving the individual scientist with the problem of choosing the appropriate approach to address a biological question. Several software applications that provide a rich environment for microarray analysis and data storage are available (e.g. GeneSpring, EMMA2), but these are mostly commercial or require an advanced informatics infrastructure. There is a need for a noncommercial, easy-to-use graphical application that aids the lab researcher to find the proper method to analyze microarray data, without this requiring expert understanding of the complex underlying statistics, or programming skills. We have developed Robin, a Java-based graphical wizard application that harnesses the advanced statistical analysis functions of the R/BioConductor project. Robin implements streamlined workflows that guide the user through all steps of two-color, single-color, or Affymetrix microarray analysis. It provides functions for thorough quality assessment of the data and automatically generates warnings to notify the user of potential outliers, low-quality chips, or low statistical power. The results are generated in a standard format that allows ready use with both specialized analysis tools like MapMan and PageMan and generic spreadsheet applications. To further improve user friendliness, Robin includes both integrated help and comprehensive external documentation. To demonstrate the statistical power and ease of use of the workflows in Robin, we present a case study in which we apply Robin to analyze a two-color microarray experiment comparing gene expression in tomato (Solanum lycopersicum) leaves, flowers, and roots.},
author = {Lohse, M. and Nunes-Nesi, A. and Kruger, P. and Nagel, A. and Hannemann, J. and Giorgi, F. M. and Childs, L. and Osorio, S. and Walther, D. and Selbig, J. and Sreenivasulu, N. and Stitt, M. and Fernie, A. R. and Usadel, B.},
doi = {10.1104/pp.109.152553},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lohse et al. - 2010 - Robin An Intuitive Wizard Application for R-Based Expression Microarray Quality Assessment and Analysis.pdf:pdf},
issn = {0032-0889},
journal = {PLANT PHYSIOLOGY},
month = {jun},
number = {2},
pages = {642--651},
pmid = {20388663},
title = {{Robin: An Intuitive Wizard Application for R-Based Expression Microarray Quality Assessment and Analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20388663 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2879776 http://www.plantphysiol.org/cgi/doi/10.1104/pp.109.152553},
volume = {153},
year = {2010}
}
@article{Penha2007,
abstract = {This paper has pedagogical motivation. It is not uncommon that students have great difficulty in accepting the new concepts of standard special relativity, since these seem contrary to common sense. Experience shows that geometrical or graphically exposition of the basic ideas of relativity theory improves student understanding of the algebraic expressions of the theory. What we suggest here may complement standard textbook approaches.},
archivePrefix = {arXiv},
arxivId = {physics/0703002},
author = {Penha, Nilton and Rothenstein, Bernhard},
eprint = {0703002},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Penha, Rothenstein - 2007 - Special Relativity properties from Minkowski diagrams.pdf:pdf},
journal = {arXiv preprint physics/0703002},
month = {feb},
pages = {1--13},
primaryClass = {physics},
title = {{Special Relativity properties from Minkowski diagrams}},
url = {http://arxiv.org/abs/physics/0703002},
year = {2007}
}
@inproceedings{Dongare2017,
abstract = {With data storage and sharing services in the cloud, users can easily modify and share data as a group. To ensure shared data integrity can be verified publicly, users in the group need to compute signatures on all the blocks in shared data. Different blocks in shared data are generally signed by different users due to data modifications performed by different users. For security reasons, once a user is revoked from the group, the blocks which were previously signed by this revoked user must be re-signed by an existing user. The straightforward method, which allows an existing user to download the corresponding part of shared data and re-sign it during user revocation, is inefficient due to the large size of shared data in the cloud. In this paper, we propose a novel public auditing mechanism for the integrity of shared data with efficient user revocation in mind. By utilizing the idea of proxy re-signatures, we allow the cloud to re-sign blocks on behalf of existing users during user revocation, so that existing users do not need to download and re-sign blocks by themselves. In addition, a public verifier is always able to audit the integrity of shared data without retrieving the entire data from the cloud, even if some part of shared data has been re-signed by the cloud. Moreover, our mechanism is able to support batch auditing by verifying multiple auditing tasks simultaneously. Experimental results show that our mechanism can significantly improve the efficiency of user revocation},
author = {Dongare, Dnyanada and Kadroli, Vijayalakshmi},
booktitle = {Proceedings of 2016 Online International Conference on Green Engineering and Technologies, IC-GET 2016},
doi = {10.1109/GET.2016.7916617},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dongare, Kadroli - 2017 - Panda Public auditing for shared data with efficient user revocation in the cloud.pdf:pdf},
isbn = {9781509045563},
issn = {0743166X},
keywords = {Cloud computing,Data integrity,Public auditing,Shared data},
month = {jan},
number = {1},
pages = {92--106},
title = {{Panda: Public auditing for shared data with efficient user revocation in the cloud}},
url = {http://ieeexplore.ieee.org/document/6690209/},
volume = {8},
year = {2017}
}
@article{Engin2019,
abstract = {The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification' of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client' and also ‘public champion' for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.},
author = {Engin, Zeynep and Treleaven, Philip},
doi = {10.1093/comjnl/bxy082},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Engin, Treleaven - 2019 - Algorithmic Government Automating Public Services and Supporting Civil Servants in using Data Science Technolo.pdf:pdf},
issn = {14602067},
journal = {Computer Journal},
keywords = {Internet of Things,artificial intelligence,big data,blockchain,data science,government},
month = {mar},
number = {3},
pages = {448--460},
publisher = {Narnia},
title = {{Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies}},
volume = {62},
year = {2019}
}
@article{Foundation2001,
abstract = {The Canadian Health Services Research Foundation has a mandate to fund a different kind of research -practically oriented work done in collaboration with the people who run the healthcare system, to answer their very concrete questions about how to make the system work better. That means a different style of writing for your final report. Writing a research summary for decision makers is not the same as writing an article for an academic journal. It has a different objective, and it takes a different approach. 1:3:25 Every report prepared for the Foundation has the same guidelines: start with one page of main messages; follow that with a three-page executive summary; present your findings in no more than 25 pages of writing, in language a bright, educated, but not research-trained person would understand. Main Messages The one in the Foundation's 1:3:25 rule is one page of main message bullets. They are the heart of your report, the lessons decision makers can take from your research. Don't confuse them with a summary of findings: you have to go one step further and tell your audience what you think the findings mean for them. The messages, per se, may not even appear in the text. They are what can be inferred from your report. This is your chance, based on your research, to tell decision makers what implications your work has for theirs. How to formulate them? Set aside your text and focus on expressing clear conclusions based on what you've learned. Consider your audience -who are they, and what do they most need to know about what you've learned? Summon up that bright, educated reader and answer this question for him or her: So what does this really mean? Say your study is on how to set budgets in a regional health system. You've found a tendency to keep money flowing on traditional lines. That's the problem. The actual main message you write may be that it's wiser to focus on reallocating other resources -people, space, equipment -to health promotion than to take cash away from acute care. A study on the impact of increasing use of homecare might show that hip-implant patients regain mobility faster out of hospital than as inpatients. The key message would be to encourage early discharge. Spell it out. Your study has found that job security is the biggest factor driving nurses to work in the U.S. Your main message might be that governments should make 10-year commitments to funding levels for nursing services. Writing main messages can be difficult for researchers to do, trained as they are to be detached and to collect evidence, rather than judge it, but it has to be done if research is to be of real use to decision makers. And remember -if you don't do it, you're leaving your work to be interpreted by someone else, who won't likely have your insight.},
author = {Foundation, Canadian Health Services Research},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Foundation - 2001 - Report Writing.pdf:pdf},
issn = {1010-7355},
journal = {Ottawa: Canadian Health {\ldots}},
pages = {1--2},
pmid = {2790829},
title = {{Report Writing}},
url = {http://webarchive.nationalarchives.gov.uk/20140305122816/https://www.civilservice.gov.uk/Assets/cn-1325{\_}e{\_}tcm6-7375.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Reader-Friendly+Writing{\#}0},
year = {2001}
}
@article{Kandel2011b,
abstract = {In spite of advances in technologies for working with data, analysts still spend an inordinate amount of time diagnosing data quality issues and manipulating data into a usable form. This process of 'data wrangling' often constitutes the most tedious and time-consuming aspect of analysis. Though data cleaning and integration are longstanding issues in the database community, relatively little research has explored how interactive visualization can advance the state of the art. In this article, we review the challenges and opportunities associated with addressing data quality issues. We argue that analysts might more effectively wrangle data through new interactive systems that integrate data verification, transformation, and visualization. We identify a number of outstanding research questions, including how appropriate visual encodings can facilitate apprehension of missing data, discrepant values, and uncertainty; how interactive visualizations might facilitate data transform specification; and how recorded provenance and social interaction might enable wider reuse, verification, and modification of data transformations. {\textcopyright} The Author(s) 2011.},
author = {Kandel, Sean and Heer, Jeffrey and Plaisant, Catherine and Kennedy, Jessie and {Van Ham}, Frank and Riche, Nathalie Henry and Weaver, Chris and Lee, Bongshin and Brodbeck, Dominique and Buono, Paolo},
doi = {10.1177/1473871611415994},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kandel et al. - 2011 - Research directions in data wrangling Visualizations and transformations for usable and credible data(2).pdf:pdf},
issn = {14738716},
journal = {Information Visualization},
keywords = {Data cleaning,Data quality,Data transformation,Uncertainty,Visualization},
month = {oct},
number = {4},
pages = {271--288},
title = {{Research directions in data wrangling: Visualizations and transformations for usable and credible data}},
volume = {10},
year = {2011}
}
@article{Mohan2018a,
abstract = {This paper reviews recent advances in missing data research using graphical models to represent multivariate dependencies. We first examine the limitations of traditional frameworks from three different perspectives: $\backslash$textit{\{}transparency, estimability and testability{\}}. We then show how procedures based on graphical models can overcome these limitations and provide meaningful performance guarantees even when data are Missing Not At Random (MNAR). In particular, we identify conditions that guarantee consistent estimation in broad categories of missing data problems, and derive procedures for implementing this estimation. Finally we derive testable implications for missing data models in both MAR (Missing At Random) and MNAR categories.},
archivePrefix = {arXiv},
arxivId = {1801.03583},
author = {Mohan, Karthika and Pearl, Judea},
eprint = {1801.03583},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mohan, Pearl - 2018 - Graphical Models for Processing Missing Data.pdf:pdf},
journal = {arXiv},
month = {jan},
title = {{Graphical Models for Processing Missing Data}},
url = {http://arxiv.org/abs/1801.03583},
year = {2018}
}
@article{Bates,
annote = {id: 1; issn: print 00138703; publication{\_}type: full{\_}text},
author = {Bates, Adam J and Sadler, Jon P and Everett, Glyn and Grundy, Dave and Lowe, Norman and Davis, George and Baker, David and Bridge, Malcolm and Clifton, Jon and Freestone, Roger and Gardner, David and Gibson, Chris and Hemming, Robin and Howarth, Stephen and Orridge, Steve and Shaw, Mark and Tams, Tom and Young, Heather},
doi = {10.1111/eea.12038},
issn = {00138703},
journal = {Entomologia Experimentalis et Applicata},
number = {3},
pages = {397},
title = {{Assessing the value of the Garden Moth Scheme citizen science dataset: how does light trap type affect catch?}},
url = {http://dx.doi.org/10.1111/eea.12038},
volume = {146},
year = {2013}
}
@misc{Aria2018,
abstract = {The use of bibliometrics is gradually extending to all disciplines. It is particularly suitable for science mapping at a time when the emphasis on empirical contributions is producing voluminous, fragmented, and controversial research streams. Science mapping is complex and unwieldly because it is multi-step and frequently requires numerous and diverse software tools, which are not all necessarily freeware. Although automated workflows that integrate these software tools into an organized data flow are emerging, in this paper we propose a unique open-source tool, designed by the authors, called bibliometrix, for performing comprehensive science mapping analysis. bibliometrix supports a recommended workflow to perform bibliometric analyses. As it is programmed in R, the proposed tool is flexible and can be rapidly upgraded and integrated with other statistical R-packages. It is therefore useful in a constantly changing science such as bibliometrics.},
author = {Aria, Massimo and Cuccurullo, Corrado},
booktitle = {Paper},
doi = {10.1016/j.joi.2017.08.007},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Aria, Cuccurullo - 2018 - bibliometrix package An R-tool for comprehensive science mapping analysis.pdf:pdf},
issn = {18755879},
keywords = {Bibliographic coupling,Bibliometrics,Co-citation,R package,Science mapping,Workflow},
number = {0},
pages = {51},
title = {{bibliometrix package: An R-tool for comprehensive science mapping analysis}},
url = {https://cran.r-project.org/web/packages/bibliometrix/bibliometrix.pdf http://www.bibliometrix.org/},
volume = {11},
year = {2018}
}
@article{Brodie1980,
abstract = {Until recently, data quality was poorly understood and seldom achieved, yet it is essential to the effective use of information systems. This paper discusses the nature and importance of data quality. The role of data quality is placed in the life cycle framework. Many new concepts, tools and techniques from both programming languages and database management systems are presented and related to data quality. In particular, the concept of a database constraint is considered in detail. Some current limitations and research directions are proposed. {\textcopyright} 1980.},
annote = {A databasefocussed approach.},
author = {Brodie, Michael L.},
doi = {10.1016/0378-7206(80)90035-X},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Brodie - 1980 - Data quality in information systems.pdf:pdf},
issn = {03787206},
journal = {Information and Management},
keywords = {Data reliability,constraints,correctness,data models,data quality,data structures,database management systems advances in design for,databases,design,integrating structure and behavior,programming languages,schemas,sematic integrity,software engineering,software life cycle,specifications,validation,verification},
month = {jan},
number = {6},
pages = {245--258},
publisher = {North-Holland},
title = {{Data quality in information systems}},
volume = {3},
year = {1980}
}
@book{Pearl2016,
abstract = {Causal Inference in Statistics: A Primer Judea Pearl, Computer Science and Statistics, University of California Los Angeles, USA Madelyn Glymour, Philosophy, Carnegie Mellon University, Pittsburgh, USA and Nicholas P. Jewell, Biostatistics, University of California, Berkeley, USA Causality is central to the understanding and use of data. Without an understanding of cause effect relationships, we cannot use data to answer questions as basic as, "Dus this treatment harm or help patients'" But though hundreds of introductory texts are available on statistical methods of data analysis, until now, no beginner-level book has been written about the exploding arsenal of methods that can tease causal information from data. Causal Inference in Statistics fills that gap. Using simple examples and plain language, the book lays out how to define causal parameters; the assumptions necessary to estimate causal parameters in a variety of situations; how to express those assumptions mathematically; whether those assumptions have testable implications; how to predict the effects of interventions; and how to reason counterfactually. These are the foundational tools that any student of statistics needs to acquire in order to use statistical methods to answer causal questions of interest. This book is accessible to anyone with an interest in interpreting data, from undergraduates, professors, researchers, or to the interested layperson. Examples are drawn from a wide variety of fields, including medicine, public policy, and law; a brief introduction to probability and statistics is provided for the uninitiated; and each chapter comes with study questions to reinforce the readers understanding. Preliminaries : statistical and causal models -- Graphical models and their applications -- The effects of interventions -- Counterfactuals and their applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.1788v3},
author = {Pearl, Judea and Li, Ang and Forney, Andrew and Textor, Johannes},
booktitle = {Causal Inference in Statistics},
doi = {10.1214/09-SS057},
eprint = {arXiv:1112.1788v3},
isbn = {1935-7516},
issn = {1935-7516},
pmid = {25246403},
title = {{Causal inference in statistics: A Primer}},
url = {http://projecteuclid.org/euclid.ssu/1255440554},
year = {2016}
}
@unpublished{BiologicalRecordsCentre2017,
abstract = {Thank you for agreeing to verify records using iRecord and helping to make high quality biological records available to support research and conservation. This is a guide (version 4, 13 March 2017) to verifying records on iRecord. If you have any additional questions please contact us on iRecord@ceh.ac.uk. For general guidance on adding records to iRecord see the Help page, and the more detailed guidance on the NBN website. See the About iRecord page for more information about iRecord's key features, and visit the iRecord forum to chat with other users, ask questions and make suggestions to improve the site.},
author = {Lightfoot, Paula and Harvey, Martin and van Breda, John},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lightfoot, Harvey, van Breda - 2017 - Guidance for verifiers.pdf:pdf},
institution = {Biological Records Centre},
pages = {17},
title = {{Guidance for verifiers}},
url = {https://www.brc.ac.uk/irecord/sites/default/files/guides/verifiers.pdf},
year = {2017}
}
@article{Schubert2005,
abstract = {More than a century after the introduction of incandescent lighting and half a century after the introduction of fluorescent lighting, solid-state light sources are revolutionizing an increasing number of applications. Whereas the efficiency of conventional incandescent and fluorescent lights is limited by fundamental factors that cannot be overcome, the efficiency of solid-state sources is limited only by human creativity and imagination. The high efficiency of solid-state sources already provides energy savings and environmental benefits in a number of applications. However, solid-state sources also offer controllability of their spectral power distribution, spatial distribution, color temperature, temporal modulation, and polarization properties. Such "smart" light sources can adjust to specific environments and requirements, a property that could result in tremendous benefits in lighting, automobiles, transportation, communication, imaging, agriculture, and medicine.},
address = {Rensselaer Polytech Inst, Dept Elect Comp {\&} Syst Engn, Troy, NY 12180 USA. Rensselaer Polytech Inst, Dept Phys Appl Phys {\&} Astron, Troy, NY 12180 USA.; Schubert, EF (reprint author), Rensselaer Polytech Inst, Dept Elect Comp {\&} Syst Engn, Troy, NY 12180 US},
annote = {PT: J; NR: 13; TC: 1494; J9: SCIENCE; PG: 5; GA: 931JT; UT: WOS:000229482300036},
author = {Schubert, E F and Kim, J K},
doi = {10.1126/science.1108712},
issn = {0036-8075},
journal = {Science},
keywords = {GANGLION-CELLS,Science {\&} Technology - Other Topics},
month = {may},
number = {5726},
pages = {1274--1278},
publisher = {AMER ASSOC ADVANCEMENT SCIENCE},
title = {{Solid-state light sources getting smart}},
volume = {308},
year = {2005}
}
@article{Gutowski2013,
abstract = {In this paper, we review the energy requirements to make materials on a global scale by focusing on the five construction materials that dominate energy used in material production: steel, cement, paper, plastics and aluminium. We then estimate the possibility of reducing absolute material production energy by half, while doubling production from the present to 2050. The goal therefore is a 75 per cent reduction in energy intensity. Four technology-based strategies are investigated, regardless of cost: (i) widespread application of best available technology (BAT), (ii) BAT to cutting-edge technologies, (iii) aggressive recycling and finally, and (iv) significant improvements in recycling technologies. Taken together, these aggressive strategies could produce impressive gains, of the order of a 50–56 per cent reduction in energy intensity, but this is still short of our goal of a 75 per cent reduction. Ultimately, we face fundamental thermodynamic as well as practical constraints on our ability to improve the energy intensity of material production. A strategy to reduce demand by providing material services with less material (called ‘material efficiency') is outlined as an approach to solving this dilemma.},
annote = {Access this paper via the DOI; else try file attchment which might work.},
author = {Gutowski, T G and Sahni, S and Allwood, J M and Ashby, M F and Worrell, E},
doi = {10.1098/rsta.2012.0003},
journal = {Philosophical Transactions Royal Society series A},
keywords = {Governance,competitive challenges,consequences,cost models,dashboard/battlespace,decision-making,design process,engineering,green,philosophy,product introduction process,product substitution,production control,resilience,strategic planning,strategy},
number = {1986},
title = {{The energy required to produce materials: constraints on energy-intensity improvements, parameters of demand}},
volume = {371},
year = {2013}
}
@article{Baddeley2005,
abstract = {spatstat is a package for analyzing spatial point pattern data. Its functionality includes exploratory data analysis, model-fitting, and simulation. It is designed to handle realistic datasets, including inhomogeneous point patterns, spatial sampling regions of arbitrary shape, extra covariate data, and 'marks' attached to the points of the point pattern. A unique feature of spatstat is its generic algorithm for fitting point process models to point pattern data. The interface to this algorithm is a function ppm that is strongly analogous to lm and glm. This paper is a general description of spatstat and an introduction for new users.},
author = {Baddeley, Adrian and Turner, Rolf},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Baddeley, Turner - 2005 - spatstat An R Package for Analyzing Spatial Point.pdf:pdf},
journal = {Journal Of Statistical Software},
keywords = {canonical correlations,cross-validation,regularization},
number = {6},
pages = {1--42},
title = {{spatstat: An R Package for Analyzing Spatial Point}},
url = {http://www.jstatsoft.org/},
volume = {12},
year = {2005}
}
@techreport{Healy,
abstract = {ABSTRACT The challenges of ever growing and ever changing Big Data are broad and far-reaching, particularly in the cyber-defense domain. The task of analyzing and making sense of this data is difficult, and only getting worse. We propose that by democratizing data science and making it accessible to everyone, we can expand the breadth and depth of analytics available to a point where we can potentially meet the challenges of Big Data.},
author = {Healy, John and McInnes, Leland and Weir, Colin},
booktitle = {Source: The Cyber Defense Review},
doi = {10.2307/26267404},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Healy, Mcinnes, Weir - 2017 - Bridging the Cyber-Analysis Gap The Democratization of Data Science.pdf:pdf},
number = {1},
pages = {109--118},
publisher = {Army Cyber Institute},
title = {{Bridging the Cyber-Analysis Gap: The Democratization of Data Science}},
url = {https://www.jstor.org/stable/26267404},
volume = {2},
year = {2017}
}
@article{Wu2014,
abstract = {Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.},
archivePrefix = {arXiv},
arxivId = {PAI},
author = {Wu, Xindong and Zhu, Xingquan and Wu, Gong Qing and Ding, Wei and {Xindong Wu} and {Xingquan Zhu} and {Gong-Qing Wu} and {Wei Ding}},
doi = {10.1109/TKDE.2013.109},
eprint = {PAI},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Xindong Wu et al. - 2014 - Data mining with big data(2).pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Big Data,autonomous sources,complex and evolving associations,data mining,heterogeneity},
mendeley-tags = {Big Data},
month = {jan},
number = {1},
pages = {97--107},
pmid = {92680836},
title = {{Data mining with big data}},
url = {http://ieeexplore.ieee.org/document/6547630/},
volume = {26},
year = {2014}
}
@article{Torrecilla2018,
abstract = {Technology is generating a huge and growing availability of observations of diverse nature. This big data is placing data learning as a central scientific discipline. It includes collection, storage, preprocessing, visualization and, essentially, statistical analysis of enormous batches of data. In this paper, we discuss the role of statistics regarding some of the issues raised by big data in this new paradigm and also propose the name of data learning to describe all the activities that allow to obtain relevant knowledge from this new source of information.},
author = {Torrecilla, Jos{\'{e}} L. and Romo, Juan},
doi = {10.1016/j.spl.2018.02.038},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Torrecilla, Romo - 2018 - Data learning from big data.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Big data,Data learning,Statistics},
month = {may},
pages = {15--19},
publisher = {North-Holland},
title = {{Data learning from big data}},
url = {https://www.sciencedirect.com/science/article/pii/S016771521830083X},
volume = {136},
year = {2018}
}
@article{Zhu2017,
abstract = {Negative selection algorithm (NSA) is an important kind of the one-class classification model, but it is limited in the big data era due to its low efficiency. In this paper, we propose a new NSA based on Voronoi diagrams: VorNSA. The scheme of the detector generation process is changed from the traditional “Random-Discard” model to the “Computing-Designated” model by VorNSA. Furthermore, we present an immune detection process of VorNSA under Map/Reduce framework (VorNSA/MR) to further reduce the time consumption on massive data in the testing stage. Theoretical analyses show that the time complexity of VorNSA decreases from the exponential level to the logarithmic level. Experiments are performed to compare the proposed technique with other NSAs and one-class classifiers. The results show that the time cost of the VorNSA is averagely decreased by 87.5{\%} compared with traditional NSAs in UCI skin dataset.},
author = {Zhu, Fangdong and Chen, Wen and Yang, Hanli and Li, Tao and Yang, Tao and Zhang, Fan},
doi = {10.1155/2017/3956415},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - A Quick Negative Selection Algorithm for One-Class Classification in Big Data Era.pdf:pdf},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
pages = {1--7},
title = {{A Quick Negative Selection Algorithm for One-Class Classification in Big Data Era}},
url = {https://www.hindawi.com/journals/mpe/2017/3956415/},
volume = {2017},
year = {2017}
}
@inproceedings{Ali2016,
abstract = {In today's world where everything is recorded digitally, right from our web surfing patterns to our medical records, we are generating and processing petabytes of data every day. Big data will be transformative in every sphere of life. But just to process and analyze those data is not enough, human brain tends to find pattern more efficiently when data is represented visually. Data Visualization and Analytics plays important role in decision making in various sectors. It also leads to new opportunities in the visualization domain representing the innovative ideation for solving the big-data problem via visual means. It is quite a challenge to visualize such a mammoth amount of data in real time or in static form. In this paper, we discuss why big data visualization is of utmost importance, what are the challenges related to it and review some big data visualization tools.},
author = {Ali, Syed Mohd and Gupta, Noopur and Nayak, Gopal Krishna and Lenka, Rakesh Kumar},
booktitle = {2016 2nd International Conference on Contemporary Computing and Informatics (IC3I)},
doi = {10.1109/IC3I.2016.7918044},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ali et al. - 2016 - Big data visualization Tools and challenges.pdf:pdf},
isbn = {978-1-5090-5256-1},
month = {dec},
pages = {656--660},
publisher = {IEEE},
title = {{Big data visualization: Tools and challenges}},
url = {http://ieeexplore.ieee.org/document/7918044/},
year = {2016}
}
@article{Vines2014,
abstract = {Policies ensuring that research data are available on public archives are increasingly being implemented at the government [1], funding agency [2-4], and journal [5, 6] level. These policies are predicated on the idea that authors are poor stewards of their data, particularly over the long term [7], and indeed many studies have found that authors are often unable or unwilling to share their data [8-11]. However, there are no systematic estimates of how the availability of research data changes with time since publication. We therefore requested data sets from a relatively homogenous set of 516 articles published between 2 and 22 years ago, and found that availability of the data was strongly affected by article age. For papers where the authors gave the status of their data, the odds of a data set being extant fell by 17{\%} per year. In addition, the odds that we could find a working e-mail address for the first, last, or corresponding author fell by 7{\%} per year. Our results reinforce the notion that, in the long term, research data cannot be reliably preserved by individual researchers, and further demonstrate the urgent need for policies mandating data sharing via public archives. {\textcopyright} 2014 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {1312.5670},
author = {Vines, Timothy H. and Albert, Arianne Y K and Andrew, Rose L. and D{\'{e}}barre, Florence and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Moore, Jean S{\'{e}}bastien and Renaut, S{\'{e}}bastien and Rennison, Diana J.},
doi = {10.1016/j.cub.2013.11.014},
eprint = {1312.5670},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vines et al. - 2014 - The availability of research data declines rapidly with article age.pdf:pdf},
isbn = {0960-9822},
issn = {09609822},
journal = {Current Biology},
month = {jan},
number = {1},
pages = {94--97},
pmid = {24361065},
title = {{The availability of research data declines rapidly with article age}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982213014000},
volume = {24},
year = {2014}
}
@article{Perer2008,
abstract = {Although both statistical methods and visualizations have been used by network analysts, exploratory data analysis remains a challenge. We propose that a tight integration of these technologies in an interactive exploratory tool could dramatically speed insight development. To test the power of this integrated approach, we created a novel social network analysis tool, SocialAction, and conducted four long-term case studies with domain experts, each working on unique data sets with unique problems. The structured replicated case studies show that the integrated approach in SocialAction led to significant discoveries by a political analyst, a bibliometrician, a healthcare consultant, and a counter-terrorism researcher. demonstrate that the tight Our contributions integration of statistics and visualizations improves exploratory data analysis, and that our evaluation methodology for long-term case studies captures the research strategies of data analysts.},
address = {New York, New York, USA},
author = {Perer, Adam and Shneiderman, Ben},
doi = {10.1145/1357054.1357101},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Perer, Shneiderman - 2008 - Integrating Statistics and Visualization Case Studies of Gaining Clarity during Exploratory Data Analysis.pdf:pdf},
isbn = {9781605580111},
issn = {0272-1716},
journal = {Proceeding of the twenty-sixth annual CHI conference on Human factors in computing systems - CHI '08},
keywords = {Big Data,case studies,evaluation,exploratory data analysis,information visualization,social networks,statistics},
mendeley-tags = {Big Data},
pages = {10},
publisher = {ACM Press},
title = {{Integrating Statistics and Visualization: Case Studies of Gaining Clarity during Exploratory Data Analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1357054.1357101},
year = {2008}
}
@phdthesis{Lauriault2012,
abstract = {The central argument of this dissertation is th at Canadian reality is conditioned by government data and their related infrastructures. Specifically, that Canadian geographical imaginations are strongly influenced by the Atlas of Canada and the Census of Canada. Both are long standing government institutions that inform government decision-making, and are normally considered to be objective and politically neutral. It is argued that they may also not be entirely politically neutral even though they may not be influenced by partisan politics, because social, technical and scientific institutions nuance objectivity. These institutions or infrastructures recede into the background of government operations, and although invisible, they shape how Canadian geography and society are imagined. Such geographical imaginations, it is argued, are important because they have real material and social effects. In particular, this dissertation empirically examines how the Atlas of Canada and the Census of Canada, as knowledge formation objects and as government representations, affect social and material reality and also normalize subjects. It is also demonstrated th at the Ian Hacking dynamic Looping Effect framework of ‘Making Up People' is not only useful to the human sciences, but is also an effective methodology that geographers can adapt and apply to the study of ‘Making Up Spaces' and geographical imaginations. His framework was adapted to the study of the six editions of the Atlas of Canada and the Census of Canada between 1871 and 2011. Furthermore, it is shown that the framework also helps structure the critical examination of discourse, in this case, Foucauldian gouvemementalit{\'{e}} and the biopower of socio-techno-political systems such as a national atlas and census, which are inextricably embedded in a social, technical and scientific milieu. As objects they both reflect the dominant value system of their society and through daily actions, support the dominance of this value system. While it is people who produce these objects, the infrastructures th at operate in the background have technological momentum th at also influence actions. Based on the work of Bruno Latour, the Atlas and the Canadian census are proven to be inscriptions that are immutable and mobile, and as such, become actors in other settings. Therefore, the Atlas of Canada and the Census of Canada shape and are shaped by geographical imaginations.},
address = {Ottawa, Canada},
author = {Lauriault, Tracey P},
doi = {10.22215/etd/2012-09890},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lauriault - 2012 - Data, Infrastructures and Geographical Imaginations.pdf:pdf},
school = {Carleton University},
title = {{Data, Infrastructures and Geographical Imaginations}},
year = {2012}
}
@article{Tomasello2017,
abstract = {We analyze large-scale data sets about collaborations from two different domains: economics, specifically 22.000 R{\&}D alliances between 14.500 firms, and science, specifically 300.000 co-authorship relations between 95.000 scientists. Considering the different domains of the data sets, we address two questions: (a) to what extent do the collaboration networks reconstructed from the data share common structural features, and (b) can their structure be reproduced by the same agent-based model. In our data-driven modeling approach we use aggregated network data to calibrate the probabilities at which agents establish collaborations with either newcomers or established agents. The model is then validated by its ability to reproduce network features not used for calibration, including distributions of degrees, path lengths, local clustering coefficients and sizes of disconnected components. Emphasis is put on comparing domains, but also sub-domains (economic sectors, scientific specializations). Interpreting the link probabilities as strategies for link formation, we find that in R{\&}D collaborations newcomers prefer links with established agents, while in co-authorship relations newcomers prefer links with other newcomers. Our results shed new light on the long-standing question about the role of endogenous and exogenous factors (i.e., different information available to the initiator of a collaboration) in network formation.},
archivePrefix = {arXiv},
arxivId = {1704.01342},
author = {Tomasello, Mario V and Vaccario, Giacomo and Schweitzer, Frank},
doi = {10.1140/epjds/s13688-017-0117-5},
eprint = {1704.01342},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Tomasello, Vaccario, Schweitzer - 2017 - Data-driven modeling of collaboration networks a cross-domain analysis.pdf:pdf},
isbn = {1368801701175},
issn = {21931127},
journal = {EPJ Data Science},
keywords = {agent-based model,complex network},
month = {dec},
number = {1},
pages = {22},
pmid = {12431062483267097210},
publisher = {Springer Berlin Heidelberg},
title = {{Data-driven modeling of collaboration networks: a cross-domain analysis}},
url = {http://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0117-5},
volume = {6},
year = {2017}
}
@techreport{NIST-CPS132013,
address = {Washington DC},
annote = {Paper in Sustainability. 

NB This is from the executive summary. the full workshop report it in File Attachments as well, but it has some stupidities. Perhaps better to quote the exec summary. 

Good, layman's document, engineering-oriented, but does recognise some high-level human issues, but these are not integrated into the rest of the text. 

Good source of quotes; see below.

Scientific challenges:
• Integrating complex, heterogeneous large-scale systems. Future CPS will contain heterogeneous distributed components and systems of large numbers that must work together effectively to deliver expected performance. There are several challenges to achieving this today. A fundamental issue is the lack of common terminology, modeling languages, and rigorous semantics for describing interactions—physical and computational—across heterogeneous systems. A lack of clear ownership of the interface between systems (e.g., between code, hardware, and multiple equipment vendors) also contributes to interoperability and integration problems. In addition to standards, interoperable systems need to ensure that timely outputs, outcome agreements, resilience, data transfers, and technical security protocols are addressed seamlessly within and between components. This includes aggregating and sharing data within systems as well as across systems and components. 

• Interaction between humans and systems. Current models for human and machine behaviors are not adequate for designing CPS when humans and machines closely interact. One of the challenges is model-ing and measuring situational awareness—human percep-tion of the system and its environment and changes in parameters that are critical to decision-making. This is particularly necessary for complex, dynamic systems, such as those used in aviation, air trafﬁ c control, power plant operations, military command and control, and emergency services. In such systems situational awareness can involve large and unpredictable combinations of human and machine behavior. Inadequate situational awareness and limited ability to model the human component in large complex systems has been identiﬁed as one of the primary factors in accidents related to human error (Nullmeyer et al, 2005).
[Not poor design?]

• Dealing with uncertainty. Complex CPS need to be able to evolve and operate reliably in new and uncertain environments. An increasing number of these systems will also demonstrate emergent and unknown behaviors as they become more and more reliant on machine learning methodologies. In both cases, uncertainty in the knowledge or outcome of a process will require new ways to quantify uncertainty during the CPS design and development stages. Current methods for character-ization and quantiﬁ cation of uncertainty are limited and inadequate. This is exacerbated by the limits of reliability and accuracy of physical components, the validity of models characterizing them, network connections, and potential design errors in software. Ongoing debate also surrounds the expectations for quantifying uncertainty, that is, attaining perfect results given the uncertainty of the physical world and approximations in design. 
[Nothing about unpredictable environments? Complexity issues?]

• Measuring and verifying system performance. The difﬁculty of verifying performance, accuracy, reliability, security, and various other requirements impedes development and investment in CPS. Today's capabilities for veriﬁ cation and validation (V{\&}V) of CPS are limited, time consuming, and costly, particularly when compared to development time.
[Don't seem to appreciate the effects of CPS evolution, autonomy and learning. You need run-time V{\&}V as well]

• System design. The design of CPS is hampered by the limited ability to design at a systems-level. There are many fac-tors impeding system-level design, such as the lack of formalized high ﬁdelity models for large systems, insufﬁ cient ways of measuring performance, and inadequate scientiﬁ c foundations (e.g., no ‘science of systems'). A key factor is compositionality and modularity in the design approach. Compositionality in CPS is impacted by the strong interdependencies of software and systems engineering and often limited by poor system design. For example, CPS development could be greatly facilitated if system components could be developed and veriﬁed in isolation and the system-level properties inferred from the properties of its parts. Designers of CPS aspire to this modular and compo-sitional approach both in design and veriﬁcation. However, it is only currently possible in narrow domains and with restricted, simple properties. Scientiﬁc and technical challenges to achieving compositionality include a lack of mathematical and system science foundations, formalized metrics, evaluation techniques, and methods for dealing with cross-cutting properties in the design space.

Institutional, societal and other challenges:
• Trust, security, and privacy. Assuring that systems are trustwor-thy, secure, and protect the privacy of information creates both technical and policy challenges. Cyber-security is a critical aspect of CPS on many levels, including the protection of national infrastructure, privacy of individuals, system integrity, and intellectual prop-erty. Recent foreign-based intrusions on U.S. computer systems, both government and commercial, illustrate the current vulnerabilities of the Internet and the rationale for addressing the global security of cyberspace (GAO, 2010). While cyber-security is a strong national priority and much progress has been made to ensure protection from cyber-attacks, CPS security raises a host of new challenges. For example, the combination of cyber and physical vulnerabilities may lead to attack models that are fundamentally new, hard to analyze, and carry substantial risk in maintaining physical integrity of critical systems. 

• Effective models of governance. The rapidly emerging global networks of CPS in energy, air trafﬁ c, transportation, cloud-based services and many others call for new governance models—both domestic and global—for providing standards, protocols, and oversight of systems that operate both in physical and cyber space. These new governance models are being explored but are not yet formalized. Governance could provide structured control and regulation for these systems and reduce liabilities that arise because of unwanted intrusions or other vulnerabilities. Governance is being discussed in many organizations, ranging from expert forums to treaty-based, decision-making bodies within governments. There is growing debate around these issues, with some push-ing for increased intergovernmental oversight while others contend that the private sector can self-regulate via development of appropriate economic incentives, rules, and controls. 

• Creation of CPS business models. The extreme systems integration inherent in CPS is a disruptive technology that changes the status quo, creates new industries, and eliminates others. Transformation of traditional industries into those that are CPS-based is a complex, high risk process because it requires fusing the business models of the IT industry with those of engineering-based industries. These fused business models are not yet well-established and can be difﬁ cult to convey. A contributing factor is that economic and other data that could be used to sup-port a business case are not well documented for CPS. The lack of a generic, proven business model can inhibit investments in new technologies and systems, in spite of the beneﬁts. 

• Understanding the value of CPS. CPS will beneﬁ t from well-developed infrastructure, which requires signiﬁ cant upfront investment. The value of CPS needs to be better understood for such investments to occur. R{\&}D on CPS is often described in terms that are theoretical or include vocabulary that is not readily recognized. As a result, understanding the substance and apply-ing the results of CPS R{\&}D can be challenging for businesses, decision-makers, and end-users. Less academic and more strategi-cally insightful ways of presenting CPS research, beneﬁ ts, and risks would facilitate quicker and less expensive industry adoption of emerging technologies as well as improved understanding of the beneﬁ ts and applications of CPS research. Some studies have pre-sented methods to successfully articulate the value of CPS-related technology (BAH, 2010) but overall this remains a challenge. 

• Multi-disciplinary education and collaboration. The science and engineering of CPS are cross-disciplinary in nature, requiring expertise in computer science, mathematics, statistics, engineering, and the full spectrum of physical sciences—even extending into the arts such as ethics and psychology. Working across disciplines can be challenging, as it requires experts with highly diverse backgrounds to communicate on a com-mon basis.

• Skilled workforce. CPS are sophisticated, advanced technology systems that require knowledge and training to design, develop, implement, and use. They require new skills and a new workforce. Creating and maintaining a skilled workforce to support future CPS is a signiﬁ cant challenge in its own right. CPS technology is a rapidly changing ﬁ eld and mechanisms for training and continu-ing education will be needed, as well as qualiﬁ ed instruc-tors that stay abreast of emerging developments. Rigorous tools for workforce training in CPS are not currently available but could be highly effective in creating and maintaining a future workforce. 

R{\&}D opportunities:Robust, Effective Design and Construction of Systems and Infrastructure
• Develop cost-eff ective system design, analysis, and construction - Today, building formalized, high fidelity models using mathematically based, formalized modeling languages is expensive, time consuming, and lacking tools and methods for large heterogeneous systems such as CPS. Such models should include an appropriate level of abstraction for the properties relevant to the system being designed, and be able to simulate system behavior under a range of conditions and assumptions. New, formal modeling methods are needed to create robust, physically relevant simulations that accurately recreate scenarios that CPS systems will experience in operation. Ultimately, domain-specific CPS design tools are required for aerospace, defense, transportation, medicine, and 
other industries that are built on standardized, configurable, and reusable tool suites for safety-critical and high-reliability systems. In addition to system modeling, major challenges include designing to confl icting requirements of system components (which can cause unintended consequences), a lack of tools or framework for co-designing heterogeneous components and systems, a lack of design standards to enable interoperability, and 
a lack of foundations to enable compositionality.

• Create domain-specific frameworks for design —Engineering methods for co-design and new standards are needed that off er a common semantic foundation for modeling languages for exchange and translation across domains. Creating domain-specific design frameworks that are built on generic but customizable methods and tools would contribute substantially to reducing time to market, development costs, and the complexity of the design process. Finally, the design and implementation of CPS needs to be understood as a process that includes not only evaluation and co-design, but incorporates the ability to build sophistication as the levels of need advance. 

• Manage the role of time and synchronization in architecture design —Management of time and synchronization is a complex yet critical issue for real-time CPS. Today, timekeeping technologies such as Global Positioning System satellites and the Network Time Protocol provide real-time approximation of Coordinated Universal Time (world time standard) and are used for many synchronization applications. Poor timing and synchronization can result in data loss, downtime, and performance failure. Major challenges include eff ective timing and synchronization of multiple tasks, developing a unifi ed, common view of time, measuring time and time scales, and communicating time characteristics to system components or sensors. Overcoming these challenges could impact any data driven, real-time application. Simply put, eff ective time management will make it easier for applications to run in a time-correct manner. Tackling these challenges may require multi-layered architecture for time management.

• Enable natural, more seamless human-CPS interactions —A better model of human strengths and weaknesses and the corresponding machine strengths and weaknesses is needed to create a more natural, seamless interaction between humans and CPS. Models that are adaptive, implementable at varying degrees of sophistication, and optimized for human interventions will help manage risks and safety as systems move toward mixed-initiative modes of operation. Th ey could also make humans more comfortable with and accepting of machine interactions. Cognitive models are needed for human-machine behavior that can be validated and become adaptable to interactions as they occur. Cognitive models should also consider the growing number of autonomous CPS and that human-machine interactions are increasingly participatory. Th e requirement to couple unpredictable human behavior with the predictable, hard-wired behavior of machines and physical systems creates inherent diffi culties in developing such models. 
[personnel churn, human variability, STS priniple, trust, acceptance, alienation, etc. - all omitted, as is authority and responsibilty]

• Develop systematic inter-process and inter-personal communication for sensors and actuators —A core component of CPS is the interpretation of data from various sources. CPS can contain highly connected and massive networks of sensors, actuators, and other devices that collect and act on many types of data. It is inherently difficult to measure the behavior of complex systems that contain multiple pathways for data interpretation, planning, and control. The need to measure human interactions adds another level of complexity and uncertainty. A structured design and process integration method is needed to systematically relate multiple signals and symbols for inter-process and interpersonal communications across domains and applications.

Improved Performance and Quality Assurance of Computational and Physical Systems
• Create methods for system-level evaluation, verifi cation, and validation of cyber-physical systems —Evaluating 
the performance of CPS against system requirements is needed to facilitate acceptance, investment, and practical use of these systems. Some classes of CPS will require extensive and sustained investment (e.g., smart transportation, smart grid) and a solid understanding of potential performance to move technologies forward. System-level evaluation can be performed with V{\&}V methods, especially for safety and trustworthiness requirements, but without standardized requirements, V{\&}V is customized and costly. V{\&}V is also challenged by an inability to eff ectively evaluate the whole system (how well components work in concert) since the performance of individual components (i.e., cyber, physical, and cyber-physical assemblies) does not necessarily translate to overall system performance. Foundations and infrastructure are currently lacking for evaluation and V{\&}V of emerging CPS, but could be developed by leveraging methods and tools already in use in other systems. An integrated approach will be needed to enable greater understanding of the interactions between components, the role and impact of interfaces, and emerging system properties. Autonomously operating systems (those with little human interaction or decision making) require certifi cation processes that attest to assured system performance. Certifi cation is a judgment that a system is adequately safe, secure, or meets other criteria for a given application in a set environment. To be valid, this judgment should be based on as much explicit and credible evidence as possible, with a foundation in good metrics including ways to measure complexity. A challenge is to create methodology to enable compositional certification, which includes certification of components separately without the need for re-certifying after the system components are integrated. 

• Develop science-based metrics (e.g., security, privacy, safety, resilience, adaptability, flexibility, reusability, dependability)—A universal set of science-based metrics is needed to evaluate and predict how CPS will perform with respect to key system-level properties such as security, privacy, safety, resiliency, and dependability. Dependability in this case means that a system is highly reliable when running, but also capable of eff ectively predicting, recognizing, and quickly covering from unforeseen events. While it is technically challenging to develop scientifi cally-based measurements for these broad concepts, they are fundamental to developing and deploying dependable CPS.

• Effectively characterize and quantify reliability amidst uncertainties—Reliable CPS must behave with some degree of certainty, even in a dynamic, unpredictable environment. Characterization and quantiﬁcation of reliability provides information on how a system responds to expected and unexpected events, and aids in understanding the potential risks to system operation. The numerous heterogeneous components, disparate characteristics of the physical versus cyber elements, and operational uncertainties found in CPS complicate the characterization of reliability. Failures could occur in both cyber snd physical components and affect other system componentsin in complex ways. For example, multiple car accidents experienced by a smart traffic control system could unexpectedly overload the information processing capacity and its ability to respond in real time.

Effective and Reliable System Integration and Interoperability
• Create universal definitions for representing ultra-large heterogeneous systems —Standard methods and shared conceptualization are needed for aligning the description of large, heterogeneous groups of system components, characteristic of many CPS, including specifi cations for technology, human elements, time, and space.

• Build an inter-connected and interoperable shared development infrastructure —Th e current market does not have governance or business models in place to motivate the development of networked, cooperating, human interactive systems. Developers must assume the risk of sharing proprietary information with competitors and the liability of successfully integrating their systems with external systems to ensure high levels of performance and functionality. For example, the manufacturers of autonomous cars will have to work with each other as well as with the developers of the traffic regulating infrastructure to develop functional products.

• Develop abstraction infrastructure to bridge digital and physical system components —Innovative approaches to abstractions and architectures that enable seamless integration of digital and physical systems for control, communication, and computation are needed for development of CPS. . In these systems, issues arise from the safety and reliability requirements of the physical components that are qualitatively diff erent from those of the computing components. Because physical components are qualitatively different from software components, standard abstractions that are only physical or only computational fail when used in CPS.

Dynamic, Multi-Disciplinary Education and Training
• Establish multi-disciplinary CPS degrees and resources 

• Pursue dynamic training and certification in CPS},
author = {NIST-CPS'13},
institution = {NIST , Intelligent Systems},
publisher = {National Institute of Standards {\&} Technology Division},
title = {{Foundations for innovations in Cyber-Physical Systems}},
url = {http://events.energetics.com/NIST-CPSWorkshop/downloads.html},
year = {2013}
}
@article{Thomas2004,
abstract = {There is growing concern about increased population, regional, and global extinctions of species. A key question is whether extinction rates for one group of organisms are representative of other taxa. We present a comparison at the national scale of population and regional extinctions of birds, butterflies, and vascular plants from Britain in recent decades. Butterflies experienced the greatest net losses, disappearing on average from 13{\%} of their previously occupied 10-kilometer squares. If insects elsewhere in the world are similarly sensitive, the known global extinction rates of vertebrate and plant species have an unrecorded parallel among the invertebrates, strengthening the hypothesis that the natural world is experiencing the sixth major extinction event in its history.},
archivePrefix = {arXiv},
arxivId = {1007.3029},
author = {Thomas, J A and Telfer, M G and Roy, D B and Preston, C D and Greenwood, J. J.D. and Asher, J and Fox, R and Clarke, R T and Lawton, J H},
doi = {10.1126/science.1095046},
eprint = {1007.3029},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Thomas et al. - 2004 - Comparative Losses of British Butterflies, Birds, and Plants and the Global Extinction Crisis.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {00368075},
journal = {Science},
month = {mar},
number = {5665},
pages = {1879--1881},
pmid = {15031508},
publisher = {American Association for the Advancement of Science},
title = {{Comparative Losses of British Butterflies, Birds, and Plants and the Global Extinction Crisis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15031508},
volume = {303},
year = {2004}
}
@incollection{Guarino2013,
abstract = {Information Security is becoming the victim of the disruptive change introduced by the latest trends in In- formation technology: Bring Your Own Device (BYOD), Cloud Computing and Social Networking. Today a corporation's systems development, and therefore that of its technical security controls, is slipping away from the carefully planned IT strategy. For the information security professional, this makes for a very challenging management landscape. This paper outlines the changing dynamics for security governance brought about by the shifts currently taking place in IT technology against the context of the developing expectations coming from our increas- ingly active policy makers. It offers the first published focus on the EMEA findings of the 2013 (ISC) 2 Work- force Study, citing the experience of 3229 people in the region with responsibility for information security, including analysis for Germany, France and the United Kingdom. Findings confirm the shift in IT: • Virtuallyallcompanieshavesomelevelofcloudcomputing, • Almosthalfofcompaniesthatallowanyuserdevicetoaccesstheircorporatenetworks, • Social media too is evolving from its beginning as a consumer platform with approved business use. Survey insights look at the impact of these developments and stresses in the ability to defend and recover from attack. At the light of these results, we will review and comment the various policy proposals currently under discussions in the various EU instances},
author = {Guarino, Alessandro},
booktitle = {ISSE 2013 Securing Electronic Business Processes},
doi = {10.1007/978-3-658-03371-2_17},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Guarino - 2013 - Digital Forensics as a Big Data Challenge.pdf:pdf},
isbn = {978-3-658-03370-5},
organization = {StudioAG},
pages = {197--203},
publisher = {Springer Vieweg, Wiesbaden},
title = {{Digital Forensics as a Big Data Challenge}},
url = {https://s3.amazonaws.com/academia.edu.documents/32165912/DigitalForensicsBigData.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A{\&}Expires=1531150704{\&}Signature=pAYoDDHcNPL{\%}2BU0MNE9Da1KY7xkI{\%}3D{\&}response-content-disposition=inline{\%}3B filename{\%}3DDigital{\_}Forensics{\_}as{\_}a},
year = {2013}
}
@article{Giczi2018,
abstract = {More than five years ago Eurostat started a project with the aim to 'tame' sources of Big Data in a way that they can be incoporated into official statistical systems. In order to solve the problems a statistician might be faced with during the official statistical application of Big Data, first of all, we give an overview of traditional data collection, and then point to the differences one has to face when dealing with Big Data. We introduce common sources of data (traditional, administrative) and highlight the ways huge sets of data are different compared to them. Next, we discuss characteristics of Big Data versus traditional statistical methods based on the qualitative criteria of official statistics, and we also elaborate on the problems of analysing Big Data. Finally, we provide a list of use cases for Big Data in official statistical data collections.},
author = {Giczi, Johanna and Szőke, Katalin},
doi = {10.17356/ieejsp.v4i1.408},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Giczi, Szőke - 2018 - Official Statistics and Big Data.pdf:pdf},
issn = {2416089X},
journal = {Intersections},
month = {jan},
number = {1},
pages = {24},
title = {{Official Statistics and Big Data}},
url = {http://intersections.tk.mta.hu/index.php/intersections/article/view/408},
volume = {4},
year = {2018}
}
@article{VanderVegt2018,
abstract = {Over the last two decades demands for greater public engagement have emerged in policy circles and academia, particularly when it comes to risk-related decision-making, or risk governance. However, the literature shows there is a lack of evidence when it comes to the impact of public engagement initiatives and significant questions remain over who to include, what processes to follow and what outcomes to expect. Furthermore, the literature exhibits contradictions in how researchers with different theoretical approaches attempt to answer these kinds of questions. This paper therefore proposes a systematic literature review in order to map the current breadth and variation in the literature and to identify any major variations from previous findings. A methodical search query has been applied to Scopus and Web of Science to search for academic articles. These were subsequently assessed for their suitability through a structured literature selection process. The results identify a number of methodologically different approaches in which knowledge on risk governance and public engagement has been developed. These diverse approaches are eventually grouped into clusters based on similarities in co-citations and references that are identified through bibliometrics and a subsequent content analysis. The proposed clusters have been labeled risk governance; environmental science, policy and governance; disaster risk management; science and technology studies; post-normal science; and public understanding of science. These six clusters are ultimately discussed and differentiated based on their main features which is particularly relevant for researchers and policy-makers seeking to get an understanding of, or broaden their disciplinary engagement with, risk governance and public engagement. {\textcopyright} 2017 Informa UK Limited, trading as Taylor {\&} Francis Group},
author = {van der Vegt, R. G.},
doi = {10.1080/13669877.2017.1351466},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/van der Vegt - 2018 - A literature review on the relationship between risk governance and public engagement in relation to complex envir.pdf:pdf},
issn = {14664461},
journal = {Journal of Risk Research},
keywords = {Risk governance,environmental issues,participation,policy-making,public engagement,stakeholders},
month = {nov},
number = {11},
pages = {1--18},
publisher = {Routledge},
title = {{A literature review on the relationship between risk governance and public engagement in relation to complex environmental issues}},
url = {https://www.tandfonline.com/doi/full/10.1080/13669877.2017.1351466},
volume = {21},
year = {2018}
}
@article{Marx2011,
abstract = {This paper deals with the specific features of historical papers relevant for information retrieval and bibliomet- rics.The analysis is based mainly on the citation indexes accessible under theWeb of Science (WoS) but also on field-specific databases: the Chemical Abstracts Service (CAS) literature database and theINSPECdatabase. First, the journal coverage of theWoS (in particular of theWoS Century of Science archive), the limitations of specific search fields as well as several database errors are dis- cussed. Then, the problem of misspelled citations and their“mutations” is demonstrated by a few typical exam- ples.Complex author names,complicated journal names, and other sources of errors that result from prior citation practice are further issues. Finally, some basic phenom- ena limiting the meaning of citation counts of historical papers are presented and explained.},
author = {Marx, Werner},
doi = {10.1002/asi.21479},
isbn = {1532-2882},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {3},
pages = {433--439},
title = {{Special features of historical papers from the viewpoint of bibliometrics}},
url = {http://doi.wiley.com/10.1002/asi.21479},
volume = {62},
year = {2011}
}
@techreport{Reinhart2011,
abstract = {Acoustic measurements have shown that children's segment durations are often longer than those of adults. It is unclear from previous research, however, why this occurs. One possibility is that children's less mature peripheral speech mechanisms limit the rate at which they can perform articulatory movements and that this results in longer acoustic segment durations. Another possible cause of children's longer segment durations is that "higher-order," organizational factors might affect their ability to plan and sequence phonetic strings. This too could limit the rate at which they produce speech gestures, thus resulting in longer segment durations. The present study attempted to clarify such issues by obtaining articulatory movement data from four children and three adults using strain gage instrumentation to monitor superior-inferior lip and jaw displacement and velocity. Results indicate that the children tended to move their articulators more slowly than the adults, despite the fact that both groups exhibited comparable articulatory displacements. Differences observed between the adults and children concerning vocal tract size and movement variability indicate that "lower-level," physical factors may be partially involved in children's slower articulatory movements. However, differences between the two groups regarding anticipation of certain articulatory sequences suggest that "higher-order," organizational effects may also be associated with children's slower movements and, therefore, their longer segment durations. MESH Subject(s) below:},
address = {Cambridge, MA},
author = {Reinhart, Carmen and Rogoff, Kenneth},
doi = {10.3386/w16827},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Reinhart, Rogoff - 2011 - A Decade of Debt.pdf:pdf},
institution = {National Bureau of Economic Research},
isbn = {9780881326222$\backslash$r0881326224},
issn = {1573-4471},
month = {feb},
pmid = {16818423},
title = {{A Decade of Debt}},
url = {http://www.nber.org/papers/w16827.pdf},
year = {2011}
}
@article{Dehainsala2007,
abstract = {Recently, several approaches and systems were proposed to store in the same database data and the ontologies describing their meanings. We call these databases, ontology-based databases (OBDBs). Ontology-based data denotes those data that represent ontology individuals (i.e., instance of ontology classes). To speed up query execution on the top of these OBDBs, efficient representations of ontology-based data become a new challenge. Two main representation schemes have been proposed for ontology-based data: vertical and binary representations with a variant called hybrid. In these schemes, each instance is split into a number of tuples. In this paper, we propose a new representation of ontology-based data, called table per class. It consists in associating a table to each ontology class, where all property values of a class instance are represented in a same row. Columns of this table represent those properties of the ontology class that are associated with a value for at least one instance of this class. We present the architecture of our ontology-based databases and a comparison of the effectiveness of our representation scheme with the existing ones used in Semantic Web applications. Our benchmark involves three categories of queries: (1) targeted class queries, where users know the classes they are querying, (2) no targeted class queries, where users do not know the class(es) they are querying, and (3) update queries.},
address = {Berlin, Heidelberg},
author = {Dehainsala, Hondjack and Pierra, Guy and Bellatreche, Ladjel},
doi = {10.1007/978-3-540-71703-4},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dehainsala, Pierra, Bellatreche - 2007 - OntoDB an ontology-based database for data intensive applications(3).pdf:pdf},
isbn = {978-3-540-71702-7},
issn = {03029743},
journal = {Advances in Databases: Concepts, Systems and Applications, Lecture Notes in Computer Science, vol. 4443},
pages = {497--508},
publisher = {Springer Berlin Heidelberg},
title = {{OntoDB: an ontology-based database for data intensive applications}},
url = {http://link.springer.com/10.1007/978-3-540-71703-4{\_}43 http://dx.doi.org/10.1007/978-3-540-71703-4{\_}43},
year = {2007}
}
@article{Mason2015,
abstract = {Many species are extending their leading-edge (cool) range margins polewards in response to recent climate change.},
author = {Mason, Suzanna C. and Palmer, Georgina and Fox, Richard and Gillings, Simon and Hill, Jane K. and Thomas, Chris D. and Oliver, Tom H.},
doi = {10.1111/bij.12574},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mason et al. - 2015 - Geographical range margins of many taxonomic groups continue to shift polewards.pdf:pdf},
issn = {10958312},
journal = {Biological Journal of the Linnean Society},
keywords = {Climate change,Distributions,Invasions,Leading-edge,Trailing-edge},
month = {jul},
number = {3},
pages = {586--597},
title = {{Geographical range margins of many taxonomic groups continue to shift polewards}},
volume = {115},
year = {2015}
}
@inproceedings{Griss1994,
annote = {id: 1; isbn: print 0897916476; publication{\_}type: full{\_}text},
author = {Griss, Martin L and Wentzel, Kevin D},
doi = {10.1145/326619.326658},
isbn = {0897916476},
pages = {47 {\textless}last{\_}page{\textgreater} 52},
title = {{Proceedings of the 1994 ACM symposium on Applied computing - SAC '94; Hybrid domain-specific kits for a flexible software factory }},
url = {http://dx.doi.org/10.1145/326619.326658},
year = {1994}
}
@misc{Higgins2008,
address = {Chichester, UK},
annote = {Not downloaded.},
author = {Higgins, J P T and Green, S},
isbn = {978-0-470-51845-8},
keywords = {handbook,methods},
publisher = {Wiley-Blackwell},
title = {{Cochrane handbook for systematic reviews of interventions }},
year = {2008}
}
@article{Sutherland2006,
abstract = {1. Evidence-based policy requires researchers to provide the answers to ecological questions that are of interest to policy makers. To find out what those questions are in the UK, representatives from 28 organizations involved in policy, together with scientists from 10 academic institutions, were asked to generate a list of questions from their organizations. 2. During a 2-day workshop the initial list of 1003 questions generated from consulting at least 654 policy makers and academics was used as a basis for generating a short list of 100 questions of significant policy relevance. Short-listing was decided on the basis of the preferences of the representatives from the policy-led organizations. 3. The areas covered included most major issues of environmental concern in the UK, including agriculture, marine fisheries, climate change, ecosystem function and land management. 4. The most striking outcome was the preference for general questions rather than narrow ones. The reason is that policy is driven by broad issues rather than specific ones. In contrast, scientists are frequently best equipped to answer specific questions. This means that it may be necessary to extract the underpinning specific question before researchers can proceed. 5. Synthesis and applications. Greater communication between policy makers and scientists is required in order to ensure that applied ecologists are dealing with issues in a way that can feed into policy. It is particularly important that applied ecologists emphasize the generic value of their work wherever possible.},
annote = {id: 1; issn: print 00218901; issn: electronic 13652664; publication{\_}type: full{\_}text},
author = {Sutherland, William J. and Armstrong-Brown, Susan and Armsworth, Paul R. and Brereton, Tom and Brickland, Jonathan and Campbell, Colin D. and Chamberlain, Daniel E. and Cooke, Andrew I. and Dulvy, Nicholas K. and Dusic, Nicholas R. and Fitton, Martin and Freckleton, Robert P. and Godfray, H. Charles J and Grout, Nick and Harvey, H. John and Hedley, Colin and Hopkins, John J. and Kift, Neil B. and Kirby, Jeff and Kunin, William E. and Macdonald, David W. and Marker, Brian and Naura, Marc and Neale, Andrew R. and Oliver, Tom and Osborn, Dan and Pullin, Andrew S. and Shardlow, Matthew E A and Showler, David A. and Smith, Paul L. and Smithers, Richard J. and Solandt, Jean Luc and Spencer, Jonathan and Spray, Chris J. and Thomas, Chris D. and Thompson, Jim and Webb, Sarah E. and Yalden, Derek W. and Watkinson, Andrew R.},
doi = {10.1111/j.1365-2664.2006.01188.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sutherland et al. - 2006 - The identification of 100 ecological questions of high policy relevance in the UK.pdf:pdf},
isbn = {0021-8901},
issn = {00218901},
journal = {Journal of Applied Ecology},
keywords = {Agricultural reform,Biodiversity,Conservation,Fisheries,Land management,Restoration ecology},
number = {4},
pages = {617--627},
pmid = {1902},
title = {{The identification of 100 ecological questions of high policy relevance in the UK}},
url = {http://dx.doi.org/10.1111/j.1365-2664.2006.01188.x},
volume = {43},
year = {2006}
}
@misc{Ubuntu2015,
annote = {id: 1},
author = {Ubuntu},
number = {10/6/2015},
title = {{Ubuntu: An OS for PC, tablet, phone and cloud.}},
url = {http://www.ubuntu.com/},
volume = {2015},
year = {2015}
}
@book{Graber1877,
abstract = {1. Th. Der Organismus der Insekten.--2. Th. (Doppelband) Vergleichende Lebens- und Entwicklungsgeschichte der Insekten https://archive.org/details/dieinsekten020102grab/page/5},
author = {Graber, Vitus},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Graber - 1877 - Der Organismus der Insekten.pdf:pdf},
pages = {641},
publisher = {M{\"{u}}nchen, R. Oldenbourg},
title = {{Der Organismus der Insekten}},
year = {1877}
}
@inproceedings{DeMauro2016a,
abstract = {Purpose – This paper promises to shed light on the heterogeneous nature of the skills required to ‘win' with Big Data by analysing a large amount of job posts published online. More specifically we: 1) identify the most important ‘job families' related to Big Data; 2) recognize homogeneous groups of skills (skillsets) that are most sought after by companies; 3) characterize each job family with the appropriate level of competence required within each Big Data skillset. Design/methodology/approach – We implement a semi-automated, fully reproducible, analytical methodology that is able to cope with the significant amount of job posts obtained by scraping some of the most popular job search online portals. Job families are identified through the expert evaluation of the most important keywords appearing in job posts' titles. Skillsets are instead obtained by using Latent Dirichlet Allocation (LDA), an unsupervised machine learning algorithm used for text classification. Finally, we characterize the job families through a measure of the relative importance of each skillset. Originality/value – This study represents one of the first attempts to classify jobs in families and describe them in terms of skill requirements by means of a large-scale, semi- automated job post analysis, based on machine learning algorithms. To do so, we propose an original combination of various analytical techniques, which are widely established in previous scientific works. The characterization of job families through text mining and topic modelling techniques is innovative and can be reapplied to similar future studies focusing on any other professional field. Practical implications – This paper brings clarity to the multifaceted nature of Big Data competency requirements and job role types. Our results can concretely help business leaders and HR managers create clearer strategies for the procurement of the right skills needed to leverage Big Data at best. In addition, the structured classification of job families and skillsets will help establish a common language to be used within the job market, through which supply and demand can more effectively meet},
author = {{De Mauro}, Andrea and Greco, Marco and Grimaldi, Michele and Nobili, Giacomo},
booktitle = {International Forum on Knowledge Asset Dynamics 2016},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/De Mauro et al. - 2016 - Beyond Data Scientists a Review of Big Data Skills and Job Families.pdf:pdf},
isbn = {9788896687093},
keywords = {Big Data,Business Intelligence,Human Resrources Management},
pages = {1844--1857},
title = {{Beyond Data Scientists: a Review of Big Data Skills and Job Families}},
url = {https://s3.amazonaws.com/academia.edu.documents/47134851/Beyond{\_}Data{\_}Scientists{\_}-{\_}De{\_}Mauro{\_}{\_}Greco{\_}{\_}Grimaldi{\_}-{\_}FULL{\_}PAPER{\_}annotated.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A{\&}Expires=1531756029{\&}Signature={\%}2BvaOxJyj0uLRvnGyX40EWd9owPw{\%}3D{\&}response-content-dispo},
year = {2016}
}
@article{Fanani2017,
author = {Fanani, Lutfi and Priandani, Nurizal Dwi},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fanani, Priandani - 2017 - Data Cleaning and Prototyping Using K-Means to Enhance Classification Accuracy.pdf:pdf},
journal = {ripublication.com},
keywords = {Big Data,classification,clustering,data,data cleaning,data mining,it can reduce accuracy,noise,of data mining,outlier data so that,prototyping,regarded as misclassified,these errors will cause},
mendeley-tags = {Big Data},
number = {15},
pages = {5242--5247},
title = {{Data Cleaning and Prototyping Using K-Means to Enhance Classification Accuracy}},
url = {https://www.ripublication.com/ijaer17/ijaerv12n15{\_} (74).pdf},
volume = {12},
year = {2017}
}
@misc{Parker2014,
abstract = {This tutorial is not about making a beautiful, perfect R package. This tutorial is about creating a bare-minimum R package so that you don't have to keep thinking to yourself, “I really should just make an R package with these functions so I don't have to keep copy/pasting them like a goddamn luddite.” Seriously, it doesn't have to be about sharing your code (although that is an added benefit!). It is about saving yourself time. (n.b. this is my attitude about all reproducibility.)},
author = {Parker, Hillary},
title = {{Writing an R package from scratch | Not So Standard Deviations}},
url = {https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/},
urldate = {2018-01-26},
year = {2014}
}
@article{Atkin2017,
abstract = {BACKGROUND Large, heterogeneous datasets are required to enhance understanding of the multi-level influences on children's physical activity and sedentary behaviour. One route to achieving this is through the pooling and co-analysis of data from multiple studies. Where this approach is used, transparency of the methodology for data collation and harmonisation is essential to enable appropriate analysis and interpretation of the derived data. In this paper, we describe the acquisition, management and harmonisation of non-accelerometer data in a project to expand the International Children's Accelerometry Database (ICAD). METHOD Following a consultation process, ICAD partners were requested to share accelerometer data and information on selected behavioural, social, environmental and health-related constructs. All data were collated into a single repository for cataloguing and harmonisation. Harmonised variables were derived iteratively, with input from the ICAD investigators and a panel of invited experts. Extensive documentation, describing the source data and harmonisation procedure, was prepared and made available through the ICAD website. RESULTS Work to expand ICAD has increased the number of studies with longitudinal accelerometer data, and expanded the breadth of behavioural, social and environmental characteristics that can be used as exposure variables. A set of core harmonised variables, including parent education, ethnicity, school travel mode/duration and car ownership, were derived for use by the research community. Guidance documents and facilities to enable the creation of new harmonised variables were also devised and made available to ICAD users. An expanded ICAD database was made available in May 2017. CONCLUSION The project to expand ICAD further demonstrates the feasibility of pooling data on physical activity, sedentary behaviour and potential determinants from multiple studies. Key to this process is the rigorous conduct and reporting of retrospective data harmonisation, which is essential to the appropriate analysis and interpretation of derived data. These documents, made available through the ICAD website, may also serve as a guide to others undertaking similar projects.},
author = {Atkin, Andrew J and Biddle, Stuart J.H. and Broyles, Stephanie T and Chinapaw, Mai and Ekelund, Ulf and Esliger, Dale W and Hansen, Bjorge H and Kriemler, Susi and Puder, Jardena J and Sherar, Lauren B and van Sluijs, Esther M.F. and Andersen, L. B. and Anderssen, S. and Cardon, G. and Davey, R. and Hallal, P. and Janz, K. F. and M{\o}ller, N. and Molloy, L. and Page, A. and Pate, R. and Reilly, J. and Salmon, J. and Sardinha, L. B. and Timperio, A.},
doi = {10.1186/s12966-017-0631-7},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Atkin et al. - 2017 - Harmonising data on the correlates of physical activity and sedentary behaviour in young people Methods and lesson.pdf:pdf},
issn = {14795868},
journal = {International Journal of Behavioral Nutrition and Physical Activity},
keywords = {Adolescents,Children,Data pooling,ICAD,Physical activity,Retrospective harmonisation,Sedentary behaviour},
number = {1},
pages = {174},
pmid = {29262830},
publisher = {BioMed Central},
title = {{Harmonising data on the correlates of physical activity and sedentary behaviour in young people: Methods and lessons learnt from the international Children's Accelerometry database (ICAD)}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29262830 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5738842},
volume = {14},
year = {2017}
}
@article{Reinhart2010,
abstract = {In this paper, we exploit a new multi-country historical dataset on public (government) debt to search for a systemic relationship between high public debt levels, growth and inflation. 1 Our main result is that whereas the link between growth and debt seems relatively weak at " nor-mal " debt levels, median growth rates for coun-tries with public debt over roughly 90 percent of GDP are about one percent lower than other-wise; average (mean) growth rates are several percent lower. Surprisingly, the relationship between public debt and growth is remarkably similar across emerging markets and advanced economies. This is not the case for inflation. We find no systematic relationship between high debt levels and inflation for advanced econo-mies as a group (albeit with individual country exceptions including the United States). By con-trast, in emerging market countries, high public debt levels coincide with higher inflation. Our topic would seem to be a timely one. Public debt has been soaring in the wake of the recent global financial maelstrom, especially in the epicenter countries. This should not be sur-prising, given the experience of earlier severe financial crises. 2 Outsized deficits and epic bank bailouts may be useful in fighting a downturn, but what is the long-run macroeconomic impact, 1 In this paper " public debt " refers to gross central government debt. " Domestic public debt " is government debt issued under domestic legal jurisdiction. Public debt does not include debts carrying a government guarantee. Total gross external debt includes the external debts of all branches of government as well as private debt that is issued by domestic private entities under a foreign jurisdiction. 2 Reinhart and Rogoff (2009a, b) demonstrate that the aftermath of a deep financial crisis typically involves a protracted period of macroeconomic adjustment, particu-larly in employment and housing prices. On average, public debt rose by more than 80 percent within three years after a crisis.},
author = {Reinhart, Carmen and Rogoff, Kenneth S.},
doi = {10.1257/aer.100.2.573},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Reinhart, Rogoff - 2010 - Growth in a Time of Debt.pdf:pdf},
journal = {American Economic Review: Papers {\&} Proceedings},
pages = {573--578},
title = {{Growth in a Time of Debt}},
url = {http://www.aeaweb.org/articles.php?doi=10.1257/aer.100.2.573},
volume = {100},
year = {2010}
}
@article{Shneiderman2015,
abstract = {The growing volumes of data available from sensors, social media sources, Web logs, and medical histories present remarkable opportunities for researchers and policy analysts.1 Although big data resources can provide valuable insights to help us understand complex systems and lead to better decisions for business, national security, cybersecurity, and healthcare, there are many challenges to dealing with the volume and variety of data.},
author = {Shneiderman, Ben and Plaisant, Catherine},
doi = {10.1109/MCG.2015.64},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shneiderman, Plaisant - 2015 - Sharpening Analytic Focus to Cope with Big Data Volume and Variety.pdf:pdf},
issn = {0272-1716},
journal = {IEEE Computer Graphics and Applications},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {may},
number = {3},
pages = {10--14},
title = {{Sharpening Analytic Focus to Cope with Big Data Volume and Variety}},
url = {http://ieeexplore.ieee.org/document/7111924/},
volume = {35},
year = {2015}
}
@misc{Ognyanova2016,
author = {Ognyanova, Katherine},
booktitle = {NetSciX 2016 School of Code Workshop, Wroclaw, Poland},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ognyanova - 2016 - Network Analysis and Visualization with R and igraph.pdf:pdf},
pages = {1--64},
title = {{Network Analysis and Visualization with R and igraph}},
url = {http://www.kateto.net/wp-content/uploads/2016/01/NetSciX{\_}2016{\_}Workshop.pdf},
urldate = {2018-02-13},
year = {2016}
}
@book{Keim2010,
abstract = {We are living in a world which faces a rapidly increasing amount of data to be dealt with on a daily basis. In the last decade, the steady improvement of data storage devices and means to create and collect data along the way, influenced the manner in which we deal with information. Most of the time, data is stored without filtering and refinement for later use. Virtually every branch of industry or business, and any political or personal activity, nowadays generates vast amounts of data. Making matters worse, the possibilities to collect and store data increase at a faster rate than our ability to use it for making decisions. However, in most applications, raw data has no value in itself; instead, we want to extract the information contained in it.},
address = {Goslar, Germany},
author = {Keim, D. and Kohlhammer, J. and Ellis, G. and Mansmann, F.},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Keim et al. - 2010 - Mastering The Information Age – Solving Problems with Visual Analytics.pdf:pdf},
isbn = {978-3-905673-77-7},
pages = {182},
publisher = {Eurographics Association},
title = {{Mastering The Information Age – Solving Problems with Visual Analytics}},
url = {http://diglib.eg.org/},
year = {2010}
}
@article{Mosterman2015,
abstract = {Embedding computing power in a physical environment has provided the functional flexibility and performance necessary in modern products such as automobiles, aircraft, smartphones, and more. As product features came to increasingly rely on software, a network infrastructure helped factor out common hardware and offered sharing functionality for further innovation. A logical consequence was the need for system integration. Even in the case of a single original end manufacturer who is responsible for the final product, system integration is quite a challenge. More recently, there have been systems coming online that must perform system integration even after deployment--that is, during operation. This has given rise to the cyber-physical systems (CPS) paradigm. In this paper, select key enablers for a new type of system integration are discussed. The needs and challenges for designing and operating CPS are identified along with corresponding technologies to address the challenges and their potential impact. The intent is to contribute to a model-based research agenda in terms of design methods, implementation technologies, and organization challenges necessary to bring the next-generation systems online.},
annote = {Paper in Road2CPS.

Quote that captures the essence of Maier's definition:
"While in traditional networked embedded systems, one OEM would be responsible for system integration prior to deployment, CPS do not integrate until after deployment ... where the actual implementation of some of the system platforms, components, and transducers is not yet available or may change at typical system integration time. Moreover, some of the physical and network fabric may be shared with other systems—the extent, behavior, and performance of which is still unknown."

Another quote about the problems:
"For all the success that networked embedded systems can lay claim to, the corresponding computational complexity has introduced system integration issues that have been a continuous source of complications and even missed delivery deadlines (e.g., [61]). While in a traditional engineering paradigm, the OEM is responsible for the system integration, a cyber-physical systems (CPS) paradigm is transforming this convention. In CPS, the system integration occurs post-deployment during operation of the system. Given the difficulty in system integration during the system development phases, it is no surprise that post-deployment system integration is an engineering problem fraught with extreme complications such as feature interaction, multirate distributed architectures, and collaborative control."
61. Sullivan, M.: Joint Strike Fighter-progress made and challenges remain. Technical report GAO-07-360, United States Government Accountability Office, March (2007)p6 CPS definition from NIST:
"S. Shyam Sunder, Director of the Engineering Laboratory at the National Institute of Standards and Technology provided the following definition of CPS on March 13, 2012:

“Cyber physical systems are hybrid networked cyber and engineered physical elements co-designed to create adaptive and predictive systems for enhanced per- formance. Performance metrics include safety and security, reliability, agility and stability, efficiency and sustainability, privacy.” "

p7 Problem statement:
"In particular, designing for new and unknown system functionalities is highlighted. Such functionalities, which become required or active not sooner than during the deployment of the system, challenge the traditional approach to system engineering. Novel system engineering approaches become a necessity, which in turn relies heavily on enabling system architectures and powerful design-time approaches to conceptualize system behavior at various levels of detail in different parts of the system [59]."
59. Steering Committee for Foundations in Innovation for Cyber- Physical Systems. Foundations for innovation: strategic oppor- tunities for the 21st century cyber-physical systems-connecting computer and information systems with the physical world. Techni- cal report, National Institute of Standards and Technology (NIST), March 2013p7 Useful example of a CPS:
"For example, in an automobile, a feature to automatically open and close windows may now interact with a feature that centrally locks doors. There may even be further interaction with a feature that detects a collision. Such interaction allows the windows to automatically close when the doors are locked, as well as the doors to automatically unlock and the windows to open when a collision is detected."

p8 The point of abstractions:
"Key to enabling the analysis at the systems level is support for abstractions. This support makes the level of detail manageable. Model-based design [40,43,47] centers around abstractions by using computational models with exe- cutable semantics. A simulation engine enables executing the computational models and performing much of the system integration effort at a high level of abstraction. Moreover, support for model elaboration allows gradually adding detail to the models to the point where actual implementations may be made part of the overall system model. These practices are known as software-in-the-loop, processor-in-the-loop, and hardware-in-the-loop configurations [37]. A valuable addi- tional benefit derived from having executable semantics is that either a hardware description or a software implementa- tion can be generated automatically from the models [11].
[Note the top-down perspective here. Presumably they imply that bottom-up was captured in the requirements process]List of technology gaps (bullets inserted {\&} text reduced):
• Of particular importance is the need for a consistent, comprehensive, and accurate simulation- based approach to quality assurance methods. 
Pertinent advances in technology include 
• property-based automated selection of model detail [13, 32, 60], 
• counterexample-guided refinement [9], 
• requirements guided selection of abstractions [21]. 
• computer automated multiparadigm modeling [39] 
• combining dynamic semantics [33] and execution semantics [36]. 
• selective design refinement calls for connectivity with implementation models (e.g., software-in-the-loop) or systems (e.g., hardware-in-the-loop), which relates to developments in real-time simulation [50].
Run-time system adaptation, which closely reflects the CPS paradigm. 
• to adapt the system, it is necessary but challenging to maintain the state of the current ensemble of subsystems, to be able to introspect and reason about potential changes, and to find an optimal configuration in the face of available resources. Work on the use of models@run.time [7] provides runtime supervision and reconfiguration technologies that serve as valuable starting points.
• simulations of the run-time configuration of the CPS to test the functionality @ runtime
• the design of emergent hehaviour, when localised planning is assembled into a global configuration for the CPS. A starting point is based on the globallyasynchronous/locally synchronous (GALS) paradigm (e.g., [31]) to the system of systems level. Further, advances in multi-agent systems [64] have developed an important and valuable body of work, while formal methods for design refinement have been illustrated in the context of Conway's game of life [54]
• data-sharing, when data may have incongruous sources that operate concurrently and at multiple rates. While traditionally synchronization of data streams is the responsibility of system integration, along with manifold further challenges [45], it now must apply at runtime and in real time. A step in this direction is the RoboEarth network where robots generate, share, and reuse data across Internet connections [65]
• Functionality sharing; A step in this direction is the RoboEarth network where robots generate, share, and reuse data across Internet connections [65]
• Collaborative functionality testing; because CPS operate in a physical world, safety mechanisms and fault mitigation strategies (also called fallback strategies) must be designed into the system. However, functionality that becomes dynam- ically available makes current testing approaches (e.g., to manage system variability [2]) challenging to apply. In addition, test behavior including the test evaluation must be reproducible and, if possible, automated. This demand is a particular challenge in a distributed system configuration as effective testing requires the initial state and test input to be set deterministically.
• design artifact sharing is an essential need. In the more general context of collaborating along the system lifecycle, the industrial Framework for Embedded Systems Tools (iFest) builds a services platform for tool coupling that is based on the Open Services for Lifecycle Collaboration (OSLC) infrastructure [56].
• precise timing and synchronization between collaborating systems must be tackled to enable the real-time and embedded nature. Technologies such as the precision time protocol IEEE 1588 [10] and support for combined periodic/nonperiodic events [67] may be adapted to integrate into the network fabric. See also the CIF3 language @ TU Eindhoven
• Hardware availability; the embedded software of available choices of hardware may have different behaviours (e.g., overflow handling, instruction fetch, etc.), requiring a good understanding of hardware semantics. Furthermore, if the hardware is safety-assured, there may be other restrictions involved for the whole CPS ensemble to be safe.
• development of service communications: developments in real-time service oriented architectures (SOA) [16, 63] include real-time OSGi using Java [6]. Devel- opments in real-time middleware (for a survey see [49]) include a real-time version of the High-Level Architecture (HLA) [30], a real-time Common Object Request Broker Architecture (CORBA) (e.g., [25]), and the Data Distribu- tion Service (DDS) [55]. Also, the robot operating system (ROS) [52] is of interest but currently lacking real-time capabilities. A challenging element here is discovering the functionality provided by services along with constraints such as real-time, memory, availability, requested guaran- tees, cost, and others [18] under which their utilization is possible. Note that the discovery must oftentimes be real- time in its own right.
• development of appropriate semantics: In related work, a service is associated with at least one semantic descrip- tion in an Ontology Web Language (OWL) [19] used for describing semantic web services [58]. Services that pro- vide different functionality, but that can still be applied or that can be orchestrated to provide the necessary func- tionality, must be discoverable.
• interoperation: support for an open paradigm requires a well-defined and strict layering in the technology stack so as to enable including distinctly separate elements. Moreover, standards and interfaces are critical to addressing the specific implementation effects of the computing and communication platform. These implementation effects include, for example, finite precision quantization effects, saturation effects induced by fixed- point integers, scheduling effects, and delays caused by the scheduling of tasks.
• run-time certification (e.g. of safety-critical systems). This needs new, simulation-based approaches},
author = {Mosterman, P J and Zander, J},
doi = {10.1007/s10270-015-0469-x},
journal = {Software and Systems Modeling},
keywords = {Cyber-physical systems,IoT,JIT,Work flow management,agents,allocation of functions,architecture,automation,certification:,communications,complexity,control,decision-making,definitions,engineering,federated control,information systems,interface design,just in time,metrics/metrication,models,network-centric warfare,networks,production control,quotes,reliability,safety,software,standards,strategic planning,strategy,supply chain,systems,telecoms},
number = {1},
pages = {5--16},
title = {{Cyber-physical systems challenges: a needs analysis for collaborating embedded software systems}},
volume = {15},
year = {2015}
}
@article{Alsheikh-Ali2011,
author = {Alsheikh-Ali, Alawi A. and Qureshi, Waqas and Al-Mallah, Mouaz H. and Ioannidis, John P. A.},
doi = {10.1371/journal.pone.0024357},
editor = {Boutron, Isabelle},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Alsheikh-Ali et al. - 2011 - Public Availability of Published Research Data in High-Impact Journals.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
month = {sep},
number = {9},
pages = {e24357},
title = {{Public Availability of Published Research Data in High-Impact Journals}},
url = {http://dx.plos.org/10.1371/journal.pone.0024357},
volume = {6},
year = {2011}
}
@article{Kaplan2008,
abstract = {We apply a cognitive lens to understanding technology trajectories across the life cycle by developing a co-evolutionary model of technological frames and technology. Applying that model to each stage of the technology life cycle, we identify conditions under which a cognitive lens might change the expected technological outcome predicted by purely economic or organizational models. We also show that interactions of producers, users and institutions shape the development of collective frames around the meaning of new technologies. We thus deepen our understanding of sources of variation in the era of ferment, conditions under which a dominant design may be achieved, the underlying architecture of the era of incremental change and the dynamics associated with discontinuities.},
author = {Kaplan, Sarah and Tripsas, Mary},
doi = {http://dx.doi.org/10.1016/j.respol.2008.02.002},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kaplan, Tripsas - 2008 - Thinking about technology Applying a cognitive lens to technical change.pdf:pdf},
issn = {0048-7333},
journal = {Research Policy},
keywords = {Cognition,Technical change,Technological frames,Technology life cycle,Technology trajectories},
month = {jun},
number = {5},
pages = {790--805},
title = {{Thinking about technology: Applying a cognitive lens to technical change}},
url = {http://www.sciencedirect.com/science/article/pii/S004873330800036X},
volume = {37},
year = {2008}
}
@article{RITCHEY2014,
abstract = {Models and modelling methods play an essential role in Operational Research and Management Science (OR/MS). This article presents four models which concern how OR/MS employs different modelling methods for different modelling tasks, under different constraints, and for different forms of uncertainty. Two of these “meta- models” concern how OR/MS modelling has been employed in decision support for the Swedish Defence Research Agency: one of them from a more academic or theoretical perspective, the other more from the perspective of the practitioner. The third model concentrates on how different modelling techniques are constrained by varying stake- holder positions. The final model is introspective and classifies a variety of modelling methods on the basis of a number of formal modelling properties. All of these meta-models were developed using the non-quantified model- ling method GeneralMorphological Analysis (GMA).},
author = {RITCHEY, T},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/RITCHEY - 2014 - Four Models about Decision Support Modeling.pdf:pdf},
issn = {2001-2241},
journal = {Acta Morphologica Generalis AMG Vol},
keywords = {decision,decision support,general morphological analysis,management science,modelling theory,operational research,science},
number = {1},
pages = {1--15},
title = {{Four Models about Decision Support Modeling}},
url = {http://www.swemorph.com/pdf/psm-gma.pdf. http://www.amg.swemorph.com/pdf/amg-3-1-2014.pdf},
volume = {3},
year = {2014}
}
@misc{Verity2018,
abstract = {A quarter of property in England and Wales owned by overseas firms is held by entities registered in the British Virgin Islands, BBC analysis has found. The Caribbean archipelago is the official home of companies that own 23,000 properties - more than any other country. Notes: The BBC analysed the January 2018 Overseas Companies Ownership data made public by the HM Land Registry. The data is accurate up to January 2018 and contains around 97,000 title records of freehold and leasehold property in England and Wales, registered to companies incorporated outside the UK. The map shows 71,000 of the 97,000 addresses. Those missing had incomplete data.},
author = {Verity, Andy and Stylianou, Nassos},
keywords = {Big Data,CARTO},
mendeley-tags = {Big Data,CARTO},
title = {{Firms on Caribbean island chain own 23,000 UK properties - BBC News}},
url = {http://www.bbc.co.uk/news/business-42666274},
urldate = {2018-03-01},
year = {2018}
}
@article{Suthaharan2014,
abstract = {This paper focuses on the specific problem of Big Data classification of network intrusion traffic. It discusses the system challenges presented by the Big Data problems associated with network intrusion prediction. The prediction of a possible intrusion attack in a network requires continuous collection of traffic data and learning of their characteristics on the fly. The continuous collection of traffic data by the network leads to Big Data problems that are caused by the volume, variety and velocity properties of Big Data. The learning of the network characteristics requires machine learning techniques that capture global knowledge of the traffic patterns. The Big Data properties will lead to significant system challenges to implement machine learning frameworks. This paper discusses the problems and challenges in handling Big Data classification using geometric representation-learning techniques and the modern Big Data networking technologies. In particular this paper discusses the issues related to combining supervised learning techniques, representation-learning techniques, machine lifelong learning techniques and Big Data technologies (e.g. Hadoop, Hive and Cloud) for solving network traffic classification problem},
author = {Suthaharan, Shan},
doi = {10.1145/2627534.2627557},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Suthaharan - 2014 - Big Data Classification Problems and Challenges in Network Intrusion Prediction with Machine Learning.pdf:pdf},
isbn = {0163-5999},
issn = {0163-5999},
journal = {Performance Evaluation Review},
keywords = {big data,hadoop distributed file systems,intrusion detec-,machine learning,tion},
month = {apr},
number = {4},
pages = {70--73},
publisher = {ACM},
title = {{Big Data Classification : Problems and Challenges in Network Intrusion Prediction with Machine Learning}},
url = {http://dl.acm.org/citation.cfm?doid=2627534.2627557},
volume = {41},
year = {2014}
}
@misc{Nakamoto2008,
abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
annote = {Document in Contracts. This is the originating source for the Distributed ledger. Included here for quoting purposes.

The idea of Bitcoin was first introduced in 2008, when a man named ‘Satoshi Nakamoto‘ published a paper on The Cryptography Mailing list at metzdowd.com. 

http://www.metzdowd.com/mailman/listinfo/cryptography},
author = {Nakamoto, S},
keywords = {contracts},
title = {{Bitcoin:  a peer-to-peer electronic cash system}},
url = {https://bitcoin.org/md5sum d56d71ecadf2137be09d8b1d35c6c042},
year = {2008}
}
@techreport{Ognyanova2018,
abstract = {Workshop summary from PolNet 2018},
address = {Washington, DC},
author = {Ognyanova, Katherine},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ognyanova - 2018 - Network visualization with R.pdf:pdf},
institution = {Rutgers University},
pages = {61},
title = {{Network visualization with R}},
url = {http://kateto.net/network-visualization},
year = {2018}
}
@article{Shapira2011,
abstract = {Is "the field of management's devotion to theory too much of a good thing?" [Hambrick, D. C. 2007. The field of management's devotion to theory: Too much of a good thing? Acad. Management J. 50(6) 1346-1352]. In his paper, Hambrick criticizes the practice employed by many journals in the management field that requires that papers submitted for publication make a strong theoretical contribution. I argue that part of the problem is caused by the misunderstanding and misuse of the term "theory." To clarify the status of theory, I review three modes of research formulation in the organizational sciences: theories, models, and conceptual frameworks. Language plays an important role in scientific research. I therefore discuss two research languages that are used in research in management that appear to be the farthest apart: mathematics, which is the language of precision; and narratives, which is the language that provides rich data. I provide a discussion of the use of mathematics in theory development and the use of narratives in research development. The two languages and three modes of research formulation are needed for contribution to knowledge, which should be the main goal of research in organization science. {\textcopyright}2011 INFORMS.},
author = {Shapira, Zur},
doi = {10.1287/orsc.1100.0636},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shapira - 2011 - I've got a theory paper-do you Conceptual, empirical, and theoretical contributions to knowledge in the organizational.pdf:pdf},
issn = {10477039},
journal = {Organization Science},
keywords = {Mathematics,Model,Narratives,Theory},
month = {sep},
number = {5},
pages = {1312--1321},
title = {{"I've got a theory paper-do you?": Conceptual, empirical, and theoretical contributions to knowledge in the organizational sciences}},
volume = {22},
year = {2011}
}
@misc{GnuPG2013,
abstract = {GnuPG.org - GNU Privacy Guard Home Page},
author = {GnuPG},
month = {jul},
number = {April},
publisher = {The GnuPG Project},
title = {{The GNU Privacy Guard}},
url = {https://gnupg.org/},
urldate = {2018-09-04},
year = {2013}
}
@article{UKGovernment2006,
abstract = {An Act to make provision about bodies concerned with the natural environment and rural communities; to make provision in connection with wildlife, sites of special scientific interest, National Parks and the Broads; to amend the law relating to rights of way; to make provision as to the Inland Waterways Amenity Advisory Council; to provide for flexible administrative arrangements in connection with functions relating to the environment and rural affairs and certain other functions; and for connected purposes.},
author = {{UK Government}},
doi = {10.1350/enlr.2006.8.4.292},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/UK Government - 2006 - The Natural Environment and Rural Communities Act 2006.pdf:pdf;:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/UK Government - 2006 - The Natural Environment and Rural Communities Act 2006(2).pdf:pdf},
issn = {1461-4529},
journal = {Environmental Law Review},
number = {4},
pages = {292--298},
publisher = {Statute Law Database},
title = {{The Natural Environment and Rural Communities Act 2006}},
url = {http://www.legislation.gov.uk/ukpga/2006/16/section/40},
volume = {8},
year = {2006}
}
@article{Sugiyama1981,
abstract = {Two kinds of new methods are developed to obtain effective representations of hierarchies automatically: theoretical and heuristic methods. The methods determine the positions of vertices in two steps. First the order of the vertices in each level is determined to reduce the number of crossings of edges. Then horizontal positions of the vertices are determined to improve further the readability of drawings. The theoretical methods are useful in recognizing the nature of the problem, and the heuristic methods make it possible to enlarge the size of hierarchies with which we can deal. Performance tests of the heuristic methods and several applications are presented. Copyright {\textcopyright} 1981 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Sugiyama, Kozo and Tagawa, Shojiro and Toda, Mitsuhiko},
doi = {10.1109/TSMC.1981.4308636},
issn = {21682909},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {2},
pages = {109--125},
title = {{Methods for Visual Understanding of Hierarchical System Structures}},
volume = {11},
year = {1981}
}
@article{Dickinson2010,
abstract = {Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global ?experiments? altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across space and time.},
author = {Dickinson, Janis L. and Zuckerberg, Benjamin and Bonter, David N.},
doi = {10.1146/annurev-ecolsys-102209-144636},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dickinson, Zuckerberg, Bonter - 2010 - Citizen Science as an Ecological Research Tool Challenges and Benefits.pdf:pdf},
issn = {1543-592X},
journal = {Annual Review of Ecology, Evolution, and Systematics},
month = {dec},
number = {1},
pages = {149--172},
publisher = {Annual Reviews},
title = {{Citizen Science as an Ecological Research Tool: Challenges and Benefits}},
volume = {41},
year = {2010}
}
@book{Sedgwick2000,
abstract = {3rd ed. Introduction to Statistical Testing 2 -- Examples of Test Procedures 5 -- Classification of Tests 19.},
author = {Sedgwick, Philip},
booktitle = {Statistical Methods in Medical Research},
doi = {10.1177/096228020000900509},
isbn = {9788178297316},
issn = {09622802},
number = {5},
pages = {519--520},
publisher = {Sage Publications},
title = {100 statistical tests},
volume = {9},
year = {2000}
}
@article{Lin2012,
abstract = {The success of a virtual organisation's (VO) operations is vitally dependent on interoperability and close collaboration between allied organisations and enterprises. This paper proposes a semantic web rule-based knowledge system applied to collaboration moderator service (CMS) to enable semantic integration of heterogeneous data for the moderation of shared knowledge among cooperative VOs. The proposed approach also shows how the new generation of the CMS supports collaborative working by: enhancing interoperability across different enterprise information systems, monitoring the activities of partners within the collaboration to identify opportunities for enhanced collaboration and raising awareness of potential misunderstandings or conflicts between partner strategies or decisions. Therefore, the semantic knowledge systems of CMS can function on a platform that supports interoperability and collaborative activities within VOs. A case study is conducted to illustrate the feasibility of the proposed model and the test results confirm its suitability. {\textcopyright} 2012 Taylor {\&} Francis.},
author = {Lin, H. K. and Harding, J. A. and Tsai, W. C.},
doi = {10.1080/00207543.2010.544074},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Harding, Tsai - 2012 - A rule-based knowledge system on semantic web for collaboration moderator services.pdf:pdf},
issn = {00207543},
journal = {International Journal of Production Research},
keywords = {Java expert system shell (JESS),collaboration moderator service,knowledge based systems,semantic web rule language (SWRL)},
month = {feb},
number = {3},
pages = {805--816},
title = {{A rule-based knowledge system on semantic web for collaboration moderator services}},
volume = {50},
year = {2012}
}
@article{webb2009packagingthermoplastics,
author = {Webb, D P and Hutt, D A and Hopkinson, N and Conway, P P and Palmer, P J},
doi = {10.1109/JMEMS.2009.2013404},
issn = {1057-7157},
journal = {Journal of Microelectromechanical Systems},
number = {(2)},
pages = {. 354----362},
title = {{Packaging of microfluidic devices for fluid interconnection using thermoplastics}},
url = {http://dx.doi.org/10.1109/JMEMS.2009.2013404},
volume = {18},
year = {2009}
}
@article{Simonsen2001,
abstract = {The ultrastructure of the dorsal forewing vestiture in exemplars of all family group taxa of non-ditrysian Lepidoptera is examined, and the evolutionary implications at family level and above are discussed. Wing-scale terminology is reviewed. Three different types of bilayer wing-scale covering are recognized; only a few groups have a single-layer wing-scale covering. The general scale arrangement is random, but a few taxa have clustered scale arrangements and scattered heteroneurans have scales arranged in transverse rows. Cross ribs are present in all taxa, but only as vestiges in eriocraniid cover scales. Ridge dimorphism is widespread in Neolepidoptera. Surprisingly, ridges and cross ribs on the adwing scale surface are of general occurrence in Neopseustidae and Hepialidae, and are even found on parts of the ground scales of many other Neolepidoptera. Morphological evidence strongly indicates that the fused wing-scale types found in non-Coelolepidan Lepidoptera and Neolepidoptera are independently evolved, as evidenced from the presence of vestigial perforations. Absence of perforations is not infallible evidence that a scale is solid. Microtrichia are independently reduced in a number of taxa and probably re-evolved in at least higher nepticulids. Wing vestiture and scale characters indicate that Tischerioidea may be the sister group of Ditrysia.},
author = {Simonsen, Thomas J.},
doi = {10.1046/j.1463-6395.2001.00089.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Simonsen - 2001 - The wing vestiture of the non-ditrysian Lepidoptera (Insecta). Comparative morphology and phylogenetic implications.pdf:pdf},
isbn = {0001-7272},
issn = {00017272},
journal = {Acta Zoologica},
keywords = {Groundplan,Herring bone,Microtrichia,Non-dirtysian lepidoptera,Ridges,Scale lumen,Wing scales},
month = {sep},
number = {4},
pages = {275--298},
publisher = {Wiley/Blackwell (10.1111)},
title = {{The wing vestiture of the non-ditrysian Lepidoptera (Insecta). Comparative morphology and phylogenetic implications}},
url = {http://doi.wiley.com/10.1046/j.1463-6395.2001.00089.x},
volume = {82},
year = {2001}
}
@misc{BSI-20162016,
address = {London, UK},
author = {BSI-2016},
doi = {ISBN  978 0 580 89530 2},
keywords = {IoT,agility,assembly,automation,ergonomics,ethics,functions,holons,product substitution,robots,safety,socio-tech,standards},
publisher = {British Standards Institution},
title = {{BS 8611:2016  Robots and robotic devices:  guide to the ethical design and application of robots and robotic systems}},
year = {2016}
}
@article{Perkel2015,
abstract = {A powerful programming language with huge community support.},
author = {Perkel, Jeffrey M.},
doi = {10.1038/518125a},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Perkel - 2015 - Programming Pick up Python.pdf:pdf},
isbn = {9781461225447},
issn = {14764687},
journal = {Nature},
month = {feb},
number = {7537},
pages = {125--126},
pmid = {25798104},
title = {{Programming: Pick up Python}},
url = {http://www.nature.com/doifinder/10.1038/518125a},
volume = {518},
year = {2015}
}
@article{Hulme2014,
abstract = {A widely recognized challenge in applied ecology is the gap between the knowledge generated by scientists and uptake by practitioners. Bridging this gap requires reciprocal and iterative flows of information from both scientists and practitioners prior to research initiation and beyond its completion. Yet current approaches to knowledge exchange ignore the complexity of translating different types of knowledge and the constraints that might limit effective knowledge exchange. Knowing who might use a particular piece of research is the first step when developing projects that might be of value to practitioners, but different types of research often can have quite different audiences. Identifying the precise target for research outputs, whether practitioners, stakeholders or end-users, is essential for successful knowledge exchange, and outputs must be tailored to the knowledge needs of the intended recipients. The scope of many leading applied ecology journals targets use-inspired basic research that aims to develop a theoretical or fundamental basis to support interventions, technologies and policies that lead to improved applied outcomes. This more conceptual approach, while essential to the development of future management applications, is probably not what most practitioners require. In contrast to the explicit knowledge generated by scientists, many practitioners apply their own tacit knowledge when making decisions regarding their conservation goals and interventions. Such knowledge is intuitive, largely experience based and hard to define. As a result is often context dependent and personal in nature. The failure of scientists to translate and consider tacit knowledge may be behind the lack of implementation of their research. Additional challenges to implementation include the continuing interest and relevance of use-inspired basic research, lack of consensus among researchers regarding management options and the need for scientists to remain independent brokers of intervention options rather than conservation advocates. Synthesis and applications. Publishing research in peer-reviewed journals will only ever be a small part of closing the knowing–doing gap. Increasingly, conservation organizations, such as NGOs and charities that employ their own scientists, steward their own protected areas, and build long-term partnerships with stakeholders will be central to implementing applied ecological science},
annote = {id: 1; issn: print 00218901; publication{\_}type: full{\_}text},
author = {Hulme, Philip E},
doi = {10.1111/1365-2664.12321},
isbn = {0021-8901},
issn = {13652664},
journal = {Journal of Applied Ecology},
keywords = {Advocacy,Biodiversity,Conservation,Consultancy,Decision-making,Ecosystem services,Partnership,Policy,Problem-solving,Restoration},
number = {5},
pages = {1131--1136},
title = {{Bridging the knowing-doing gap: Know-who, know-what, know-why, know-how and know-when}},
url = {http://dx.doi.org/10.1111/1365-2664.12321},
volume = {51},
year = {2014}
}
@article{Rebane1988,
abstract = {Poly-trees are singly connected causal networks in which variables may arise from multiple causes. This paper develops a method of recovering ply-trees from empirically measured probability distributions of pairs of variables. The method guarantees that, if the measured distributions are generated by a causal process structured as a ply-tree then the topological structure of such tree can be recovered precisely and, in addition, the causal directionality of the branches can be determined up to the maximum extent possible. The method also pinpoints the minimum (if any) external semantics required to determine the causal relationships among the variables considered.},
archivePrefix = {arXiv},
arxivId = {1304.2736},
author = {Rebane, George and Pearl, Judea},
doi = {10.1016/0888-613x(88)90158-2},
eprint = {1304.2736},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Rebane, Pearl - 1988 - The recovery of causal poly-trees from statistical data.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
month = {mar},
number = {3},
pages = {341},
title = {{The recovery of causal poly-trees from statistical data}},
url = {http://arxiv.org/abs/1304.2736},
volume = {2},
year = {1988}
}
@article{Pautasso2013,
abstract = {Literature reviews are in great demand in most scientific fields. Their need stems from the ever-increasing output of scientific publications [1]. For example, compared to 1991, in 2008 three, eight, and forty times more papers were indexed in Web of Science on malaria, obesity, and biodiversity, respectively [2]. Given such mountains of papers, scientists cannot be expected to examine in detail every single new paper relevant to their interests [3]. Thus, it is both advantageous and necessary to rely on regular summaries of the recent literature. Although recognition for scientists mainly comes from primary research, timely literature reviews can lead to new synthetic insights and are often widely read [4]. For such summaries to be useful, however, they need to be compiled in a professional way [5]. When starting from scratch, reviewing the literature can require a titanic amount of work. That is why researchers who have spent their career working on a certain research issue are in a perfect position to review that literature. Some graduate schools are now offering courses in reviewing the literature, given that most research students start their project by producing an overview of what has already been done on their research issue [6]. However, it is likely that most scientists have not thought in detail about how to approach and carry out a literature review. Reviewing the literature requires the ability to juggle multiple tasks, from finding and evaluating relevant material to synthesising information from various sources, from critical thinking to paraphrasing, evaluating, and citation skills [7]. In this contribution, I share ten simple rules I learned working on about 25 literature reviews as a PhD and postdoctoral student. Ideas and insights also come from discussions with coauthors and colleagues, as well as feedback from reviewers and editors.},
archivePrefix = {arXiv},
arxivId = {1511.05078},
author = {Pautasso, Marco},
doi = {10.1371/journal.pcbi.1003149},
eprint = {1511.05078},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pautasso - 2013 - Ten Simple Rules for Writing a Literature Review.pdf:pdf},
isbn = {1553-7358 (Electronic) 1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {7},
pages = {4},
pmid = {23874189},
publisher = {Public Library of Science},
title = {{Ten Simple Rules for Writing a Literature Review}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23874189 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3715443},
volume = {9},
year = {2013}
}
@book{Fox2016,
author = {Fox, John},
booktitle = {Using the R Commander: A Point-and-Click Interface for R},
doi = {10.1201/9781315380537},
isbn = {9781498741910},
issn = {1548-7660},
month = {sep},
pages = {1--219},
publisher = {Chapman and Hall/CRC},
title = {{Using the R commander: A point-and-click interface for R}},
url = {https://www.taylorfrancis.com/books/9781315380537},
year = {2016}
}
@article{Weissgerber2015,
abstract = {Figures in scientific publications are critically important because they often show the data supporting key findings. Our systematic review of research articles published in top physiology journals (n = 703) suggests that, as scientists, we urgently need to change our practices for presenting continuous data in small sample size studies. Papers rarely included scatterplots, box plots, and histograms that allow readers to critically evaluate continuous data. Most papers presented continuous data in bar and line graphs. This is problematic, as many different data distributions can lead to the same bar or line graph. The full data may suggest different conclusions from the summary statistics. We recommend training investigators in data presentation, encouraging a more complete presentation of data, and changing journal editorial policies. Investigators can quickly make univariate scatterplots for small sample size studies using our Excel templates.},
author = {Weissgerber, Tracey L. and Milic, Natasa M. and Winham, Stacey J. and Garovic, Vesna D.},
doi = {10.1371/journal.pbio.1002128},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Weissgerber et al. - 2015 - Beyond Bar and Line Graphs Time for a New Data Presentation Paradigm.pdf:pdf},
issn = {15457885},
journal = {PLoS Biology},
month = {apr},
number = {4},
pages = {e1002128},
publisher = {Public Library of Science},
title = {{Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm}},
url = {https://dx.plos.org/10.1371/journal.pbio.1002128},
volume = {13},
year = {2015}
}
@misc{Ozzie,
abstract = {Personal data mining mechanisms and methods are employed to identify relevant information that otherwise would likely remain undiscovered. Users supply personal data that can be analyzed in conjunction with data associated with a plurality of other users to provide useful information that can improve business operations and/or quality of life. Personal data can be mined alone or in conjunction with third party data to identify correlations amongst the data and associated users. Applications or services can interact with such data and present it to users in a myriad of manners, for instance as notifications of opportunities.},
author = {Ozzie, RE},
booktitle = {Google Patents},
institution = {Microsoft Technology Licensing LLC},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{Personal data mining}},
url = {https://patents.google.com/patent/US7930197B2/en}
}
@techreport{Fox2017,
abstract = {As its title implies, Using the R Commander (Fox, 2017) is written from the point of view of the user of the R Commander graphical user interface (GUI) to R. The book includes a brief introduction to the use of R Commander plug-in packages, employing the RcmdrPlu- gin.TeachingDemos and RcmdrPlugin.survival packages as examples. In contrast, the current document aims to be a comprehensive manual for R Commander plug-in package authors.},
author = {Fox, John},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fox - 2017 - Writing R Commander Plug-in Packages.pdf:pdf},
institution = {R Package},
pages = {103},
title = {{Writing R Commander Plug-in Packages}},
url = {https://socialsciences.mcmaster.ca/jfox/Books/RCommander/Writing-Rcmdr-Plugins.pdf},
year = {2017}
}
@misc{Ouellette2015,
annote = {id: 1},
author = {Ouellette, Jennifer},
number = {8/10/2015},
title = {{How Complex Networks Explode with Growth | Quanta Magazine }},
url = {https://www.quantamagazine.org/20150714-explosive-percolation-networks/?utm{\_}source=Complexity+Digest{\&}utm{\_}campaign=7ec4d8faa9-RSS{\_}EMAIL{\_}CAMPAIGN{\&}utm{\_}medium=email{\&}utm{\_}term=0{\_}f55ea67de1-7ec4d8faa9-67211033},
volume = {2015},
year = {2015}
}
@article{Lieberman2005,
abstract = {Despite repeated calls for the use of “mixed methods” in comparative analysis, political scientists have few systematic guides for carrying out such work. This paper details a unified approach which joins intensive case-study analysis with statistical analysis. Not only are the advantages of each approach combined, but also there is a synergistic value to the nested research design: for example, statistical analyses can guide case selection for in-depth research, provide direction for more focused case studies and comparisons, and be used to provide additional tests of hypotheses generated from small-N research. Small-N analyses can be used to assess the plausibility of observed statistical relationships between variables, to generate theoretical insights from outlier and other cases, and to develop better measurement strategies. This integrated strategy improves the prospects of making valid causal inferences in cross-national and other forms of comparative research by drawing on the distinct strengths of two important approaches.},
author = {Lieberman, Evan S.},
doi = {10.1017/S0003055405051762},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lieberman - 2005 - Nested analysis as a mixed-method strategy for comparative research.pdf:pdf},
isbn = {doi:10.1017/S0003055405051762},
issn = {00030554},
journal = {American Political Science Review},
month = {aug},
number = {3},
pages = {435--452},
pmid = {17793584},
publisher = {Cambridge University Press},
title = {{Nested analysis as a mixed-method strategy for comparative research}},
url = {http://www.journals.cambridge.org/abstract{\_}S0003055405051762},
volume = {99},
year = {2005}
}
@article{Abel2018,
abstract = {Data scientists are usually interested in a subset of sources with properties that are most aligned to intended data use. The SOURCERY system supports interactive multi-criteria user-driven source selection. SOURCERY allows a user to identify criteria they consider of importance and indicate their relative importance, and seeks a source selection result aligned to the user-supplied criteria preferences. The user is given an overview of the properties of the sources that are selected along with visual analyses contextualizing the result in relation to what is theoretically possible and what is possible given the set of available sources. The system also enables a user to interactively perform iterative fine-tuning to explore how changes to preferences may impact results.},
author = {Abel, Edward and Keane, John A. and Paton, Norman W. and Fernandes, Alvaro A.A. and Koehler, Martin and Konstantinou, Nikolaos and Azuan, Nurzety A. and Embury, Suzanne M.},
doi = {10.1145/3269206.3269209},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Abel et al. - 2018 - Sourcery User driven multi-criteria source selection.pdf:pdf},
isbn = {9781450360142},
issn = {00200255},
journal = {Information Sciences},
keywords = {Multi-criteria decision analysis,Optimization,Source selection},
month = {mar},
pages = {179--199},
title = {{Sourcery: User driven multi-criteria source selection}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025517310927},
volume = {430-431},
year = {2018}
}
@article{Atkinson2003,
abstract = {Metamodeling is an essential foundation for MDD, but there's little consensus on the precise form it should take and role it should play. The authors analyze the underlying motivation for MDD and then derive a concrete set of requirements that a supporting infrastructure should satisfy. They discuss why the traditional "language definition" interpretation of metamodeling isn't a sufficient foundation and explain how it can be extended to unlock MDD's full potential.},
author = {Atkinson, Colin and K{\"{u}}hne, Thomas},
doi = {10.1109/MS.2003.1231149},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Atkinson, K{\"{u}}hne - 2003 - Model-driven development A metamodeling foundation.pdf:pdf},
isbn = {0740-7459},
issn = {07407459},
journal = {IEEE Software},
month = {sep},
number = {5},
pages = {36--41},
title = {{Model-driven development: A metamodeling foundation}},
url = {http://ieeexplore.ieee.org/document/1231149/},
volume = {20},
year = {2003}
}
@article{Salvador2020,
abstract = {Natural history collections are now being championed as key to broad ecological studies, especially those involving human impacts in the Anthropocene. However, collections are going through a crisis that threatens their present and future value, going beyond underfunding/understaffing to a more damaging practice: current researchers are no longer depositing material. This seems to be especially true for ecological studies that now benefit from historical collections, as those researchers are not trained to think about voucher specimens. We investigated indexed journals in Ecology and Zoology to assess if they have guidelines concerning voucher specimens. Only 4{\%} of ecological journals presently encourage (but mostly do not require) voucher deposition, while 15{\%} of zoological journals encourage it. In the first place, this goes contrary to scientific standards of reproducibility, since specimens are primary data. Secondly, this erodes the legacy we will leave for future researchers, because if this trend goes on unchecked, it will leave a massive gap in collections' coverage, undermining the quality that is presently acclaimed. The scientific community needs a wakeup call to avoid impoverishing the future value of natural history collections. Training and changing researchers' mindsets is essential, but that takes time. For the moment, we propose a stopgap measure: at the minimum, academic journals should encourage authors to deposit specimens in open collections, such as museums and universities.},
author = {Salvador, Rodrigo B. and Cunha, Carlo M.},
doi = {10.1007/s00442-020-04620-0},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Salvador, Cunha - 2020 - Natural history collections and the future legacy of ecological research.pdf:pdf},
issn = {14321939},
journal = {Oecologia},
keywords = {Museums,Primary data,Reproducibility,Voucher specimens,Vouchering},
month = {mar},
number = {3},
pages = {641--646},
publisher = {Springer},
title = {{Natural history collections and the future legacy of ecological research}},
volume = {192},
year = {2020}
}
@misc{Fox2017a,
abstract = {R provides a powerful and comprehensive system for analysing data and when used in conjunction with the R-commander (a graphical user interface, commonly known as Rcmdr) it also provides one that is easy and intuitive to use. Basically, R provides the engine that carries out the analyses and Rcmdr provides a convenient way for users to input commands. The Rcmdr program enables analysts to access a selection of commonly-used R commands using a simple interface that should be familiar to most computer users. It also serves the important role of helping users to implement R commands and develop their knowledge and expertise in using the command line --- an important skill for those wishing to exploit the full power of the program},
author = {Fox, John},
title = {{Rcmdr.com}},
url = {http://www.rcommander.com/ https://socialsciences.mcmaster.ca/jfox/Misc/Rcmdr/},
urldate = {2018-02-27},
year = {2017}
}
@article{Dennis2017,
abstract = {Appropriate large-scale citizen-science data present important new opportunities for biodiversity modelling, due in part to the wide spatial coverage of information. Recently proposed occupancy modelling approaches naturally incorporate random effects in order to account for annual variation in the composition of sites surveyed. In turn this leads to Bayesian analysis and model fitting, which are typically extremely time consuming. Motivated by presenceonly records of occurrence from the UK Butterflies for the New Millennium data base, we present an alternative approach, in which site variation is described in a standard way through logistic regression on relevant environmental covariates. This allows efficient occupancy model-fitting using classical inference, which is easily achieved using standard computers. This is especially important when models need to be fitted each year, typically for many different species, as with British butterflies for example. Using both real and simulated data we demonstrate that the two approaches, with and without random effects, can result in similar conclusions regarding trends. There are many advantages to classical model-fitting, including the ability to compare a range of alternative models, identify appropriate covariates and assess model fit, using standard tools of maximum likelihood. In addition, modelling in terms of covariates provides opportunities for understanding the ecological processes that are in operation. We show that there is even greater potential; the classical approach allows us to construct regional indices simply, which indicate how changes in occupancy typically vary over a species' range. In addition we are also able to construct dynamic occupancy maps, which provide a novel, modern tool for examining temporal changes in species distribution. These new developments may be applied to a wide range of taxa, and are valuable at a time of climate change. They also have the potential to motivate citizen scientists.},
author = {Dennis, Emily B. and Morgan, Byron J.T. and Freeman, Stephen N. and Ridout, Martin S. and Brereton, Tom M. and Fox, Richard and Powney, Gary D. and Roy, David B.},
doi = {10.1371/journal.pone.0174433},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dennis et al. - 2017 - Efficient occupancy model-fitting for extensive citizen-science data.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {mar},
number = {3},
publisher = {Public Library of Science},
title = {{Efficient occupancy model-fitting for extensive citizen-science data}},
volume = {12},
year = {2017}
}
@article{Fisher2012,
author = {Fisher, Danyel and DeLine, Rob and Czerwinski, Mary and Drucker, Steven},
doi = {10.1145/2168931.2168943},
issn = {10725520},
journal = {Interactions},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {may},
number = {3},
pages = {50},
publisher = {ACM},
title = {{Interactions with big data analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2168931.2168943},
volume = {19},
year = {2012}
}
@misc{Maddaus2019,
abstract = {How to find the peaks and troughs in a density distribution},
author = {Maddaus, Ian},
booktitle = {Blog},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Maddaus - 2019 - Finding Peak Values For a Density Distribution.pdf:pdf},
title = {{Finding Peak Values For a Density Distribution}},
url = {http://ianmadd.github.io/pages/PeakDensityDistribution.html},
urldate = {2019-01-08},
year = {2019}
}
@article{Tsai2015,
abstract = {The age of big data is now coming. But the traditional data analytics may not be able to handle such large quantities of data. The question that arises now is, how to develop a high performance platform to efficiently analyze big data and how to design an appropriate mining algorithm to find the useful things from big data. To deeply discuss this issue, this paper begins with a brief introduction to data analytics, followed by the discussions of big data analytics. Some important open issues and further research directions will also be presented for the next step of big data analytics.},
author = {Tsai, Chun Wei and Lai, Chin Feng and Chao, Han Chieh and Vasilakos, Athanasios V.},
doi = {10.1186/s40537-015-0030-3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Tsai et al. - 2015 - Big data analytics a survey.pdf:pdf},
isbn = {9781467379106},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big data,data analytics,data mining},
month = {dec},
number = {1},
pages = {21},
pmid = {1511170304},
publisher = {Nature Publishing Group},
title = {{Big data analytics: a survey}},
url = {http://www.journalofbigdata.com/content/2/1/21},
volume = {2},
year = {2015}
}
@article{Russom2011,
author = {Russom, Philip},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Russom - 2011 - Big data analytics.pdf:pdf},
isbn = {9783642355417},
issn = {97836423},
journal = {TDWI best practices report, fourth quarter},
keywords = {advanced,analytics,barriers,benefits,best practices,big data,organization,recommendations,techniques,tools,trends,vendor products},
number = {4},
pages = {1--34},
title = {{Big data analytics}},
url = {https://vivomente.com/wp-content/uploads/2016/04/big-data-analytics-white-paper.pdf},
volume = {19},
year = {2011}
}
@inproceedings{palmer1997webproducts,
author = {Palmer, P J and Williams, D J and Dixon, A C},
booktitle = {Proceedings of the 21st IEEE International Electronic Manufacturing Technology Symposium},
isbn = {0-7803-3929-0},
organization = {Austin, Texas},
pages = {134--139},
title = {{Web Based Tools in Support of Life Cycle Engineering of Telecommunications Products}},
year = {1997}
}
@article{Munoz2005,
annote = {id: 1; isbn: print 978-3-540-26095-0; isbn: electronic 978-3-540-32127-9; issn: print 0302-9743; issn: electronic 1611-3349},
author = {Mu{\~{n}}oz, Javier and {Pelechano {\textless}component{\_}number{\textgreater} Chapter 24}, Vicente},
doi = {10.1007/11431855_24},
issn = {0302-9743},
pages = {342 {\textless}last{\_}page{\textgreater} 356},
title = {{Advanced Information Systems Engineering; Building a Software Factory for Pervasive Systems Development }},
url = {http://dx.doi.org/10.1007/11431855{\_}24},
volume = {3520},
year = {2005}
}
@article{Szent-Gyorgyi1972,
author = {Szent-Gy{\"{o}}rgyi, Albert},
doi = {10.1126/science.176.4038.966},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Szent-Gy{\"{o}}rgyi - 1972 - Dionysians and apollonians.pdf:pdf},
issn = {00368075},
journal = {Science},
number = {4038},
pages = {966},
title = {{Dionysians and apollonians}},
volume = {176},
year = {1972}
}
@article{Moppett2011,
abstract = {/st{\textgreater} Bibliometrics provide surrogate measures of the quality and quantity of research undertaken by departments and individuals. Previous reports have suggested that academic anaesthesia research in the UK is in decline. We wished to provide a comprehensive description of current and historical published output of UK anaesthesia researchers.},
author = {Moppett, I. K. and Hardman, J. G.},
doi = {10.1093/bja/aer124},
isbn = {0007-0912},
issn = {00070912},
journal = {British Journal of Anaesthesia},
keywords = {Big Data,achievement,anaesthesia,bibliometrics,publications},
mendeley-tags = {Big Data},
month = {sep},
number = {3},
pages = {351--356},
pmid = {21622666},
title = {{Bibliometrics of anaesthesia researchers in the UK}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0007091217330738},
volume = {107},
year = {2011}
}
@book{GreatBritain.OfficeofGovernmentCommerce.2009,
abstract = {5th ed. This publication provides a universally applicable project management method - the principles, processes and techniques that enable individuals and organisations to successfully deliver their projects within time, cost and quality constraints. Managing Successful Projects with PRINCE2 forms part of a pair of publications that are the result of the PRINCE2: 2009 Project to update the PRINCE2 guidance. Its companion is Directing Successful Projects with PRINCE2. The title has been designed to be a role specific handbook for Project Managers, Team Managers and Project Support. The new 2009 edition: explains the principles that underpin PRINCE2; describes the PRINCE2 processes in their entirety and 'Key Themes' of project management, specific to PRINCE2, that are required for the processes to be effective; cross-references techniques that may be applied; also explains how to tailor the method; and, provides context of when and how to use PRINCE2 for different project environments. Conventions used in this manual -- Principles -- Introduction to PRINCE2 themes -- Business case -- Organization -- Quality -- Plans -- Risk -- Change -- Progress -- Introduction to processes -- Starting up a project -- Directing a project -- Initiating a project -- Controlling a stage -- Managing product delivery -- Managing a stage boundary -- Closing a project -- Tailoring PRINCE2 to the project environment -- Product description outlines -- Governance -- Roles and responsibilities -- Product-based planning example -- Health check.},
author = {{Great Britain. Office of Government Commerce.}},
isbn = {9780113310593},
publisher = {TSO},
title = {{Managing successful projects with PRINCE2.}},
year = {2009}
}
@inproceedings{Panov2008,
abstract = {Motivated by the need for unification of the field of data mining and the growing demand for formalized representation of outcomes of research, we address the task of constructing an ontology of data mining. The proposed ontology, named OntoDM, is based on a recent proposal of a general framework for data mining, and includes definitions of basic data mining entities, such as datatype and dataset, data mining task, data mining algorithm and components thereof (e.g., distance function), etc. It also allows for the definition of more complex entities, e.g., constraints in constraint-based data mining, sets of such constraints (inductive queries) and data mining scenarios (sequences of inductive queries). Unlike most existing approaches to constructing ontologies of data mining, OntoDM is a deep/heavy-weight ontology and follows best practices in ontology engineering, such as not allowing multiple inheritance of classes, using a predefined set of relations and using a top level ontology.},
author = {Panov, Pane and Deroski, Sao and Soldatova, Larisa N.},
booktitle = {Proceedings - IEEE International Conference on Data Mining Workshops, ICDM Workshops 2008},
doi = {10.1109/ICDMW.2008.62},
isbn = {9780769535036},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {752--760},
title = {{OntoDM: An ontology of data mining}},
url = {http://ieeexplore.ieee.org/abstract/document/4734003/},
year = {2008}
}
@misc{Taylor2019,
abstract = {Email: R Usage At Sheffield University},
author = {Taylor, Mark and Palmer, P J},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Palmer - 2019 - Personal Communication. October 1, 2019.pdf:pdf},
title = {{Personal Communication. October 1, 2019.}},
year = {2019}
}
@article{Mittelstadt2016,
abstract = {In information societies, operations, decisions and choices previously left to humans are increasingly delegated to algorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken as a result. More and more often, algorithms mediate social processes, business transactions, governmental decisions, and how we perceive, understand, and interact among ourselves and with the environment. Gaps between the design and operation of algorithms and our understanding of their ethical implications can have severe consequences affecting individuals as well as groups and whole societies. This paper makes three contributions to clarify the ethical importance of algorithmic mediation. It provides a prescriptive map to organise the debate. It reviews the current discussion of ethical aspects of algorithms. And it assesses the available literature in order to identify areas requiring further work to develop the ethics of algorithms.},
annote = {Problems:
•   Inconclusive evidence.   When algorithms draw conclusions from the data theyprocess using inferential statistics and/or machinelearning techniques, they produce probable yet inevit-ably uncertain knowledge.
•   Inscrutable evidence.   When data are used as (or processed to produce) evi-dence for a conclusion, it is reasonable to expect that theconnection between the data and the conclusion shouldbe accessible (i.e. intelligible as well as open to scrutinyand perhaps even critique).
•   Misguided evidence.   Algorithms process data and are therefore subject to alimitation shared by all types of data-processing,namely that the output can never exceed the input.
•   Unfair outcomes.   Actions driven by algorithms can be assessedaccording to numerous ethical criteria and principles,which we generically refer to here as the observer-dependent ‘fairness' of the action and its eﬀects. Anaction can be found discriminatory, for example,solely from its eﬀect on a protected class of people,even if made on the basis of conclusive, scrutable andwell-founded evidence.
•   Transformative effects.   Algorithmic activities, like proﬁling, re-ontologise theworld by understanding and conceptualising it in new,unexpected ways, and triggering and motivating actionsbased on the insights it generates.
•   Traceability.   harm caused by algorithmic activity is hardto debug (i.e. to detect the harm and ﬁnd its cause), butalso that it is rarely straightforward to identify whoshould be held responsible for the harm caused
 
They do a lit review on the problems:
•   Inconclusive evidence leading to unjustified actions.   The search for causal links is diﬃcult, as correlationsestablished in large, proprietary datasets are frequentlynot reproducible or falsiﬁable (cf. Ioannidis, 2005;Lazer et al., 2014). Despite this, correlations based ona suﬃcient volume of data are increasingly seen as suf-ﬁciently credible to direct action without ﬁrst establish-ing causality (Hildebrandt, 2011; Hildebrandt andKoops, 2010; Mayer-Scho ¨nberger and Cukier, 2013;Zarsky, 2016). In this sense data mining and proﬁlingalgorithms often need only establish a suﬃciently reli-able evidence base to drive action, referred to here asactionable insights.Acting on correlations can be doubly problematic.   Spurious correlations may be discovered rather thangenuine causal knowledge. In predictive analytics cor-relations are doubly uncertain (Ananny, 2016). Even if strong correlations or causal knowledge are found, thisknowledge may only concern populations while actionsare directed towards individuals (Illari and Russo,2014).
•    Inscrutable evidence leading to opacity.   The primary components of transparency are accessibility and comprehensibility of information.Information about the functionality of algorithms isoften intentionally poorly accessible. Proprietary algorithms are kept secret for the sake of competitiveadvantage (Glenn and Monteith, 2014; Kitchin, 2016;Stark and Fins, 2013), national security (Leese, 2014),or privacy. Transparency can thus run counter to otherethical ideals, in particular the privacy of data subjectsand autonomy of organisations.
•   Misguided evidence leading to bias.   Algorithms inevitably make biased decisions. Analgorithm's design and functionality reﬂects the valuesof its designer and intended uses, if only to the extentthat a particular design is preferred as the best or mosteﬃcient option. Development is not a neutral, linearpath; there is no objectively correct choice at anygiven stage of development, but many possible choices(Johnson, 2006). As a result, ‘‘the values of the author[of an algorithm], wittingly or not, are frozen into thecode, eﬀectively institutionalising those values''(Macnish, 2012: 158). It is diﬃcult to detect latentbias in algorithms and the models they produce whenencountered in isolation of the algorithm's develop-ment history (Friedman and Nissenbaum, 1996;Hildebrandt, 2011; Morek, 2006).
•   Unfair outcomes leading todiscrimination.   Proﬁling by algorithms, broadly deﬁned ‘‘asthe construction or inference of patterns by means of data mining and . . . the application of the ensuing proﬁles to people whose data match with them''(Hildebrandt and Koops, 2010: 431), is frequentlycited as a source of discrimination. Proﬁling algorithms identify correlations and make predictions about behaviour at a group-level, albeit with groups (or proﬁles) that are constantly changing and re-deﬁned by the algorithm (Zarsky, 2013). Whether dynamic or static,the individual is comprehended based on connections with others identiﬁed by the algorithm, rather than actual behaviour (Newell and Marabelli, 2015: 5). Individuals' choices are structured according to information about the group (Danna and Gandy, 2002:382). Proﬁling can inadvertently create an evidence-base that leads to discrimination (Vries, 2010).
•   Transformative effects leading to challenges for [individual] autonomy.   Personalisation algorithms tread a ﬁne line betweensupporting and controlling decisions by ﬁltering whichinformation is presented to the user based upon in-depth understanding of preferences, behaviours, andperhaps vulnerabilities to inﬂuence (Bozdag, 2013;Goldman, 2006; Newell and Marabelli, 2015; Zarsky,2016). Classiﬁcations and streams of behavioural dataare used to match information to the interests and attributes of data subjects. The subject's autonomy in decision-making is disrespected when the desired choice reﬂects third party interests above the individual's (Applin and Fischer, 2015; Stark and Fins, 2013).This situation is somewhat paradoxical. In principle, personalisation should improve decision-making by providing the subject with only relevant information when confronted with a potential information overload; however, deciding which information is relevant is inherently subjective. The subject can be pushed to make the ‘‘institutionally preferred action rather than their own preference'' (Johnson,2013); online consumers, for example, can be nudgedto ﬁt market needs by ﬁltering how products are displayed (Coll, 2013). Lewis and Westlund(2015: 14) suggest that personalisation algorithmsneed to be taught to ‘act ethically' to strike a balance between coercing and supporting users' decisional autonomy.
•   Transformative effects leading tochallenges for informational privacy [NB this is before GDPR].   
•   Traceability leading to moral responsibility.   Traditionally, computer programmers have had‘‘control of the behaviour of the machine in everydetail'' insofar as they can explain its design and func-tion to a third party (Matthias, 2004). This traditionalconception of responsibility in software design assumesthe programmer can reﬂect on the technology's likelyeﬀects and potential for malfunctioning (Floridi et al.,2014), and make design choices to choose the mostdesirable outcomes according to the functional speciﬁ-cation (Matthias, 2004). With that said, programmersmay only retain control in principle due to the complex-ity and volume of code (Sandvig et al., 2014), and theuse of external libraries often treated by the program-mer as ‘black boxes' (cf. Note 7).Superﬁcially, the traditional, linear conception of responsibility is suitable to non-learning algorithms.When decision-making rules are ‘hand-written', theirauthors retain responsibility (Bozdag, 2013). Decision-making rules determine the relative weight given to thevariables or dimensions of the data considered by thealgorithm. A popular example is Facebook's EdgeRankpersonalisation algorithm, which prioritises contentbased on date of publication, frequency of interactionbetween author and reader, media type, and otherdimensions. Altering the relative importance of eachfactor changes the relationships users are encouragedto maintain. The party that sets conﬁdence intervals foran algorithm's decision-making structure sharesresponsibility for the eﬀects of the resultant false posi-tives, false negatives and spurious correlations (Birrer,2005; Johnson, 2013; Kraemer et al., 2011). Fule andRoddick (2004: 159) suggest operators also have aresponsibility to monitor for ethical impacts of deci-sion-making by algorithms because ‘‘the sensitivity of a rule may not be apparent to the mine . .  the ability to harm or to cause oﬀense can often be inadvertent.'
{\ldots}
Particular challenges arise for algorithms withlearning capacities, which defy the traditional concep-tion of designer responsibility. The model requires thesystem to be well-deﬁned, comprehensible and predict-able; complex and ﬂuid systems (i.e. one with count-less decision-making rules and lines of code) inhibitholistic oversight of decision-making pathways anddependencies. Machine learning algorithms are par-ticularly challenging in this respect (Burrell, 2016;Matthias, 2004; Zarsky, 2016), seen for instance ingenetic algorithms that program themselves. The trad-itional model of responsibility fails because ‘‘nobodyhas enough control over the machine's actions to beable to assume the responsibility for them'' (Matthias,2004: 177)},
author = {Mittelstadt, Brent Daniel and Allo, Patrick and Taddeo, Mariarosaria and Wachter, Sandra and Floridi, Luciano},
doi = {10.1177/2053951716679679},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mittelstadt et al. - 2016 - The ethics of algorithms Mapping the debate.pdf:pdf},
issn = {2053-9517},
journal = {Big Data {\&} Society},
keywords = {Algorithms,Big Data,automation,data analytics,data mining,ethics,machine learning},
month = {dec},
number = {2},
pages = {21},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{The ethics of algorithms: Mapping the debate}},
url = {http://journals.sagepub.com/doi/10.1177/2053951716679679},
volume = {3},
year = {2016}
}
@inproceedings{Yakout2011,
abstract = {In this paper we present GDR, a Guided Data Repair framework that incorporates user feedback in the cleaning process to enhance and accelerate existing automatic repair techniques while minimizing user involvement. GDR consults the user on the updates that are most likely to be benecial in improving data quality. GDR also uses machine learning methods to identify and apply the correct updates directly to the database without the actual involvement of the user on these specic updates. To rank potential updates for consultation by the user, we rst group these repairs and quantify the utility of each group using the decision-theory concept of value of information (VOI). We then apply active learning to order updates within a group based on their ability to improve the learned model. User feedback is used to repair the database and to adaptively rene the training set for the model. We empirically evaluate GDR on a real-world dataset and show signicant improvement in data quality using our user guided repairing process. We also, assess the trade-off between the user efforts and the resulting data quality. {\textcopyright} 2011 VLDB Endowment.},
author = {Yakout, Mohamed and Elmagarmid, Ahmed K. and Neville, Jennifer and Ouzzani, Mourad and Ilyas, Ihab F.},
booktitle = {Proceedings of the VLDB Endowment},
doi = {10.14778/1952376.1952378},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yakout et al. - 2011 - Guided data repair.pdf:pdf},
issn = {21508097},
number = {5},
pages = {279--289},
publisher = {Association for Computing Machinery},
title = {{Guided data repair}},
volume = {4},
year = {2011}
}
@book{Cone2018,
abstract = {Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Created by John Gruber in 2004, Markdown is now one of the world's most popular markup languages. Using Markdown is different than using a WYSIWYG editor. In an application like Microsoft Word, you click buttons to format words and phrases, and the changes are visible immediately. Markdown isn't like that. When you create a Markdownformatted file, you add Markdown syntax to the text to indicate which words and phrases should look different.},
address = {International e-Book},
author = {Cone, Matt},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cone - 2018 - The Markdown Guide.pdf:pdf},
keywords = {Software,Software Manual},
pages = {62},
publisher = {Mark Cone},
title = {{The Markdown Guide}},
url = {https://www.markdownguide.org/assets/book/markdown-guide.pdf},
year = {2018}
}
@misc{Daw2007,
abstract = {The paper explores the problem of Defence Acquisition, with particular reference to the UK Acquisition environment, and considers the issues associated with the current initiatives relating to the acquisition of capability.  The paper offers a view that Defence Acquisition in Capability terms is a “wicked problem” and considers a number of models by which the “wickedness” may be managed.  The paper also considers the issue of Potential Residual Wickedness within the ‘final' delivery and implementation stages of acquisition.   Over the course of several years, UK Defence Acquisition has been the subject of many reviews, initiatives and change programmes, the most recent of which is Smart Procurement which became Smart Acquisition and is now instantiated within the McKane ‘Enabling Acquisition Change' Programme and Through Life Capability Management (TLCM).  In parallel, the output of the process has subtly changed from tangible product views (encapsulated by Better Faster Cheaper) to less tangible capability and effect perspectives.  The paper offers that these initiatives ‘fail' and do not deliver the proposed desired benefits because the basic premise that the problem has a ‘solution' is false and that in fact Defence Acquisition – particularly of abstract concepts such as capability – is a wicked problem.  From this hypothesis the paper articulates a number of perspectives that may enable the management of this problem (rather than its solution) in areas such as Systems Engineering, Commercial constructs, behavioural issues, skillsets and competencies, lifecycle, and organisational demands.  From these perspectives can be derived a number of essential characteristics and styles that may enable the comprehensive management of wicked problems.  However, it is recognised that within the sphere of Defence Acquisition, even when the abstract problem of capability has been translated into definable products and services, there will remain an issue of Potential Residual Wickedness which reflects the underlying nature of the required development (in terms of either the complexity or degree of complication).  These constructs are considered against a standard model of guidance to Defence Acquisition, which enables a broad model of engagement through life.  From this, the concepts of Through Life Capability Management can be addressed and the characteristics defined for the acquisition environment used as an information medium to inform the acquisition community},
address = {Belfast, N.I.},
annote = {Paper in T-AREA-SoS.

Recognises intrinsic {\&} induced complexity - calls the former 'potential resdiual wickedness', no name for the latter. 

Starts by pointing out that the MoD and industry use different definitions of capability:
MoD:
"In the UK, the Joint Doctrine and Concepts Centre offers a framework (Figure 1) in which 7 ‘capabilities' are defined as the primary goals of the military domain and they are expressed as verbs; Command, Inform, Prepare, Project, Protect, Sustain and Operate [Ref. 9: JDCC Joint Concepts Paper www.mod.uk/jdcc/concepts.htm]"
Industry:
"The industrial perspective however is generally constructed around the 5 elements of People, Process, Product, Technology and Facilities (as represented in Figure 2). The relationship between the 5 elements is also more clear cut than those expressed in abstract terms, as each offer some contribution to the development and subsequent sale of product – the product is the focus of the Industrial capability definition."

Then says the interface is the DLoDs, but the latter aren't static, and he reckons there are morethan the usual TEPID OIL set. Includes Industrial readiness as one of them, to a total of 13 or so. And all this has to be accomplished within the notion of NEC(at that time; since replaced by SoSA).

Discusses MOD's 'Acquisition Operating Framework', emergent from the MOD Equipment Capability Group, and core of the DES approach.

Then has a section on the AOF 4-blobs diagram, Fig 3, which he then links to a version of DeMeyer, in Fig. 4. What links the projects in fig 4 is Co-ordination and communication, equivalent to the feed-back arrows in the blobs diagram. But note that time has a role in this, too; implied by C{\&}C, but the context had=s a flow to it as well, which solidifies the view forwards (as new uncertainties appear).

Goes on to relate his Fig 4 to 'engineering styles, or deMeyer project mgmt styles. From this, he mentions 4 engineering styles to match these:
Waterfall model
VEE model
Option model
Emergence model
Unfortunately, he doesn't describe the latter two. But reading the document, it seems that the 'Emergence' model is about strategy formulation (abstraction, representation from multiple perspectives, integration or aggregation to define the problem more precisely, and communication), and the option model has to do with trade spaces, basedon the emergence outcomes.

Table 1 lists wicked problem characteristics and how to mitigate them; Appendix A does the same thing for the defence realm.'Immune' quote - resistance to change:
"Too often well-intentioned individuals seeking a quite obvious and necessary change can attempt to make up for poor processes or a lack of management attention. Regrettably they are doomed to fail. This has led to the concept of an ‘immune system' [Barton Ref 6] in that the underpinning Defence Acquisition community responds to the change programmes and initiatives as does the body in response to a virus – it takes it in, recognises the existence of and then analyses the infection, applies antibodies to the wound, negates or nullifies the infection and then returns to something that is very similar to the previous ‘healthy' state. As a set of loosely coupled ‘fiefdoms', well distributed and each operating to its own sub-optimal targets, the Defence Acquisition communities have evolved to exemplify the perfect immune system."





References:
1. Jordan, Lee, Cawsey, ‘A Report on the Arrangements for Managing Major Projects in the Procurement Executive' HMG MoD 1988
2. Downey W G, 1968, Report of the Steering Group on Development Cost Estimating, Ministry of Technology, London
3. The Strategic Defence Review HMG MoD July 98 and the subsequent Strategic Defence Review: A New Chapter HMG MoD July 2002
4. McKinsey, ‘Transforming the UK's Procurement System' HMG MoD February 1998
5. McKane T, ‘Enabling Acquisition Change' HMG MoD June 2006
6. Private conversations Robert Barton (BAE SYSTEMS) and Andrew Daw, 2005 and adapted from Burke, W. and Litwin, G.H. (1994). Diagnostic Models for Organizational Development, in: A Howard and Associates (eds.): Diagnosis for Organizational Change, London and New York, Guilford Press.
7. Daw A J, ‘New Process and Structure Thinking for Capability Development' 9th International CCRP Symposium Sept 04 Copenhagen
8. Daw A J, ‘On the Use of Synthetic Environments for the Through Life Delivery of Capability' NATO RTO SAS Conference April 2005, Norfolk Virginia
9. JDCC Joint Concepts Paper www.mod.uk/jdcc/concepts.htm
10. Unpublished research and study activity for the UK MoD Equipment Capability Group (Michael Collins, Andrew Daw, Richard Westgarth)
11. Oliver R, ‘Trade Space Method for Capability and Lines of Development', INCOSE UK Spring Conference April 07, QinetiQ/D{\&}TS/C{\&}IS/PUB0703567
12. Rittel, H. W. J., Webber M.M, ‘Dilemmas in a General Theory of Planning.' Policy Sciences 4, 155-169, 1973
13. John, P Prof, ‘Contracting against Requirements Documents or Shared Models?', BAE SYSTEMS R{\&}T Conference February 2007
14. Aaby, A ‘Computational Complexity and Problem Hierarchy', 2002, 
http://cs.wwc.edu/{\~{}}aabyan/Theory/complexity.html 
15. ‘Acquisition Operating Framework, sub-section ‘What is the Scope of Capability Management?'' HMG MOD Apr 2007
http://www.ams.mod.uk/aofcontent/operational/business/capabilitymanagement/capabilitymanagement{\_}scope.htm 
[NB: Superseding the ‘Acquisition Management System handbook' Vol 6, HMG MoD]
16. Private conversations Professor Phil John (Cranfield University) and Andrew Daw, February 2007
17. Capt Toby Edison ‘Rugby and the OODA Loop' Feb March 2002 Rugby Magazine
18. Conway B, Mawby D, ‘Early Lessons for Establishing Through Life Capability Programmes', RUSI Defence Project Management 2006, Oct 2006
19. Price S.N {\&} John P. The Status of Models in Systems Engineering IFORS 2002, RMCS Shrivenham
20. Internal Presentations R Westgarth (Equipment Capability Group) for the Surface Combatant DIS Pathfinder 2006
21. Defence Industrial Strategy: Defence White Paper HMG MoD Dec 05
22. Internal BAE SYSTEMS Project Development work with Strategic Business Development and JA Consulting (April 07)
23. Private conversations Robert Barton (BAE SYSTEMS) and Andrew Daw, 2006},
author = {Daw, A J},
booktitle = {7th AIAA Aviation Technology, Integration and Operations Conference:  Challenges in Systems Engineering for Advanced Technology Programmes},
keywords = {BPR,Work flow management,agility,allocation of functions,business process re-engineering,competitive challenges,complexity,control,dashboard/battlespace,decision-making,design process,engineering,knowledge,learning organisation,market conditions,network-centric warfare,org. trust,organisational design,philosophy,product introduction process,reference architectures,socio-tech,strategic planning,supply chain,theory},
pages = {1--26},
publisher = {AIAA},
title = {{Keynote:  On the wicked problem of defence acquisition}},
year = {2007}
}
@article{Wooodward2020,
author = {Wooodward, Steve (Editor)},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wooodward - 2020 - NEWSLETTER 63.pdf:pdf},
journal = {Leicester Entomological Society Newsletter},
pages = {16},
title = {{NEWSLETTER 63}},
year = {2020}
}
@techreport{Lovelace2017,
abstract = {This tutorial is an introduction to analysing spatial data in R, specifically through map-making with R's ‘base' graphics and various dedicated map-making packages for R including tmap and leaflet. It teaches the basics of using R as a fast, user-friendly and extremely powerful command-line Geographic Information System (GIS).},
address = {Leeds},
author = {Lovelace, Robin and Cheshire, James},
booktitle = {National Centre for Research Methods Working Paper 08/14},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lovelace, Cheshire - 2017 - Introduction to visualising spatial data in R.pdf:pdf},
institution = {Leeds University},
keywords = {GIS,Geospatial},
mendeley-tags = {GIS,Geospatial},
pages = {20},
title = {{Introduction to visualising spatial data in R}},
url = {https://cran.r-project.org/doc/contrib/intro-spatial-rl.pdf http://eprints.ncrm.ac.uk/3295/ https://github.com/Robinlovelace/Creating-maps-in-R/raw/master/intro-spatial-rl.pdf},
year = {2017}
}
@article{Lepri2016,
abstract = {The unprecedented availability of large-scale human behavioral data is profoundly changing the world we live in. Researchers, companies, governments, financial institutions, non-governmental organizations and also citizen groups are actively experimenting, innovating and adapting algorithmic decision-making tools to understand global patterns of human behavior and provide decision support to tackle problems of societal importance. In this chapter, we focus our attention on social good decision-making algorithms, that is algorithms strongly influencing decision-making and resource optimization of public goods, such as public health, safety, access to finance and fair employment. Through an analysis of specific use cases and approaches, we highlight both the positive opportunities that are created through data-driven algorithmic decision-making, and the potential negative consequences that practitioners should be aware of and address in order to truly realize the potential of this emergent field. We elaborate on the need for these algorithms to provide transparency and accountability, preserve privacy and be tested and evaluated in context, by means of living lab approaches involving citizens. Finally, we turn to the requirements which would make it possible to leverage the predictive power of data-driven human behavior analysis while ensuring transparency, accountability, and civic participation.},
archivePrefix = {arXiv},
arxivId = {1612.00323},
author = {Lepri, Bruno and Staiano, Jacopo and Sangokoya, David and Letouz{\'{e}}, Emmanuel and Oliver, Nuria},
doi = {10.1007/978-3-319-54024-5_1},
eprint = {1612.00323},
isbn = {2197-6503},
pages = {3--24},
title = {{The Tyranny of Data? The Bright and Dark Sides of Data-Driven Decision-Making for Social Good}},
url = {http://link.springer.com/10.1007/978-3-319-54024-5{\_}1 http://arxiv.org/abs/1612.00323},
year = {2016}
}
@book{Watson1873,
abstract = {A DIVISION of this work into two or more Parts lias become a matter of necessity or forced convenience, more than one of preference. At the time of writing and printing the Intro- i $\backslash$ duction, it was hoped by the wiiter that he would be able to keep ahead of the printer uninterruptedly until the whole series $\backslash$ of Orders should be completed. Under that expectation a sS " "First Volume" was there mentioned, with the possibility of a " Second to follow. But the Dicotyledonous Orders only were got tlirough (in the winter of 1872-3) before the diverging avocations of summer obliged the Compiler's pen to be set aside until a succeeding winter.},
address = {[London]},
author = {Watson, Hewett Cottrell},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Watson - 1873 - Topographical botany being local and personal records towards shewing the distribution of British plants traced through.pdf:pdf},
keywords = {Botany / Great Britain.},
pages = {2 v.},
publisher = {Thames Ditton},
title = {{Topographical botany; being local and personal records towards shewing the distribution of British plants traced through the 112 counties and vice-counties of England and Scotland.}},
url = {file://catalog.hathitrust.org/Record/100595348 http://hdl.handle.net/2027/uiug.30112009943611 (v.2) http://hdl.handle.net/2027/uiuo.ark:/13960/t84j4158s (v.1)},
year = {1873}
}
@article{Griffin2010,
abstract = {{\{}$\backslash$LaTeX{\}}, in addition to its typesetting role, has considerable potential as a tool to assist in workflow automation for Qualitative Data Analysis (QDA) of collected research data.},
author = {Griffin, Ivan and Richardson, Ita},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Griffin, Richardson - 2010 - Using LATEX for Qualitative Data Analysis.pdf:pdf},
journal = {The PracTEX Journal},
number = {1},
pages = {1--17},
title = {{Using LATEX for Qualitative Data Analysis}},
url = {https://www.tug.org/pracjourn/2010-1/griffin/griffin.pdf},
year = {2010}
}
@techreport{GSMA142014,
address = {Barcelona, ESP},
annote = {Document in Road2CPS. GSM started in France about 1980, became EU standard for digital comunication, later adopted by 200+ countries. 

Enables SMS, and M2M mobile comms; likely to be very significant in CPS. See IEEE-SA document for similar comment. Document is a good read, albeit massively detailed. Discusses the basic technology, such as IMEI codes, SIM cards, etc.

Lists glossary, standards, organises information for designers and mobile network operators. Important point about this document is that it presumes no person will oversee operation of the network at the M2M level; that's rthe reason for following the guidelines, to include all the recovery stuff (or to exclude debilitated and hacked devices)

Document is sealed; however, via exporting the Adobe PDF to Word, and then pulling it back into Acrobat, it becomes possible to access the text.

A good, useful diagram of a networked device is captured as well; not just a mobile phone, but any device. Presumably ARM's mbed architecture has cheap efficient chips for all of the generic devices in this diagram

Annex A has actual Use Cases outlining problems that following the guidelines will obviate

Attached at the bottom is a brief description of the 3GPP organisation, which combines 7 stds bobies, including ETSI, to draw up global standards for the mobile network. I expect this will discuss full M2M communication via wireless.

{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}


1 Introduction
1.1 Problem Statement

The predicted large scale growth of loT Devices and their associated loT Device Applications will create major challenges for Moble Network Operators. One major challenge that Mobile Network Operators must overcome is the risk caused by the mass deployment of inefficient. insecure or defective loT Devices on the Mobile Network Operators' networks. When deployed on a mass scale such devices can cause network signalling traffic to increase exponentially which impacts network services for all users of the mobile network. In the worst cases the mass deployment of suchloT Devices can disable a mobile network completely. 

Mobile Network Operators have faced similar issues in the past, most recently with the massive growth of smartphones. In this case many smartphone application developers inadvertently created many inefficient applications. Over the past decade Mobile Network Operators, smartphone device makers and smartphone appication developers have worked together to resolve these difficulties through a mix of increasing network capacity (e.g. 3.5G and 4G network deployment), 3GPP standardisation, improvements to smartphone operating systems and development of smartphone application developer guidelines. With the forecasted high growth in loT Devices the industry is in a similar situation to the start of the smartphone boom, but with a different group of device makers and application developers. With the loT however the potential number of devicesis higher and, due to the different commercial models for loT Devices, it is far more challenging for the Mobile Network Operator to influence the behaviour of loT Device manufacturers and loT Device Application developers. 

An loT Device overusing the network maylead to problems such as: 

• Reducing the lifetime of the (U)SIM card by increasing dramatically the read/write cycles.
• Increased power consumption of the device due to continuous restarts which may also affect the device lifetime.
• Local issues within the Mobile Network Operator's network such as cell congestion.
• Capacity and performance problems within the Mobile Network Operator's core network, such as signalling storms, which result in wide area network disruption.
• Negativelyimpacting the loT Service's performance,potentially resultingin delayed communications, degradation of the service quality and even service outages.

loT Devices overusing the moble network can affect not only the devices causing the incident but also other devices on the same loT Service Platform or those devices of other End Customers. 

Network signalling resources are dimensioned assuming an overall device usage profile with a sensible balance between traffic and signalling needs. It is therefore important that loT Devices using mobile networks adhere to some basic principles before they can be safely connected to mobile networks. 

Good design is essential to ensure that loT Device performanceis optimized and to prevent failure mechanisms creating runaway situations which may result in network overload. In situations where many loT Devices of the same type may be deployed on a single moble network the cumulative effect may have a detrimental impact on overall network performance. Poor design of loT Device Application Tool Service Platform 
communications which disregard the mobile network and loT Device status may result in inefficient use of network and device resources, affecting the loT Service experience end-to- end. 

See annex A for example cases where problematic loT Device behaviour has impacted network and device performance. 

1.2 Document Scope 
In loT scenarios loT Device firmware and software play a significant part in determining the overall performance and behaviour of the loT Service on the mobile network. With no human intervent on to fall back upon, the mechanisms that manage recovery from loT Service failure need to be built into loT Devices. 

This document willserve as a key deliverable from the GSMA Connected living programme for 2014/15. The objective of this document is to specify requirements for efficient use of mobile network connectivity. 

With the exception of section 9, the requirements and solutions captured in this document for efficient use of 3GPP moble networks are for use withinthe current (short-term) timeframe, i.e.for the current generation of loT Devices which do not necessarily support comparable 3GPP network efficiency features or are connecting to networks that do not support the necessary 3GPP network efficiency features. 


In the mid to long term loT Devices may make use of available features from 3GPP or other standards organisations to address the issues highighted in this document. In section 9 we list the 3GPP feature that may be deployed within moble networks andloT Devices in the mid to long term. 

{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}

About 3GPP

The 3rd Generation Partnership Project (3GPP) unites [Seven] telecommunications standard development organizations (ARIB, ATIS, CCSA, ETSI, TSDSI, TTA, TTC), known as “Organizational Partners” and provides their members with a stable environment to produce the Reports and Specifications that define 3GPP technologies.

The project covers cellular telecommunications network technologies, including radio access, the core transport network, and service capabilities - including work on codecs, security, quality of service - and thus provides complete system specifications. The specifications also provide hooks for non-radio access to the core network, and for interworking with Wi-Fi networks.

3GPP specifications and studies are contribution-driven, by member companies, in Working Groups and at the Technical Specification Group level.

The Four Technical Specification Groups (TSG) in 3GPP are;

• Radio Access Networks (RAN),
• Service {\&} Systems Aspects (SA),
• Core Network {\&} Terminals (CT) and
• GSM EDGE Radio Access Networks (GERAN).

The Working Groups, within the TSGs, meet regularly and come together for their quarterly TSG Plenary meeting, where their work is presented for information, discussion and approval.

Each TSG has a particular area of responsibility for the Reports and Specifications within its own Terms of Reference (Details available in the Specification Groups pages).

The last meeting of the cycle of Plenary meetings is TSG SA, which also has responsibility for the overall coordination of work and for the monitoring of its progress.},
author = {GSMA'14},
edition = {1.0},
keywords = {CSCW,Cyber-physical systems,Governance,IoT,agents,architecture,communications,complexity,control,engineering,federated control,handbook,intranets,networks,reference architectures,resilience,standards},
number = {CLP.03 - M2M Device Guidelines},
publisher = {GSMA (Groupe Spec iale Mobile Association)},
title = {{IoT device connection efficiency guidelines}},
url = {https://www.telenor.no/bedrift/images/CLP.03-Connection-Efficiency-Guidelines-Version-1.0{\_}tcm60-260558.pdf},
year = {2014}
}
@article{Vanhooff2005,
abstract = {The Model Driven Architecture (MDA) initiative of the OMG heavily promotes the use of abstract models for software development. A key ingredient of the MDA is the automation of transformations on these models. Inserting semantically rich transformation traceability links into our models allows us to better comprehend the exact effects of the applied transformations. Empowering each transformation unit to insert its own specific traceability links provides subsequent transformation units with the ability to make use of that traceability information to improve their own actions. In addition, this permits us to better modularize transformations into smaller and more reusable units that, to a certain extent, depend on each other. In this paper, we define a UML transformation traceability profile that allows the addition of semantically rich traceability links into UML models.},
address = {Nuremberg},
author = {Vanhooff, Bert and Berbers, Yolande},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vanhooff, Berbers - 2005 - Supporting Modular Transformation Units with Precise Transformation Traceability Metadata.pdf:pdf},
journal = {ECMDATW Workshop SINTEF},
number = {point 2},
pages = {15--27},
title = {{Supporting Modular Transformation Units with Precise Transformation Traceability Metadata}},
url = {https://distrinet.cs.kuleuven.be/projects/martes/public/papers/vanhooff-TraceabilityMetadata.pdf http://www.agile-itea.org/public/papers/vanhooff-TraceabilityMetadata.pdf},
year = {2005}
}
@techreport{TheCA/BrowserForum2011,
abstract = {This version of the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates present criteria established by the CA/Browser Forum for use by Certification Authorities when issuing, maintaining, and revoking publicly-trusted Certificates. The Requirements may be revised from time to time, as appropriate, in accordance with procedures adopted by the CA/Browser Forum. Because one of the primary beneficiaries of these Requirements is the end user, the Forum openly invites anyone to make recommendations and suggestions by email to the CA/Browser Forum at questions@cabforum.org. The Forum members value all input, regardless of source, and will seriously consider all such input.},
author = {{The CA / Browser Forum}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/The CA Browser Forum - 2011 - Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.0.pdf:pdf},
institution = {CA / Browser Forum},
pages = {35},
title = {{Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.0}},
url = {https://www.cabforum.org/wp-content/uploads/Baseline{\_}Requirements{\_}V1.pdf},
year = {2011}
}
@misc{IEEE2018,
abstract = {Increase the impact of your work by sharing your data and code for others to view, build upon, and reuse. Benefits of data and code sharing include: Improve the discoverability of your data by hosting it in an easily accessible repository Make your data citable with a unique Digital Object Identifier (DOI) for your dataset Increase the pace of scientific advancement by enabling other researchers to build upon your work Follow best practices of reproducible research by archiving your article's underlying data Ensure long-term preservation and accessibility of your data through the repository's archival curation},
author = {IEEE},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{About Sharing Your Data and Code - IEEE Author Center}},
url = {http://ieeeauthorcenter.ieee.org/create-your-ieee-article/use-authoring-tools-and-ieee-article-templates/about-managing-your-data/},
urldate = {2018-05-16},
year = {2018}
}
@book{Codd1990,
abstract = {From the Preface (See Front Matter for full Preface) An important adjunct to precision is a sound theoretical foundation. The relational model is solidly based on two parts of mathematics: firstorder predicate logic and the theory of relations. This book, however, does not dwell on the theoretical foundations, but rather on all the features of the relational model that I now perceive as important for database users, and therefore for DBMS vendors. My perceptions result from 20 years of practical experience in computing and data processing (chiefly, but not exclusively, with large-scale customers of IBM), followed by another 20 years of research. I believe that this is the first book to deal exclusively with the relational approach. It does, however, include design principles in Chapters 21 and 22. It is also the first book on the relational model by the originator of that model. All the ideas in the relational model described in this book are mine, except in cases where I explicitly credit someone else. In developing the relational model, I have tried to follow Einstein's advice, "Make it as simple as possible, but no simpler." I believe that in the last clause he was discouraging the pursuit of simplicity to the extent of distorting reality. So why does the book contain 30 chapters and two appendixes? To answer this question, it is necessary to look at the history of research and development of the relational model.},
address = {Reading, Massachusetts},
author = {Codd, Edgar Frank},
booktitle = {Database},
edition = {Version 2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Codd - 1990 - The Relational Model for Database Management Version 2.pdf:pdf},
isbn = {0201141922},
pages = {538},
pmid = {1917145},
publisher = {Addison-Wesley Publishing Company, Inc.},
title = {{The Relational Model for Database Management : Version 2}},
url = {https://codeblab.com/wp-content/uploads/2009/12/rmdb-codd.pdf http://books.google.co.kr/books?q=9780201141924},
year = {1990}
}
@inproceedings{Kaur2016,
abstract = {Now a days everything around the globe is connected via networks like information, places and events which make a tangle of connections. Analyzing social network is to make sense of these complex connections. This work represents the framework to analyze twitter social media tweets using NetworkX and Twitter API. Python language tool IPython/Jupyter is used to examine the networks by applying visual analytic techniques like degree centrality and betweenness centrality to the dataset of twitter hashtags which provides an easier way to analyze the network connections. This framework describes methodology to diagnose each tweet for identification of certain pattern like 'who talk to whom about what' and 'most influential person' in the interconnected/attached network.},
author = {Kaur, Navpreet and Singh, Maninder and Singh, V. P.},
booktitle = {Proceedings of the International Conference on Inventive Computation Technologies, ICICT 2016},
doi = {10.1109/INVENTIVE.2016.7830086},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kaur, Singh, Singh - 2016 - Design and develop A framework for social networking analysis.pdf:pdf},
isbn = {978-1-5090-1285-5},
keywords = {Centralities,Python framework,Social networking analysis,Twitter},
month = {aug},
pages = {1--6},
publisher = {IEEE},
title = {{Design and develop A framework for social networking analysis}},
url = {http://ieeexplore.ieee.org/document/7830086/},
volume = {2016},
year = {2016}
}
@techreport{IEEEAuthorshipSeries,
abstract = {You will learn how to prepare, write, and submit your manuscript for peer review by an ieee conference, journal, or magazine. We will show you how successful authors structure quality work to improve their chances of being accepted. You will find practical tips on how to select an appropriate periodical or conference, organize your manuscript, write in a clear and grammatically correct style, and work through peer review. You will also learn how to avoid common mistakes and ethical lapses that will prevent your manuscript from being accepted and may damage your reputation. Publishing is central to the mission of ieee: to foster technological innovation and excellence for the benefit of humanity. ieee provides high quality, innovative information by attracting the best authors and supporting them through the publishing process. a Web-based workflow and tools such as reference validation, graphics checking, and templates streamline the submission process.},
author = {{IEEE Authorship Series}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/IEEE Authorship Series - 2018 - How to Write for Technical Periodicals and Conferences.pdf:pdf},
institution = {IEEE},
pages = {32},
title = {{How to Write for Technical Periodicals and Conferences}},
url = {http://ieeeauthorcenter.ieee.org/wp-content/uploads/How-to-Write-for-Technical-Periodicals-and-Conferences.pdf},
year = {2018}
}
@article{Efron1979,
abstract = {We discuss the following problem: given a random sample X = (X1, X2, ⋯, Xn) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = $\theta$(F̂) - $\theta$(F), $\theta$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
author = {Efron, B},
issn = {00905364},
journal = {The Annals of Statistics},
number = {1},
pages = {1--26},
publisher = {Institute of Mathematical Statistics},
title = {{Bootstrap Methods: Another Look at the Jackknife}},
url = {http://www.jstor.org/stable/2958830},
volume = {7},
year = {1979}
}
@book{Hoffman2019,
abstract = {Do we see the world as it truly is? In The Case Against Reality, pioneering cognitive scientist Donald Hoffman says no? we see what we need in order to survive. Our visual perceptions are not a window onto reality, Hoffman shows us, but instead are interfaces constructed by natural selection. The objects we see around us are not unlike the file icons on our computer desktops: while shaped like a small folder on our screens, the files themselves are made of a series of ones and zeros - too complex for most of us to understand. In a similar way, Hoffman argues, evolution has shaped our perceptions into simplistic illusions to help us navigate the world around us. Yet now these illusions can be manipulated by advertising and design.},
address = {London, UK},
author = {Hoffman, Donald D.},
isbn = {9780241262627},
pages = {272},
publisher = {Penguin},
title = {{Case Against Reality : how evolution hid the truth from our eyes.}},
year = {2019}
}
@article{Cox2018,
abstract = {The last 100 years have seen a huge change in the global structure of the human population, with the majority of people now living in urban rather than rural environments. An assumed consequence is that people will have fewer experiences of nature, and this could have important consequences given the myriad health benefits that they can gain from such experiences. Alternatively, as experiences of nature become rarer, people might be more likely actively to seek them out, mitigating the negative effects of urbanisation. In this study, we used data for 3000 survey respondents from across the UK, and a nature-dose framework, to determine whether (a) increasing urbanisation is associated with a decrease in the frequency, duration and intensity of nature dose; and (b) differences in nature exposure associated with urbanisation impact on four population health outcomes (depression, self-reported health, social cohesion and physical activity). We found negative exponential relationships between nature dose and the degree of urbanisation. The frequency and duration of dose decreased from rural to suburban environments, followed by little change with further increases in urbanisation. There were weak but positive associations between frequency and duration of dose across all four health domains, while different dimensions of dose showed more positive associations with specific health domains in towns and cities. We show that people in urban areas with a low nature dose tend to have worse health across multiple domains, but have the potential for the greatest gains from spending longer in nature, or living in green areas.},
author = {Cox, Daniel T.C. and Shanahan, Danielle F. and Hudson, Hannah L. and Fuller, Richard A. and Gaston, Kevin J.},
doi = {10.1016/j.landurbplan.2018.07.013},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cox et al. - 2018 - The impact of urbanisation on nature dose and the implications for human health.pdf:pdf},
issn = {01692046},
journal = {Landscape and Urban Planning},
keywords = {Connectedness to nature,Exposure to nature,Human-nature interactions,Nature orientation,Nature relatedness scale,Rural-urban gradient},
month = {nov},
pages = {72--80},
publisher = {Elsevier B.V.},
title = {{The impact of urbanisation on nature dose and the implications for human health}},
volume = {179},
year = {2018}
}
@unpublished{Network2014,
abstract = {R Spatial Reference Card},
author = {Lovelace, Robin and Oldroyd, Rachel},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lovelace, Oldroyd - 2014 - R Spatial Reference Card Spatial subsetting Spatial aggregation.pdf:pdf},
pages = {1--2},
title = {{R Spatial Reference Card Spatial subsetting Spatial aggregation}},
year = {2014}
}
@inproceedings{Chu2015,
abstract = {Classical approaches to clean data have relied on using integrity constraints, statistics, or machine learning. These approaches are known to be limited in the cleaning accuracy, which can usually be improved by consulting master data and involving experts to resolve ambiguity. The advent of knowledge bases KBs both general-purpose and within enterprises, and crowdsourcing marketplaces are providing yet more opportunities to achieve higher accuracy at a larger scale. We propose KATARA, a knowledge base and crowd powered data cleaning system that, given a table, a KB, and a crowd, interprets table semantics to align it with the KB, identifies correct and incorrect data, and generates top-k possible repairs for incorrect data. Experiments show that KATARA can be applied to various datasets and KBs, and can efficiently annotate data and suggest possible repairs.},
address = {New York, New York, USA},
author = {Chu, Xu and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Tang, Nan and Ye, Yin},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data - SIGMOD '15},
doi = {10.1145/2723372.2749431},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Chu et al. - 2015 - KATARA A Data Cleaning System Powered by Knowledge Bases and Crowdsourcing.pdf:pdf},
isbn = {9781450327589},
keywords = {Big Data,crowdsourcing,data cleaning,data quality,knowledge base},
mendeley-tags = {Big Data},
pages = {1247--1261},
publisher = {ACM Press},
title = {{KATARA: A Data Cleaning System Powered by Knowledge Bases and Crowdsourcing}},
url = {http://dl.acm.org/citation.cfm?doid=2723372.2749431},
year = {2015}
}
@article{Guazzelli2009,
abstract = {The PMML package exports a variety of predic- tive and descriptive models from R to the Predic- tive Model Markup Language (Data Mining Group, 2008). PMML is an XML-based language and has become the de-facto standard to represent not only predictive and descriptive models, but also data pre- and post-processing. In so doing, it allows for the interchange of models among different tools and en- vironments, mostly avoiding proprietary issues and incompatibilities.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Guazzelli, Alex and Zeller, Michael and Lin, Wc and Williams, Graham},
doi = {doi=10.1.1.538.9778},
eprint = {1011.1669},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Guazzelli et al. - 2009 - PMML An open standard for sharing models.pdf:pdf},
isbn = {2073-4859},
issn = {2073-4859},
journal = {The R Journal},
number = {1},
pages = {60--65},
pmid = {12948721},
title = {{PMML: An open standard for sharing models}},
url = {http://www.dmg.org http://bocowgill.com/R{\_}PMML{\_}article.pdf},
volume = {1},
year = {2009}
}
@article{Bingemer2017,
abstract = {The present paper aims at providing a practical identification tool for soil zoologists. It shall facilitate taxonomic examination of tardigrade communities in order to encourage further investigations and by this expand our scarce knowledge on soil tardigrades. From faunistic studies on soil tardigrades a list of the eutardigrade species presently known from European soils was gathered comprising 22 genera, 58 species, 3 species groups. Based on the most important standard works and on up-to-date nomenclature an illustrated key to the eutardigrade genera of European soils was created. Genus descriptions and identification keys to the soil species were added while those genera that hold only one or two soil species were accomplished with short species descriptions. Additional information is given on the relevant determination features, such as claws and bucco-pharyngeal apparatus. Difficulties in tardigrade identification and taxonomy are discussed. Due to the comparably small number of studies that so far exist on soil tardigrades, the key will most likely not cover all species present in European soils, but shall provide a basis to facilitate further research.},
author = {Bingemer, Jana and Hohberg, Karin},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bingemer, Hohberg - 2017 - An illustrated identification key to the eutardigrade species (Tardigrada, Eutardigrada) presently known from.pdf:pdf},
issn = {2509-9523},
journal = {Soil Organisms},
keywords = {Eutardigrada |,Identification key |,Soil tardigrades},
number = {3},
pages = {127--149},
title = {{An illustrated identification key to the eutardigrade species (Tardigrada, Eutardigrada) presently known from European soils}},
url = {www.soil-organisms.de http://www.senckenberg.de/files/content/forschung/publikationen/soilorganisms/volume{\_}89{\_}3/89-3-01{\_}bingemerhohberg.pdf},
volume = {89},
year = {2017}
}
@book{Mayer-Schonberger2013,
abstract = {"Explores the idea of big data, which refers to our newfound ability to crunch vast amounts of information, analyze it instantly, and draw profound and surprising conclusions from it. Now -- More -- Messy -- Correlation -- Datafication -- Value -- Implications -- Risks -- Control -- Next.},
author = {Mayer-Schonberger, Viktor. and Cukier, Kenneth.},
isbn = {9781848547926},
pages = {242},
publisher = {Houghton Mifflin Harcourt},
title = {{Big data: a revolution that will transform how we live, work, and think}},
url = {https://www.oii.ox.ac.uk/research/books/big-data/},
year = {2013}
}
@article{EU-RRIinfo2012,
abstract = {INTRODUCTORY SPEECH, ANNOUNCING THIS POLICY “The dialogue between science and the rest of society has never been more important. As the Europe 2020 Strategy makes clear, to overcome the current economic crisis we need to create a smarter, greener economy, where our prosperity will come from research and innovation. Science is the basis for a better future and the bedrock of a knowledge-based society and a healthy economy. A er ten years of action at EU level to develop and promote the role of science in society, at least one {\{}Faden, 1986 {\#}3917{\}}thing is very clear: we can only nd the right answers to the challenges we face by involving as many stakeholders as possible in the research and innovation process. Research and innovation must respond to the needs and ambitions of society, re ect its values, and be responsible. To my mind, there are a number of keys to doing this”. MÁIRE GEOGHEGAN-QUINN, European Commissioner for Research, Innovation and Science Message delivered at the conference «Science in Dialogue - Towards a European Model for Responsible Research and Innovation» Odense, Denmark, 23-25 April 2012},
address = {Odense, Denmark},
annote = {Document in Road2CPS.

PREFACE
"The Directorate-General for Research and Innovation of the European Commission is determined to bridge the gap between the scienti c community and society at large. In 2001, the «Science and Society» Action Plan was launched to set out a common strategy to make a better connection between science and European citizens. In 2007, under the 7th Framework Programme for Research and Technological Development (FP7), «Science and Society» became «Science in Society (SiS)» with the main objective to foster public engagement and a sustained two-way dialogue between science and civil society.

Since 2010 the focus of SiS has been to develop a concept responding to the aspirations and ambitions of European citizens: a framework for Responsible Research and Innovation (RRI). The grand societal challenges that lie before us will have a far better chance of being tackled if all societal actors are fully engaged in the co-construction of innovative solutions, products and services.

Responsible Research and Innovation means that societal actors work together during the whole research and innovation process in order to better align both the process and its outcomes, with the values, needs and expectations of European society. RRI is an ambitious challenge for the creation of a Research and Innovation policy driven by the needs of society and engaging all societal actors via inclusive participatory approaches.

The Responsible Research and Innovation framework consists of 6 keys:"

1 Choose together: Engagement of all socie- tal actors - researchers, industry, policy- makers and civil society – and their joint participation in the research and innovation process, in accordance with the value of inclusiveness,asre ectedintheCharterof FundamentalRightsoftheEuropeanUnion

2 Unlock the full potential: Gender equality in research institutions

3 Creative learning, fresh ideas: Science Education. Europe must not only increase its number of researchers, it also needs to enhance the current education process to better equip future researchers and other socie- tal actors with the necessary knowledge and tools to fully participate and take responsibility in the research and innova- tion process.

4 Share results to advance: In order to be responsible, research and innovation must be both transparent and accessible.Our fourth key is to make Open Access a reality. This means giving free online access to the results of publicly-funded research (publications and data).

5 Do the right 'think' and do it right: Ethics. European society is based on shared values. In order to adequately respond to societal challenges, research and inno- vation must respect fundamental rights and the highest ethical standards. Beyond the mandatory legal aspects, this aims to ensure increased societal relevance and acceptability of research and innovation outcomes. Ethics should not be perceived as a constraint to research and innova- tion, but rather as a way of ensuring high quality results.

6 designscience for and with society: Governance; the umbrella for the other 5 keys. Policymakers also have a responsibility to prevent harmful or unethical develop- ments in research and innovation. Through this key we will develop harmo- nious models for Responsible Research and Innovation that integrate public enga- gement, gender equality, science educa- tion, open access and ethics.},
author = {Geoghegan-Quinn, M},
doi = {10.2777/11739},
journal = {Proceedings of the Science in Dialogue--Towards a European Model for Responsible Research and Innovation},
keywords = {Cyber-physical systems,Governance,contracts,corporate governance,design process,ethics,green,legal,legislation,methods,papers,philosophy,projects,quotes,safety,security,strategy,sustainability},
pages = {23--25},
publisher = {ECDG Research and Innovation},
title = {{Responsible Research and Innovation - Europe's ability to respond to societal challenges}},
year = {2012}
}
@techreport{Sherley2016,
abstract = {DOCCM-248862 Invertebrates: pitfall trapping v1.0 2 Inventory and monitoring toolbox: invertebrates Synopsis Pitfall trapping is commonly used for capturing invertebrates that are active on the ground. As the name suggests, the trap works on the principle that an invertebrate moving on the ground simply falls into an open (usually circular) container dug into the ground. The invertebrates are usually killed in a preservative solution and are taken back to a laboratory for later sorting and identification. Pitfall traps are generally unspecific in what they capture, although they can sometimes be modified to targeted specific groups (e.g. by baiting). Typically, large numbers of invertebrates are caught in any one session—usually large-bodied, mobile species such as predatory ground beetles (Carabidae), rove beetles (Staphylinidae), wandering spiders (e.g. lycosids, clubionids) and ants. Pitfall trapping measures the activity of species that may be susceptible to capture in pitfall traps. It can be used to estimate an index of relative abundance from one time to another or between different places. Such indices have different relationships to the actual abundance of different species because they are influenced by a huge range of factors such as meteorological conditions, time of year, sex and developmental stage of the invertebrate, etc. As a result, when pitfall traps are used to monitor changes in numbers of individuals then each species should be treated individually (i.e. such studies should be intra-specific). The technique is also sensitive to variations in how the traps are set so the quality of the data depends on paying attention to the details of how to set the traps (e.g. the diameter of the pitfall trap affects the size and number of invertebrates caught; lips of traps left projecting above the soil surface will reduce catch rates). Pitfall trapping is suitable for studies of community richness (number of species present) or habitat assessments of mainly surface-dwelling invertebrates, but be aware that other trapping methods may reveal an entirely different range of invertebrates at a given locality. Pitfall trapping has been extensively used in forested habitats in New Zealand, whereas elsewhere it has been considered ideal for open habitats such as grasslands and arable land. There is extensive literature and the reader is directed initially to New (1998), Southwood {\&} Henderson (2000), and Leather (2005) for reviews. Two case studies (Pawson et al. 2008; Sinclair et al. 2005) are provided to demonstrate the use of pitfall traps to examine invertebrate fauna. Other examples that include distributions are studies by Stevens et al. (2007) and McCartney et al. (2007), and a good example of an inventory study is outlined in Winks et al. (2004). Pitfall trapping has been used in the following types of studies:},
address = {New Zealand},
author = {Sherley, Greg and Stringer, Ian},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sherley, Stringer - 2016 - Invertebrates pitfall trapping.pdf:pdf},
pages = {1--30},
title = {{Invertebrates : pitfall trapping}},
url = {https://www.doc.govt.nz/Documents/science-and-technical/inventory-monitoring/im-toolbox-invertebrates-pitfall-trapping.pdf},
year = {2016}
}
@inproceedings{Kandel2011,
abstract = {Though data analysis tools continue to improve, analysts still expend an inordinate amount of time and effort manip-ulating data and assessing data quality issues. Such " data wrangling " regularly involves reformatting data values or layout, correcting erroneous or missing values, and integrat-ing multiple data sources. These transforms are often dif-ficult to specify and difficult to reuse across analysis tasks, teams, and tools. In response, we introduce Wrangler, an interactive system for creating data transformations. Wran-gler combines direct manipulation of visualized data with automatic inference of relevant transforms, enabling ana-lysts to iteratively explore the space of applicable operations and preview their effects. Wrangler leverages semantic data types (e.g., geographic locations, dates, classification codes) to aid validation and type conversion. Interactive histories support review, refinement, and annotation of transformation scripts. User study results show that Wrangler significantly reduces specification time and promotes the use of robust, auditable transforms instead of manual editing.},
address = {Vancouver, BC, Canada.},
author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
booktitle = {ACM Human Factors in Computing Systems (CHI)},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kandel et al. - 2011 - Wrangler Interactive Visual Specification of Data Transformation Scripts.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {10},
title = {{Wrangler: Interactive Visual Specification of Data Transformation Scripts}},
url = {http://vis.stanford.edu/files/2011-Wrangler-CHI.pdf},
year = {2011}
}
@unpublished{Sinclair2016,
abstract = {What is important is the quality of the data involved (i.e. knowing its provenance). There will be excluded data, because of IPRs and confidentiality agreements, implied measures, unsynchronized data, and all the issues of permits to access data (ownership, privacy, security, protection, safety). Then},
author = {Sinclair, Murry},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sinclair - 2016 - 6C ' s of big data.pdf:pdf},
institution = {Loughborough University},
pages = {6},
title = {{6C ' s of big data}},
year = {2016}
}
@techreport{Mikkola,
abstract = {During the last decades of the 20 th century, a new step to the traditional way of preparing lepidopteran microscope slides was widely adopted, the technique of inflating and fixing the soft parts of internal genitalia. The taxo-nomic resolution in revising problematic species groups, particularly allopatric relationships, improved considerably. At the same time, the so-called lock-and-key hypothesis has been revived, since usually the sexes show corresponding structural details in their internal genitalia. On the basis of intercontinental studies on the Noctuidae, it is considered that the divergence of internal genitalia in pairs of sister species is based on genetic drift. The history of the eversion technique is revised. El aumento de las t{\'{e}}cnicas de eversi{\'{o}}n en la taxonom{\'{i}}a de los lepid{\'{o}}pteros (Insecta: Lepidoptera) Resumen Durante las {\'{u}}ltimas d{\'{e}}cadas del siglo veinte, un nuevo paso para la manera tradicional de realizar las prepara-ciones microsc{\'{o}}picas de los lepid{\'{o}}pteros fue adoptada extensamente, la t{\'{e}}cnica de inflar y adaptar las zonas blandas de las genitalias internas. Mejora considerablemente, la revisi{\'{o}}n de la soluci{\'{o}}n taxon{\'{o}}mica en los grupos de especies problem{\'{a}}ticas, particularmente las relaciones alop{\'{a}}tricas. Al mismo tiempo, se ha revisado la conocida hip{\'{o}}tesis llave-cerradura, ya que generalmente los sexos muestran los detalles estructurales correspondientes en sus genitalias. Sobre la base de los estudios intercontinentales de los Noctuidae, se considera que la divergencia de sus genitalias en especies gemelas, est{\'{a}} basada en la tendencia gen{\'{e}}tica. Se revisa la historia de la t{\'{e}}cnica de la eversi{\'{o}}n.},
author = {Mikkola, K},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mikkola - Unknown - The rise of eversion techniques in lepidopteran taxonomy (Insecta Lepidoptera).pdf:pdf},
keywords = {Insecta,Lepidoptera,bursa de la hembra,female bursa,genitalia interna,internal genitalia,llave-cerradura,lock-and-key,male vesica,microscopic slides PALABRAS CLAVE: Insecta,preparaci{\'{o}}n microsc{\'{o}}pica,vesica del macho},
title = {{The rise of eversion techniques in lepidopteran taxonomy (Insecta: Lepidoptera)}}
}
@article{teh2001statisticalsubassembly,
abstract = {This paper reports work from a major UK research project which seeks to develop a cost effective integrated production technology for the manufacture of plastic automotive components with embedded electronics and power distribution. The said electronics sub-systems are simultaneously packaged within the automotive structural thermoplastic (TP) component during the injection moulding process. Such innovative process and product integration represents an opportunity to significantly reduce component counts and wiring loom overhead for the vehicle. The assembly, weight and cost advantages envisaged from the proposed technology will satisfy the ever-increasing demand for automotive suppliers to manufacture complete, ready-to-assemble, reconfigurable component modules with superior reliability and as such may contribute to the increased incorporation of vehicle telematics. The paper addresses the technological and economic implications of the proposed overmoulding technology, presenting particular aspects of technological hurdless such as thermal, electrical and process requirements in the production of highly integrated polymer encapsulated electronics products. This paper highlights the findings derived from experiments undertaken to explore critical factors in the manufacture of such encapsulated electronics sub-systems, including such variables as electronics interconnection, materials selection and interactions and moulding process parameters.},
annote = {From Duplicate 1 (Statistical Optimisation of Thermoplastic Injection Moulding Process for the Encapsulation of Electronic Subassembly - Teh, N J; Conway, P P; Palmer, P J; Kioul, A; Prosser, S)

Encapsulated electronics is becoming a major technology theme within the research group with significant industrial applications.},
author = {Teh, N J and Conway, P P and Palmer, P J and Prosser, S and Kioul, A and Prosser, S},
doi = {10.1142/S0960313100000150},
issn = {0960-3131},
journal = {Journal of Electronics Manufacturing},
month = {aug},
number = {(3)},
pages = {171--179},
title = {{Statistical Optimisation of Thermoplastic Injection Moulding Process for the Encapsulation of Electronic Subassembly}},
url = {http://dx.doi.org/10.1142/S0960313100000150},
volume = {10},
year = {2000}
}
@inproceedings{Singh2017,
abstract = {Entity matching (EM) is a critical part of data integration and cleaning. In many applications, the users need to un- derstand why two entities are considered a match, which reveals the need for interpretable and concise EM rules. We model EM rules in the form of General Boolean Formulas (GBFs) that allows arbitrary attribute matching combined by conjunctions ({\v{Z}}), disjunctions ({\'{Z}}), and negations (?). GBFs can generate more concise rules than traditional EM rules represented in disjunctive normal forms (DNFs). We use program synthesis, a powerful tool to automatically gen- erate rules (or programs) that provably satisfy a high-level specification, to automatically synthesize EM rules in GBF format, given only positive and negative matching examples. In this demo, attendees will experience the following fea- tures: (1) Interpretability – they can see and measure the conciseness of EM rules defined using GBFs; (2) Easy cus- tomization – they can provide custom experiment parame- ters for various datasets, and, easily modify a rich predefined (default) synthesis grammar, using aWeb interface; and (3) High performance – they will be able to compare the gener- ated concise rules, in terms of accuracy, with probabilistic models (e.g., machine learning methods), and hand-written EM rules provided by experts. Moreover, this system will serve as a general platform for evaluating different methods that discover EM rules, which will be released as an open- source tool on GitHub.},
author = {Singh, Rohit and Meduri, Vamsi and Elmagarmid, Ahmed and Madden, Samuel and Papotti, Paolo and Quian{\'{e}}-Ruiz, Jorge-Arnulfo and Solar-Lezama, Armando and Tang, Nan},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data - SIGMOD '17},
doi = {10.1145/3035918.3058739},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Singh et al. - 2017 - Generating Concise Entity Matching Rules.pdf:pdf},
isbn = {9781450341974},
issn = {07308078},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {1635--1638},
title = {{Generating Concise Entity Matching Rules}},
url = {https://dl.acm.org/citation.cfm?id=3058739 http://dl.acm.org/citation.cfm?doid=3035918.3058739},
year = {2017}
}
@article{Paterson2016,
abstract = {he Natural History Museum, London (NHMUK) has embarked on an ambitious programme to digitise its collections . The first phase of this programme has been to undertake a series of pilot projects that will develop the necessary workflows and infrastructure development needed to support mass digitisation of very large scientific collections. This paper presents the results of one of the pilot projects – iCollections. This project digitised all the lepidopteran specimens usually considered as butterflies, 181,545 specimens representing 89 species from the British Isles and Ireland. The data digitised includes, species name, georeferenced location, collector and collection date - the what, where, who and when of specimen data. In addition, a digital image of each specimen was taken. This paper explains the way the data were obtained and the background to the collections which made up the project. New information Specimen-level data associated with British and Irish butterfly specimens have not been available before and the iCollections project has released this valuable resource through the NHM data portal.},
author = {Paterson, Gordon and Albuquerque, Sara and Blagoderov, Vladimir and Brooks, Stephen and Cafferty, Steve and Cane, Elisa and Carter, Victoria and Chainey, John and Crowther, Robyn and Douglas, Lyndsey and Durant, Joanna and Duffell, Liz and Hine, Adrian and Honey, Martin and Huertas, Blanca and Howard, Theresa and Huxley, Rob and Kitching, Ian and Ledger, Sophie and McLaughlin, Caitlin and Martin, Geoff and Mazzetta, Gerardo and Penn, Malcolm and Perera, Jasmin and Sadka, Mike and Scialabba, Elisabetta and Self, Angela and Siebert, Darrell and Sleep, Chris and Toloni, Flavia and Wing, Peter},
doi = {10.3897/BDJ.4.e9559},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Paterson et al. - 2016 - iCollections – Digitising the British and Irish Butterflies in the Natural History Museum, London.pdf:pdf},
issn = {1314-2828},
journal = {Biodiversity Data Journal},
number = {4},
pages = {e9559},
title = {{iCollections – Digitising the British and Irish Butterflies in the Natural History Museum, London}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5136670/pdf/biodiversity{\_}data{\_}journal-4-e9559.pdf http://bdj.pensoft.net/articles.php?id=9559},
volume = {4},
year = {2016}
}
@article{Keim2002,
author = {Keim, D.A.},
doi = {10.1109/2945.981847},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Keim - 2002 - Information visualization and visual data mining.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {1},
pages = {1--8},
title = {{Information visualization and visual data mining}},
url = {http://ieeexplore.ieee.org/document/981847/},
volume = {8},
year = {2002}
}
@article{webb2010designharnesses,
author = {Webb, D P and Jaggernauth, W A and Cottrill, M C W and Palmer, P J and West, A A and Conway, P P},
doi = {10.1243/09544070JAUTO1175},
issn = {2041-2991},
journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
number = {(D6)},
pages = {785--797},
title = {{Design and construction of large-area flexible printed-circuit automotive electrical interconnection harnesses}},
volume = {224},
year = {2010}
}
@incollection{Rieder,
address = {Wiesbaden},
author = {Rieder, G and Simon, J},
booktitle = {Berechenbarkeit der Welt?},
doi = {10.1007/978-3-658-12153-2_4},
editor = {Pietsch, W. and Wernecke, J. and Ott, M.},
file = {:Users/TechTrends/Documents/Temp/PDF/2017{\_}Big{\_}Data{\_}A{\_}New{\_}Empiricism{\_}and{\_}its{\_}E.pdf:pdf},
isbn = {978-3-658-12153-2},
publisher = {Springer VS,},
title = {{Big Data: A New Empiricism and its Epistemic and Socio-Political Consequences}}
}
@article{Johansson2010,
author = {Johansson, Sara and Johansson, Jimmy},
doi = {10.1145/1809400.1809406},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Johansson, Johansson - 2010 - Visual analysis of mixed data sets using interactive quantification.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
month = {may},
number = {2},
pages = {29},
publisher = {ACM},
title = {{Visual analysis of mixed data sets using interactive quantification}},
url = {http://portal.acm.org/citation.cfm?doid=1809400.1809406},
volume = {11},
year = {2010}
}
@book{Creswell2018,
address = {London, UK},
annote = {Own Kindle Copy.},
author = {Creswell, John W. and Creswell, J. David},
edition = {5th},
isbn = {978-1506386768},
publisher = {SAGE Publications Inc},
title = {{Research Design: Qualitative, Quantitative, and Mixed Methods Approaches}},
year = {2018}
}
@book{Baddeley2015,
abstract = {Spatial Point Patterns: Methodology and Applications with R shows scientific researchers and applied statisticians from a wide range of fields how to analyze their spatial point pattern data. Making the techniques accessible to non-mathematicians, the authors draw on their 25 years of software development experiences, methodological research, and broad scientific collaborations to deliver a book that clearly and succinctly explains concepts and addresses real scientific questions.},
author = {Baddeley, Adrian and Rubak, Ege and Turner, Rolf},
edition = {1st},
isbn = {9781482210200},
pages = {810},
publisher = {Chapman and Hall/CRC},
title = {{Spatial point patterns : methodology and applications with R}},
year = {2015}
}
@article{Lin2011,
abstract = {There is a growing awareness that high quality of data is a key to today's business success and that dirty data existing within data sources is one of the causes of poor data quality. To ensure high quality data, enterprises need to have a process, methodologies and resources to monitor and analyze the quality of data, methodologies for preventing and/or detecting and repairing dirty data. Nevertheless, research shows that many enterprises do not pay adequate attention to the existence of dirty data and have not applied useful methodologies to ensure high quality data for their applications. One of the reasons is a lack of appreciation of the types and extent of dirty data. In practice, detecting and cleaning all the dirty data that exists in all data sources is quite expensive and unrealistic. The cost of cleaning dirty data needs to be considered for most of enterprises. This problem has not attracted enough attention from researchers. In this paper, a rule-based taxonomy of dirty data is developed. The proposed taxonomy not only provides a mechanism to deal with this problem but also includes more dirty data types than any of existing such taxonomies.},
annote = {Links to really interesting looking papers on data cleaning},
author = {Lin, L. and Peng, T and Kennedy, J.},
doi = {10.5176/2010-2283_1.2.52},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Peng, Kennedy - 2011 - A rule based taxonomy of dirty data.pdf:pdf},
issn = {20102283},
journal = {Journal on Computing},
keywords = {Big Data,QA75 Electronic computers. Computer science},
mendeley-tags = {Big Data},
number = {2},
pages = {140--148},
title = {{A rule based taxonomy of dirty data}},
url = {http://dl6.globalstf.org/index.php/joc/article/download/931/864 http://researchrepository.napier.ac.uk/3887/},
volume = {1},
year = {2011}
}
@techreport{Durrant2016,
abstract = {This paper discusses the challenges faced by creative teams when they come together to develop new creative IP. This consideration is set in the context of the UK Games Fund and other projects run by UK Games Talent and Finance CIC (UKGTF). As a not-for-profit, Community Interest Company UKGTF is seeking to improve the UK's early stage ecosystem for video games developers. The paper focuses on a new business process that UKGTF is experimenting with allowing a new type of IP share called a Tal. Tals can be issued by small teams of creatives so that company formation can be delayed until the potential business has a clearer line of sight to success whilst still maintaining a record of intent. The purpose of this business process is to set new creative projects up with a cost effective way of tracking the founding creators' shares (as Tals) and other stakeholders associated with the business so that the transactions could be recorded via a distributed ledger of some sort. This would then allow the Project Proof to define revenue shares at any point, based on Tal distribution. UKGTF is piloting this with the Digital Catapult1 on the Tranzfuser2 graduate games development project. The Digital Catapult's open innovation approach is seen as critical to support our SME community. The paper notes how the concept of “smart contracts” often associated with this type of activity is somewhat of a misnomer, with UKGTF electing to describe the process as Project Proofs. The “human factors” in creative team formation are summarised with a list of issues that creative teams should resolve at the outset. The paper describes a vision for a more connected ecosystem for creative teams connected by Project Proofs and building measures of integrity through contractual performance. The paper does not set out to address the most effective type of distributed ledger to use for this application though does provide a summary of the user requirements for the pilot project, the rules for Tals used in the pilot and details of the Tal Registry3 established.},
annote = {Document in Contracts. See also Tennison, explaining blockchain construction. 

But theere's still a lot I don't understand about this. For example:1 If a block only contains a hash value for a document, where is the document held, and how do you reference back to it? How do you know this link is reliable? Unless the document is part of the shared ledger? but then have you revealed your IP? Could be encrypted, of course, 
2 Why does it take time to calculate a hash value?
3 Can you have a hash value for a device or a drawing? Note that the hash value exists in the virtual world, whereas the device exists in the real world. So you would need a blockchain for maintenance. If this contained a hash for V{\&}V as well, this might be trustworthy, meaning that run-time V{\&}V might be much simplified.
4 Might enable us to know when an autonomous device has changed its knowledge base, too. Get it to compute a hash value hourly ofr faster, to knowe when its knowledge base has changed.
5 If you have a number of etities that are subsumed into a block, (e.g. a bunch of design documents), does each document get a hash (to prove it hasn't changed, and then all the hashes are re-hashed, to generate the hash for the block?

See also BlockGeeks {\#}4253, Tennison {\#}4235

Prepared as a part of the Digital Catapult, to rnsable creative groups to copyright their work (IP), to enable them to get paid by uswrs of their copyright, and have no 3rd person involved. Appendix 4 has a scenario as an example.Important term here is 'smart contract', because the contract verifiably records changes as time passes by. Paragraph below explainds the problem:
"Independent creative teams (such as games developers) planning to create new, original IP come together through various serendipitous routes. There may or may not be a fully shared vision of the project they aspire to tackle together at the outset. The individuals capable of delivering the project may be present at the start, or they might join along the way. Throughout the creative process there's also significant potential for change through both positive and negative events. There may be key individuals without whose talent and creativity the project will never achieve greatness or whose loss mid-project will leave the remaining team high and dry. The work itself may include third-party assets or content that have distinct rights that require to be embodied in the finished work. In addition to all of these challenges the speculative outputs from creative teams are pitched into hit-driven markets where commercial success is not assured."
1 See https://www.digitalcatapultcentre.org.uk
2 See https://twitter.com/search?q={\%}23tranzfuser{\&}src=hash 3 See www.talregistry.com

"This paper primarily focuses on developing the business process that could allow creative teams to harness distributed ledger technology and create a Project Proof. It's not really clear quite yet what the ideal distributed ledger solution will be for this. However, it is fairly clear that there will be a solution – and the easier it is for a creative team to plug a well-defined business process into the emergent solution the better. UKGTF has been fortunate to identify the Digital Catapult as a partner for this work. There is a significant innovation challenge for SMEs working in the digital space both jointly (as in the context of the community that UKGTF serves) and severally for small companies pioneering their own ideas. UKGTF falls into both these definitions as we want to serve our community and we also want to innovate as a company in our own right. The Digital Catapult represents a highly significant, expert innovation resource and is also ideally placed to convene4 the key players to drive the critical mass needed to build scale around new ideas. For the time being the underlying technology under development at the Digital Catapult uses Ethereum5."
4 We are particularly grateful to Matt Ward, Senior Manager Creative Programmes at the Digital Catapult for his oversight of this work 
5 https://www.ethereum.org/

Relevance to manufacturing:
"Ultimately, this thinking can be applied to any project that requires a group of individuals to undertake work towards a single output formed from their blended inputs. For now, the focus is on the creative digital industries where the complexity of team collaboration across different disciplines is growing in areas such as VR and cross-media, with a heightened need for early stage experimentation and pre-visualisation as well as digital-to-physical such as 3D printing. However, our proposed approach could apply to a group collaborating to undertake pretty much anything where there's a complex blend of inputs and unpredictable outputs and outcomes. The overarching aim is to provide an open solution built around trusted standards, available as an “out of the box” solution for creative teams to adopt and utilise to drive democratised creative team work."
[In a CPSoS lots og IP owners come together in perhps sort-term collaboration were the configuration happens at run-time; there needs to be a contract both to bind and pay the players, and also to ensure that tracking for liability claims is available. This seems to be a step in the required direction]



Durrant/Hogsrth {\#}4234},
author = {Durrant, P and Hogarth, M},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Durrant, Hogarth - 2016 - Tal TM Stories A new concept for creative teams to share project ownership and revenues.pdf:pdf},
keywords = {Blockchain},
title = {{Tal TM  Stories: A new concept for creative teams to share project ownership and revenues}},
url = {http://nebula.wsimg.com/c89a91d3e60d56b45efe7fc409c41105?AccessKeyId=C527A018C673F95B6BF8{\&}disposition=0{\&}alloworigin=1},
year = {2016}
}
@article{Pearl2019,
abstract = {We demonstrate how counterfactuals can be used to compute the probability that one event was/is a sufficient cause of another, and how counterfactuals emerge organically from basic scientific knowledge, rather than manipulative experiments. We contrast this demonstration with the potential outcome framework and address the distinction between causes and enablers.},
author = {Pearl, Judea},
doi = {10.1515/jci-2019-0026},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2019 - Sufficient causes On oxygen, matches, and fires.pdf:pdf},
issn = {21933685},
journal = {Journal of Causal Inference},
keywords = {Abduction,Counterfactuals,Explanation,Sufficient causes},
number = {2},
pages = {8},
title = {{Sufficient causes: On oxygen, matches, and fires}},
volume = {7},
year = {2019}
}
@article{Eck2013,
abstract = {1 January 2013},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Eck, Nees Jan Van and Waltman, Ludo},
doi = {10.3402/jac.v8.30072},
eprint = {0803.1716},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Eck, Waltman - 2013 - VOSviewer Manual.pdf:pdf},
isbn = {0305-1862},
issn = {01389130},
journal = {1 January 2013},
number = {January},
pages = {1--28},
pmid = {502955140},
title = {{VOSviewer Manual}},
url = {http://www.vosviewer.com/documentation/Manual{\_}VOSviewer{\_}1.5.4.pdf},
year = {2013}
}
@book{Kopka2003,
address = {Boston, MA},
author = {Kopka, Helmut and Daly, Patrick W.},
edition = {4th editio},
isbn = {978-0321173850},
publisher = {Addison-Wesley Professional},
title = {{Guide to LaTeX (TTCT series)}},
url = {https://www.latex-project.org/help/books/},
year = {2003}
}
@misc{JimHester2019,
abstract = {Collection of package development tools.},
author = {Hester, Jim},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hester - 2019 - Package 'devtools' Title Tools to Make Developing R Packages Easier namespace and vignette code extracted from base R).pdf:pdf},
pages = {31},
title = {{Package 'devtools' Title Tools to Make Developing R Packages Easier namespace and vignette code extracted from base R)}},
url = {https://github.com/r-lib/devtools},
year = {2019}
}
@article{Siemieniuch2015,
annote = {Paper in Papers},
author = {Siemieniuch, C E and Sinclair, M A and Henshaw, M.J.deC.},
doi = {10.1016/j.apergo.2015.04.018},
journal = {Applied Ergonomics},
keywords = {BPR,Consent (informed):,Cyber-physical systems,GRAI,IoT,STS,agility,allocation of functions,automation,business process re-engineering,circular economy,competitive challenges,complexity,consequences,contracts,control,corporate governance,definitions,engineering,ergonomics,ethics,federated control,green,job design,knowledge configuration,knowledge nets,legal,legislation,models,networks,org. trust,organisational design,product introduction process,resilience,robots,robustness,safety,situation awareness,skills,smart city,socio-tech,software,staffing,standards,strategy,supply chain,sustainability,theory,trust,wisdom},
pages = {104--119},
title = {{Global Drivers, sustainable manufacturing and Systems Ergonomics}},
volume = {51},
year = {2015}
}
@misc{Gruber2004,
abstract = {Markdown is a text-to-HTML conversion tool for web writers. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).},
author = {Gruber, John and Swartz, Aaron},
booktitle = {Daring Fireball (Blog)},
title = {{Daring Fireball: Markdown}},
url = {https://daringfireball.net/projects/markdown/},
urldate = {2019-04-16},
year = {2014}
}
@book{GreatBritain-OfficeofCommunications2015,
abstract = {The report contains statistics and analysis of the UK communications sector and is a reference for industry, stakeholders and consumers. It also provides context to the work Ofcom undertakes in furthering the interests of consumers and citizens in the markets we regulate. The report contains data and analysis on broadcast television and radio, fixed and mobile telephony, internet take-up and consumption and post. We publish this report to support Ofcom's regulatory goal to research markets constantly and to remain at the forefront of technological understanding. It also fulfils the requirements on Ofcom under Section 358 of the Communications Act 2003 to publish an annual factual and statistical report. It also addresses the requirement to undertake and make public our consumer research (as set out in Sections 14 and 15 of the same Act)},
author = {{Great Britain - Office of Communications}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Great Britain - Office of Communications - 2015 - The Communications Market Report 2015.pdf:pdf},
title = {{The Communications Market Report 2015}},
url = {https://www.ofcom.org.uk/{\_}{\_}data/assets/pdf{\_}file/0022/20668/cmr{\_}uk{\_}2015.pdf},
year = {2015}
}
@article{Gantz2012,
abstract = {Welcome to the "digital universe" — a measure of all the digital data created, replicated, and consumed in a single year. It's also a projection of the size of that universe to the end of the decade. The digital universe is made up of images and videos on mobile phones uploaded to YouTube, digital movies populating the pixels of our high-definition TVs, banking data swiped in an ATM, security footage at airports and major events such as the Olympic Games, subatomic collisions recorded by the Large Hadron Collider at CERN, transponders recording highway tolls, voice calls zipping through digital phone lines, and texting as a widespread means of communications.},
author = {Gantz, John and Reinsel, David},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gantz, Reinsel - 2012 - THE DIGITAL UNIVERSE IN 2020 Big Data, Bigger Digi tal Shadows, and Biggest Growth in the Far East.pdf:pdf},
journal = {Idc},
number = {December 2012},
pages = {1--16},
title = {{THE DIGITAL UNIVERSE IN 2020: Big Data, Bigger Digi tal Shadows, and Biggest Growth in the Far East}},
url = {www.emc.com/leadership/digital-universe/index.htm.},
volume = {2007},
year = {2012}
}
@book{Wickham2015,
address = {Boston, MA},
author = {Wickham, Hadley and Bryan, Jennifer},
isbn = {978-1491910597},
pages = {202},
publisher = {O'Reilly Media},
title = {{R Packages}},
year = {2015}
}
@article{Dogan2011,
abstract = {This paper appraises the criticism that "Knowledge Management (KM) is little more than re-packaged Information Management (IM)" through analysis of the relationships and inconsistencies between IM and KM. This is supported by a case study of the loss of an UK Royal Air Force aircraft known as 'Nimrod' as reported in the Haddon-Cave Independent Review. The first part discusses the research methodology adopted and analyses the literature including the theoretical characteristics and practical aspects of IM and KM. This is supported by logical models and relationship tables for comparison. The second part develops an analytical framework by applying evaluation criteria, based on principles for Through Life Management of information, to a case study to address the statement that "information is inadequate without knowledge." The logical models and case study insertions uncovered important conclusions: (1) KM is frequently confused with IM and reliance on IM only can sometimes result in a disaster; (2) it is imperative to understand the distinctions between IM and KM as "management of knowledge" is concerned with socio-technical, hence human, aspects to a greater extent than IM; (3) IM should be considered as a prerequisite to engaging KM; and (4) KM should be perceived as the creation and management of knowledge as a human centred attribute that involves a learning and transformation process. This paper systematically applies the derived logical models and analysis framework to a case study to better understand and illustrate the implications of Through Life Management of information and knowledge. {\textcopyright} 2011 World Scientific Publishing Co.},
author = {Dogan, Huseyin and Henshaw, Michael J. de C. and Ragsdell, Gillian},
doi = {10.1142/S0219649211003085},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dogan, Henshaw, Ragsdell - 2011 - The Risk of Information Management Without Knowledge Management A Case Study.pdf:pdf},
isbn = {0219-6492},
issn = {0219-6492},
journal = {Journal of Information {\&} Knowledge Management},
keywords = {Knowledge management,defence sector,information management,systems analysis,systems thinking,tacit knowledge},
month = {dec},
number = {04},
pages = {393--408},
publisher = {World Scientific Publishing Co.},
title = {{The Risk of Information Management Without Knowledge Management: A Case Study}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0219649211003085},
volume = {10},
year = {2011}
}
@techreport{SPARC2015,
address = {Brussels},
annote = {Document in DECyPHEr - 313 pages.

Very good review of current status and issues. Specifically aimed at H2020 proposals (Horizon 2020 Call ICT-2016 (ICT-25 {\&} ICT-26)); they expect this to be quoted. Wish we had had this during BS 8611 negotiations.

Comprehensive.

Quotes lots of metrics, but I can't find them on the Internet:

Decisional Autonomy - 11 levels?
Object Recognition - 7 levels?
Dependability - 6 levels?
Human-Robot Interation Safety - 6 levels?
Human-Robot Interaction - 7 levels?
Robot-Robot Interaction - 5 levels?
Configurability - 4 levels?
Task adaptability - 4 levels?
Perception Ability - 7 levels?
Scene perception - 6 levels?
Cognitive Action Ability - 7 levels?
Cognitive Knowledge Acquisition - 6 levels? [note: level 6 is knowledge scaffolding]
Cognitive Interpretation Ability - 9 levels?
Cognitive Acquired Knowledge - 8 levels?
Safety Interaction Abilit - 7 levels? 
Mechatronic Configuration - 4 levels?
Constrained Motion - 6 levels?
Reasoning Ability - 7 levels?




pp.310-313: extended description of H2020 TRLs and their intended usage.},
author = {SPARC},
edition = {REv B: Fin},
institution = {SPARC partnership for robotics in Europe},
keywords = {BPR,Cyber-physical systems,IoT,automation,business process re-engineering,engineering,ergonomics,federated control,mobile,network-centric warfare,robots},
publisher = {euRobotics AISBL},
title = {{Robotics 2020 Multi-Annual Roadmap.}},
year = {2015}
}
@article{Callaghan2012,
abstract = {The NERC Science Information Strategy Data Citation and Publication project aims to develop and formalise a method for formally citing and publishing the datasets stored in its environmental data centres. It is believed that this will act as an incentive for scientists, who often invest a great deal of effort in creating datasets, to submit their data to a suitable data repository where it can properly be archived and curated. Data citation and publication will also provide a mechanism for data producers to receive credit for their work, thereby encouraging them to share their data more freely.},
author = {Callaghan, Sarah and Donegan, Steve and Pepler, Sam and Thorley, Mark and Cunningham, Nathan and Kirsch, Peter and Ault, Linda and Bell, Patrick and Bowie, Rod and Leadbetter, Adam and Lowry, Roy and Moncoiff{\'{e}}, Gwen and Harrison, Kate and Smith-Haddon, Ben and Weatherby, Anita and Wright, Dan},
doi = {10.2218/ijdc.v7i1.218},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Callaghan et al. - 2012 - Making Data a First Class Scientific Output Data Citation and Publication by NERC's Environmental Data Centr.pdf:pdf},
isbn = {1746-8256},
issn = {1746-8256},
journal = {International Journal of Digital Curation},
month = {mar},
number = {1},
pages = {107--113},
title = {{Making Data a First Class Scientific Output: Data Citation and Publication by NERC's Environmental Data Centres}},
url = {http://www.ijdc.net/article/view/208 http://ijdc.net/index.php/ijdc/article/view/208},
volume = {7},
year = {2012}
}
@article{Henshaw2017,
abstract = {This document summarises the findings of the Road2CPS project, co-financed by the European Commission under the H2020 Research and Innovation Programme, to develop a roadmap and recommendations for strategic action required for future deployment of Cyber-Physical Systems (CPS). The term Cyber-Physical System describes hardware-software systems, which tightly couple the physical world and the virtual world. They are established from networked embedded systems that are connected with the outside world through sensors and actuators and have the capability to collaborate, adapt, and evolve. In the ARTEMIS Strategic Research Agenda 2016, CPS are described as {\^{a}}€˜Embedded Intelligent ICT Systems{\^{a}}€™ that make products smarter, more interconnected, interdependent, collaborative, and autonomous. In the future world of CPS, a huge number of devices connected to the physical world will be able to exchange data with each other, access web services, and interact with people. Moreover, information systems will sense, monitor and even control the physical world via Cyber-Physical Systems and the Internet of Things (HiPEAC Vision 2015). Cyber-Physical Systems find their application in many highly relevant areas to our society: multi-modal transport, health, smart factories, smart grids and smart cities amongst others. The deployment of Cyber-Physical Systems (CPS) is expected to increase substantially over the next decades, holding great potential for novel applications and innovative product development. Digital technologies have already pervaded day-to-day life massively, affecting all kinds of interactions between humans and their environment. However, the inherent complexity of CPSs, as well as the need to meet optimised performance and comply with essential requirements like safety, privacy, security, raises many questions that are currently being explored by the research community. Road2CPS aims at accelerating uptake and implementation of these efforts. The Road2CPS project identifying and analysing the relevant technology fields and related research priorities to fuel the development of trustworthy CPS, as well as the specific technologies, needs and barriers for a successful implementation in different application domains and to derive recommendations for strategic action. The document at hand was established through an interactive, community-based approach, involving over 300 experts from academia, industry and policy making through a series of workshops and consultations. Visions and priorities of recently produced roadmaps in the area of CPS, IoT (Internet of Things), SoS (System-of-Systems) and FoF (Factories of the Future) were discussed, complemented by sharing views and perspectives on CPS implementation in application domains, evolving multi-sided eco-systems as well as business and policy related barriers, enablers and success factors. From the workshops and accompanying activities recommendations for future research and innovation activities were derived and topics and timelines for their implementation proposed. Amongst the technological topics, and related future research priorities {\^{a}}€˜integration, interoperability, standards{\^{a}}€™ ranged highest in all workshops. The topic is connected to digital platforms and reference architectures, which have already become a key priority theme for the EC and their Digitisation Strategy as well as the work on the right standards to help successful implementation of CPSs. Other themes of very high technology/research relevance revealed to be {\^{a}}€˜modelling and simulation{\^{a}}€™, {\^{a}}€˜safety and dependability{\^{a}}€™, {\^{a}}€˜security and privacy{\^{a}}€™, {\^{a}}€˜big data and real-time analysis{\^{a}}€™, {\^{a}}€˜ubiquitous autonomy and forecasting{\^{a}}€™ as well as {\^{a}}€˜HMI/human machine awareness{\^{a}}€™. Next to this, themes emerged including {\^{a}}€˜decision making and support{\^{a}}€™, {\^{a}}€˜CPS engineering (requirements, design){\^{a}}€™, {\^{a}}€˜CPS life-cycle management{\^{a}}€™, {\^{a}}€˜System-of-Systems{\^{a}}€™, {\^{a}}€˜distributed management{\^{a}}€™, {\^{a}}€˜cognitive CPS{\^{a}}€™, {\^{a}}€˜emergence, complexity, adaptability and flexibility{\^{a}}€™ and work on the foundations of CPS and {\^{a}}€˜cross-disciplinary research/CPS Science{\^{a}}€™.},
author = {Henshaw, M and Siemieniuch, C and Palmer, P J and Sinclair, M and Ingram, C and Stock, D and G{\~{A}}{\P}tz, B and Butler, T and Rico, J and Reimann, M and Mortimer, S and Fitzgerald, J and Servat, D and Rauschecker, U and Ord{\~{A}}³{\~{A}}±ez, D and {De Lama}, N and Alonso, J and R{\~{A}}¼ckriegel, C},
editor = {Reimann, M and R{\~{A}}¼ckriegel, C},
issn = {978-3-95663-117-7},
keywords = {e-Book PDF (also hard copy)},
month = {jan},
publisher = {steinbeis-edition; Stuttgart, Germany.},
title = {{Road2CPS Priorities and Recommendations for Research and Innovation in Cyber-Physical Systems}},
url = {http://www.steinbeis-edition.de/},
year = {2017}
}
@article{Ji2014,
abstract = {With the increasing availability of Location-Based Services (LBS) and mobile internet, the amount of spatial data is growing larger. It poses new requirements and challenges for distributed index and query processing on large scale spatial data. A scalable and distributed spatial data index is important for the effective Nearest Neighbor (NN) query. There are several approaches that implement distributed indices and NN query processing with MapReduce, such as R-tree and Voronoi-based index. However, R-tree is unsuitable for parallelization and Voronoi requires extra computation for localization or local index reconstruction. In this paper, we investigate how to perform NN queries in a distributed environment. Firstly, we present distributed approaches that construct a novel distributed spatial data index: Inverted Grid Index, which is a combination of inverted index and grid partition. Secondly, we illustrate the implementations of two typical applications: distributed k Nearest Neighbor (kNN) and Reverse Nearest Neighbor (RNN) queries which are based on our index structure under cloud computing environment. Finally, we evaluate the effectiveness of our algorithms with extensive experiments using both real and synthetic data sets. Our experiments demonstrate that the time of constructing index structure decreases almost linearly as the number of cluster nodes increases. The results also demonstrate the efficiency and scalability of our NN query algorithms based on Inverted Grid Index. {\textcopyright} 2014 Elsevier Ltd.},
author = {Ji, Changqing and Li, Zhiyang and Qu, Wenyu and Xu, Yujie and Li, Yuanyuan},
doi = {10.1016/j.jnca.2014.05.010},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ji et al. - 2014 - Scalable nearest neighbor query processing based on Inverted Grid Index.pdf:pdf},
isbn = {1084-8045},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Big data,Inverted Grid Index,NN query,Spatial database},
month = {sep},
pages = {172--182},
publisher = {Academic Press},
title = {{Scalable nearest neighbor query processing based on Inverted Grid Index}},
url = {https://www.sciencedirect.com/science/article/pii/S1084804514001295?via{\%}3Dihub},
volume = {44},
year = {2014}
}
@techreport{NCOIC2011,
abstract = {“This is the second document in the series of NIF version 2 documents and builds upon the requirements captured in the NIF Scope and Problem Statement (NSPS) released earlier. The NSD-RM provides a framework for overarching guidance for network centric interoperability, including the specific structure and content for the Specialized Frameworks and Network Centric Patterns that will be developed by all our teams.”},
address = {Washington, DC},
annote = {Document in T-AREA-SoS. 

Fig 9, p 19 has a diagram of the levels. Also in Diagrams.},
author = {NCOIC},
institution = {Network Centric Operations Industry Consortium},
keywords = {BPR,allocation of functions,business process re-engineering,dashboard/battlespace,decision-making,definitions,engineering,intranets,network-centric warfare,networks,org. trust,philosophy,readiness,reference architectures,socio-tech,strategy},
number = {NIF v2.1 NSD-RM v1.2},
publisher = {Network Centric Operations Industry Consortium},
title = {{NCOIC Interoperability Framework 2.1}},
url = {https://www.ncoic.org/apps/group{\_}public/document.php?document{\_}id=17780{\&}wg{\_}abbrev=approved{\_}documents},
year = {2011}
}
@misc{LatexTeam2020,
author = {{Latex Team}},
title = {{Latex Typsetting Software}},
url = {https://www.latex-project.org/},
year = {2020}
}
@article{Tyner2017,
abstract = {This paper explores three different approaches to visualize networks by building on the grammar of graphics framework implemented in the ggplot2 package. The goal of each approach is to provide the user with the ability to apply the flexibility of ggplot2 to the visualization of network data, including through the mapping of network attributes to specific plot aesthetics. By incorporating networks in the ggplot2 framework, these approaches (1) allow users to enhance networks with additional information on edges and nodes, (2) give access to the strengths of ggplot2, such as layers and facets, and (3) convert network data objects to the more familiar data frames.},
author = {Tyner, Samantha and Briatte, Fran{\c{c}}ois and Hofmann, Heike},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Tyner, Briatte, Hofmann - 2017 - Network Visualization with ggplot2.pdf:pdf},
issn = {2073-4859},
journal = {The R Journal},
month = {may},
number = {1},
pages = {27--59},
title = {{Network Visualization with ggplot2}},
url = {https://cran.r-project.org/web/packages/ggplot2/news.html https://journal.r-project.org/archive/2017/RJ-2017-023/index.html https://hal.archives-ouvertes.fr/hal-01722543/},
volume = {9},
year = {2017}
}
@misc{Chaordix2013,
abstract = {Google AI Learning. Several available approaches},
author = {Chaordix},
pages = {2--4},
title = {{Our work}},
url = {https://ai.google/our-work/{\#}?modal{\_}active=none},
year = {2013}
}
@techreport{Gilbertson2018,
abstract = {This document attempts to set out a clear route for a wide variety of bodies towards compliance with the new Data Protection Act and the General Data Protection Regulations. It is the product of the author's reading of relevant legislation, guidance, and conversations with the NBN's Data Protection Group.},
author = {Gilbertson, Paul},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gilbertson - 2018 - Biological Recording and Data Protection.pdf:pdf},
institution = {Joint Nature Conservation Committee},
pages = {7},
title = {{Biological Recording and Data Protection}},
year = {2018}
}
@article{Gardner1986,
abstract = {Overemphasis on hypothesis testing—and the use of P values* to dichotomise significant or non-significant results—has detracted from more useful approaches to interpreting study results, such as estimation and confidence intervals. In medical studies investigators are usually interested in determining the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant. Confidence intervals present a range of values, on the basis of the sample data, in which the population value for such a difference may lie. Some methods of calculating confidence intervals for means and differences between means are given, with similar information for proportions. The paper also gives suggestions for graphical display. Confidence intervals, if appropriate to the type of study, should be used for major findings in both the main text of a paper and its abstract. {\textcopyright} 1986, British Medical Journal Publishing Group. All rights reserved.},
author = {Gardner, Martin J. and Altman, Douglas G.},
doi = {10.1136/bmj.292.6522.746},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gardner, Altman - 1986 - Confidence intervals rather than P values Estimation rather than hypothesis testing.pdf:pdf},
issn = {02670623},
journal = {British Medical Journal (Clinical research ed.)},
number = {6522},
pages = {746--750},
pmid = {3082422},
title = {{Confidence intervals rather than P values: Estimation rather than hypothesis testing}},
volume = {292},
year = {1986}
}
@article{Salminen2018,
abstract = {In this research, we conceptually examine the use of personas in an age of large-scale online analytics data. Based on the criticism and benefits outlined in prior work and by practitioners working with online data, we formulate the major arguments for and against the use of personas given real-time online analytics data about customers, analyze these arguments, and demonstrate areas for the productive employment of data-driven personas by leveraging online analytics data in their creation. Our key tenet is that data-driven personas are located between aggregated and individual customer statistics. At their best, digital data-driven personas capture the coverage of the customer base attributed to aggregated data representations while retaining the interpretability of individual-level analytics; they benefit from powerful computational techniques and novel data sources. We discuss how digital data-driven personas can draw from technological advancements to remedy the notable concerns voiced by scholars and practitioners, including persona validation, inconsistency problem, and long development times. Finally, we outline areas of future research of personas in the context of online analytics. We argue that to survive in the rapidly developing online customer analytics industry, personas must evolve by adopting new practices.},
author = {Salminen, Joni and Jansen, Bernard J. and Jung, Soon-gyo},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Salminen, Jansen, Jung - 2018 - ARE PERSONAS DONE EVALUATING THE USEFULNESS OF PERSONAS IN THE AGE OF ONLINE ANALYTICS.pdf:pdf},
issn = {2205-5258},
journal = {Persona Studies},
keywords = {Customer Segmentation,Data-Driven Personas,Online Analytics},
month = {nov},
number = {2},
pages = {47--65},
title = {{ARE PERSONAS DONE? EVALUATING THE USEFULNESS OF PERSONAS IN THE AGE OF ONLINE ANALYTICS}},
url = {https://ojs.deakin.edu.au/index.php/ps/article/view/737/735 http://www.bernardjjansen.com/uploads/2/4/1/8/24188166/jansen{\_}personas{\_}done.pdf},
volume = {4},
year = {2018}
}
@article{Ritchey2006,
abstract = {General morphological analysis (GMA) is a method for structuring and investigating the total set of relationships contained in multidimensional, usually non-quantifiable, problem complexes. Pioneered by Fritz Zwicky at the California Institute of Technology in the 1930s and 1940s, it relies on a constructed parameter space, linked by way of logical relationships, rather than on causal relationships and a hierarchal structure. During the past 10 years, GMA has been computerized and extended for structuring and analysing complex policy spaces, developing futures scenarios and modelling strategy alternatives. This article gives a historical and theoretical background to GMA as a problem structuring method, compares it with a number of other 'soft-OR' methods, and presents a recent application in structuring a complex policy issue. The issue involves the development of an extended producer responsibility (EPR) system in Sweden.},
author = {Ritchey, T},
doi = {10.1057/palgrave.jors.2602177},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 2006 - Problem structuring using computer-aided morphological analysis.pdf:pdf},
isbn = {01605682},
issn = {01605682},
journal = {Journal of the Operational Research Society},
keywords = {General morphology,Morphological analysis,OR-methods,Problem structuring methods,Typology analysis},
month = {jul},
number = {7},
pages = {792--801},
publisher = {Taylor {\&} Francis},
title = {{Problem structuring using computer-aided morphological analysis}},
url = {https://www.tandfonline.com/doi/full/10.1057/palgrave.jors.2602177},
volume = {57},
year = {2006}
}
@book{Owen2010,
abstract = {Plot is approximately 15 x 61 m, with full area 930 m2, of which 741 m2 is garden. Did not use pesticides except occasionally slug pellets. In the 12 years 1975-86, she recorded 360 plant species, of which 214 were alien, 146 native; 266 cultivated, 94 came in of their own accord. There 2 liverworts (Lophocolea and Marchantia) and 8 mosses. Captured insects with malaise trap, pitfall trap, mercury vapour and butterfly net. There were 375 species of moth (284 macros, 91 micros). Of the 284 macros, she classed 115 as common, 35 as uncommon, 82 scarce ({\textless}10 records in 30 yrs), and 50 once-only. Buddleja davidii was the most widely used with moth larvae of 19 species feeding on it. 46 species of moths used 40 native plants and 38 species used 75 alien plants. The most polyphagous was Angle Shades Phlogophora meticulosa, which had fed on 12 native and 42 alien species.. There is no evidence that 'native is best' for moths (p. 77). Some families such as Lamiaceae, Grossulariaceae, Rosaceae and Saliceae are notably good. Insect numbers generally declined over the 30-year period; exceptions were beetles, sawflies, solitary wasps and Psocoptera. Species totals for all groups on p. 227. Total was 2673 (at least), with Parasitic wasps (553), Beetles (442), Flowering plants (436), Macro-moths (282), Hemiptera (183) as the really numerous groups. Other groups with {\textgreater}50 species were Spiders (80), Micro-moths (93), Hoverflies (94), other Diptera (51), Sawflies (91), Wasps (62), Bees (59) and Birds (54). There were only 2 species of ants, namely Lasius niger and Lasius flavus.},
author = {Owen, Jennifer.},
isbn = {1907057129},
keywords = {floristics Britain fauna},
publisher = {Royal Horticultural Society},
title = {{Wildlife of a garden: a thirty-year study}},
year = {2010}
}
@article{Rosenthal2018,
abstract = {Network biology is widely used to elucidate mechanisms of disease and biological processes. The ability to interact with biological networks is important for hypothesis generation and to give researchers an intuitive understanding of the data. We present visJS2jupyter, a tool designed to embed interactive networks in Jupyter notebooks to streamline network analysis and to promote reproducible research. The tool provides functions for performing and visualizing useful network operations in biology, including network overlap, network propagation around a focal set of genes, and co-localization of two sets of seed genes. visJS2jupyter uses the JavaScript library vis.js to create interactive networks displayed within Jupyter notebook cells with features including drag, click, hover, and zoom. We demonstrate the functionality of visJS2jupyter applied to a biological question, by creating a network propagation visualization to prioritize risk-related genes in autism. The visJS2jupyter package is distributed under the MIT License. The source code, documentation, and installation instructions are freely available on GitHub at https://github.com/ucsd-ccbb/visJS2jupyter . The package can be downloaded at https://pypi.python.org/pypi/visJS2jupyter .},
author = {Rosenthal, Sara Brin and Len, Julia and Webster, Mikayla and Gary, Aaron and Birmingham, Amanda and Fisch, Kathleen M},
doi = {10.1093/bioinformatics/btx581},
editor = {Stegle, Oliver},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Rosenthal et al. - 2018 - Interactive network visualization in Jupyter notebooks VisJS2jupyter.pdf:pdf},
isbn = {1367-4811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
month = {jan},
number = {1},
pages = {126--128},
pmid = {28968701},
publisher = {Oxford University Press},
title = {{Interactive network visualization in Jupyter notebooks: VisJS2jupyter}},
url = {https://academic.oup.com/bioinformatics/article/34/1/126/4158037},
volume = {34},
year = {2018}
}
@article{Cronin2000,
abstract = {The reciprocal relationship between bibliographic references and citations in the context of the scholarly communication system is examined. Semiotic analysis of referencing behaviours and citation counting reveals the complexity of prevailing sign systems and asso- ciated symbolic practices.},
author = {Cronin, Blaise},
doi = {10.1108/EUM0000000007123},
isbn = {00220418},
issn = {0022-0418},
journal = {Journal of Documentation},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {aug},
number = {4},
pages = {440--453},
title = {{Semiotics and evaluative bibliometrics}},
url = {http://www.emeraldinsight.com/doi/10.1108/EUM0000000007123},
volume = {56},
year = {2000}
}
@misc{Sinclair2018a,
abstract = {Murray's observations on the rules of life: Rule 1: Any good idea can be bent for other purposes. Rule 2: Suitable bending removes the need for quantum theory and mathematics Rule 3: Labels can be used to generate awe.},
address = {Loughborough},
author = {Sinclair, M. A.},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sinclair - 2018 - Murray's Rules for Life.pdf:pdf},
pages = {1},
publisher = {Self Published},
title = {{Murray's Rules for Life}},
year = {2018}
}
@article{Palmer2019a,
abstract = {Supporting documents for academic publication},
address = {Loughborough},
author = {Palmer, P. J.},
doi = {10.17605/OSF.IO/T6M89},
institution = {Loughborough University},
journal = {OSF DOI: 10.17605/osf.io/t6m89},
title = {{Colostygia pectinataria analysis supporting documents}},
year = {2019}
}
@article{Peng2015,
abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng. Over the last two decades, the price of collecting a unit of data has dropped dramatically. New technologies touching every aspect of our lives – from our finances, to our health, to our social interactions – have made data collection cheap and easy. In 1967 Stanley Milgram did an experiment (bit.ly/1PWzLDy) to determine the number of degrees of separation between two people in the USA. In his experiment he sent 296 letters to people in Omaha, Nebraska, and Wichita, Kansas, and the goal was to get the letters to a specific person in Boston, Massachusetts. His experiment gave us the notion of “six degrees of separation”. A 2007 study (bit.ly/1PWA2q8) updated that number to “seven degrees of separation” – except the newer study was based on 30 billion instant messaging conversations collected over 30 days. This},
author = {Peng, Roger},
doi = {10.1111/j.1740-9713.2015.00827.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Peng - 2015 - The reproducibility crisis in science A statistical counterattack.pdf:pdf},
issn = {17409705},
journal = {Significance},
month = {jun},
number = {3},
pages = {30--32},
publisher = {Wiley/Blackwell (10.1111)},
title = {{The reproducibility crisis in science: A statistical counterattack}},
url = {http://doi.wiley.com/10.1111/j.1740-9713.2015.00827.x},
volume = {12},
year = {2015}
}
@incollection{Pearl2019b,
abstract = {Judea Pearl is a professor of computer science and director of the Cognitive Systems Laboratory at UCLA. His most recent book, co-authored with Dana Mackenzie, is The Book of Why: The New Science of Cause and Effect. As a former physicist, I was extremely interested in cybernetics. Though it did not utilize the full power of Turing Machines, it was highly transparent, perhaps because it was founded on classical control theory and information theory. We are losing this transparency now, with the deep-learning style of machine learning. It is fundamentally a curve-fitting exercise that adjusts weights in intermediate layers of a long input-output chain. I find many users who say that it "works well and we don't know why." Once you unleash it on large data, deep learning has its own dynamics, it does its own repair and its own optimization, and it gives you the right results most of the time. But when it doesn't, you don't have a clue about what went wrong and what should be fixed. In particular, you do not know if the fault is in the program, in the method, or because things have changed in the environment. We should be aiming at a different kind of transparency. Some argue that transparency is not really needed. We don't understand the neural architecture of the human brain, yet it runs well, so we forgive our meager understanding and use human helpers to great advantage. In the same way, they argue, why not unleash deep-learning systems and create intelligence without understanding how they work? I buy this argument to some extent. I personally don't like opacity, so I won't spend my time on deep learning, but I know that it has a place in the makeup of intelligence. I know that non-transparent systems can do marvelous jobs, and our brain is proof of that marvel. But this argument has its limitation. The reason we can forgive our meager understanding of how human brains work is because our brains work the same way, and that enables us to communicate with other humans, learn from them, instruct them, and motivate them in our own native language. If our robots will all be as opaque as AlphaGo, we won't be able to hold a meaningful conversation with them, and that would be unfortunate. We will need to retrain them whenever we make a slight change in the task or in the operating environment. So, rather than experimenting with opaque learning machines, I am trying to understand their theoretical limitations and examine how these limitations can be overcome. I do it in the context of causal-reasoning tasks, which govern much of how scientists think about the world and, at the same time, are rich in intuition and toy examples, so we can monitor the progress in our analysis. In this context, we've discovered that some basic barriers exist, and that unless they are breached we won't get a real human kind of intelligence no matter what we do. I believe that charting these barriers may be no less important than banging our heads against them. Current machine-learning systems operate almost exclusively in a statistical, or model-blind, mode, which is analogous in many ways to fitting a function to a cloud of data points. Such systems cannot reason about "what if ?" questions and, therefore,},
author = {Pearl, Judea},
booktitle = {John Brockman},
chapter = {2},
editor = {Brockman, John},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2019 - THE LIMITATIONS OF OPAQUE LEARNING MACHINES.pdf:pdf},
pages = {Possible Minds: 25 Ways of Looking at AI},
title = {{THE LIMITATIONS OF OPAQUE LEARNING MACHINES}},
year = {2019}
}
@article{Moed2014,
abstract = {The assessment of scientific merit and individuals has a long and respectable history which has been demonstrated in numerous methods and models utilizing different data sources and approaches (1, 2). The proliferation and increasing availability of primary data has created the ability to evaluate research on many levels and degrees of complexity, but has also introduced some fundamental challenges to all who are involved in this process, including evaluators, administrators and researchers, and others (3). Evaluative methods are used on several levels within the scientific world: (1) Institutional (including departmental) level, (2) Program level, and (3) Individual level. Each of these levels has its own objectives and goals; for example, Institutional evaluation is being used in order to establish accreditation, define missions, establish new programs and monitor the quality of an institute's research activities among others. The types of evaluative results can be seen in the ranking systems of universities, which at present are produced at both regional and international levels, based on different criteria (4). Institutional evaluations are performed based on prestige measures derived from publications, citations, patents, collaborations and levels of expertise of the individuals within the institution. Program level evaluations are performed in order to measure the cost-benefit aspects of specific scientific programs. These are usually based on discovering the linkage between the investment made and the potential results of the program (5). Within this realm we find measures developed for technology transfer capabilities and commercialization potentialities of the program, among others (6). Finally an individual evaluation is mainly performed for purposes of promotion and retention of individuals and is done at specific times in a researcher's career. Individual assessment methods rely mainly on counts of publications or citations (7). In the past few years, with the advent of social media, we have seen an increase in the use of measures based on mentions in social media sites such as blogs, Facebook, LinkedIn, Wikipedia, Twitter, and others, which are labelled as sources of “altmetrics” and include news outlets as well (8). The data used for each of these evaluation goals, whether they measure a publication's impact in social or economic terms or both, varies by the method chosen in each case.},
author = {Moed, Henk and Halevi, Gali},
journal = {Research Trends},
number = {36},
pages = {3--5},
title = {{Review of methodologies and approaches}},
url = {https://www.researchtrends.com/issue-36-march-2014/research-assessment/ http://www.researchtrends.com/issue-36-march-2014/research-assessment/},
year = {2014}
}
@misc{Town2020,
author = {Town, Bob},
booktitle = {GXM Ultra-zoom-4 Manual},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Town - 2020 - GXM Ultra-zoom-4 Manual.pdf:pdf},
title = {{GXM Ultra-zoom-4 Manual}},
year = {2020}
}
@article{Plackett1983,
abstract = {Pearson's paper of 1900 introduced what subsequently became known as the chi-squared test of goodness of fit. The terminology and allusions of 80 years ago create a barrier for the modern reader, who finds that the interpretationo f Pearson'st est procedurea nd the assessmento f what he achieveda re less than straightforwardn, otwithstandingth e technicala dvancesm ade since then. An attempti s made here to surmountt hese difficultiesb y exploringP earson'sr elevanta ctivitiesd uring the first decade of his statistical career, and by describing the work by his contemporaries and predecessors which seem to have influenced his approach to the problem. Not all the questions are answered, and others remain for further study.},
author = {Plackett, R. L.},
doi = {10.2307/1402731},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Plackett - 1983 - Karl Pearson and the Chi-Squared Test.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
month = {apr},
number = {1},
pages = {59},
title = {{Karl Pearson and the Chi-Squared Test}},
url = {https://www.jstor.org/stable/1402731},
volume = {51},
year = {1983}
}
@article{Kim2018,
abstract = {Reproducibility has been shown to be limited in many scientific fields. This question is a fundamental tenet of scientific activity, but the related issues of reusability of scientific data are poorly documented. Here, we present a case study of our difficulties in reproducing a published bioinformatics method even though code and data were available. First, we tried to re-run the analysis with the code and data provided by the authors. Second, we reimplemented the whole method in a Python package to avoid dependency on a MATLAB license and ease the execution of the code on a high-performance computing cluster. Third, we assessed reusability of our reimplementation and the quality of our documentation, testing how easy it would be to start from our implementation to reproduce the results. In a second section, we propose solutions from this case study and other observations to improve reproducibility and research efficiency at the individual and collective levels.While finalizing our code, we created case-specific documentation and tutorials for the associated Python package StratiPy. Readers are invited to experiment with our reproducibility case study by generating the two confusion matrices (see more in section "Robustness: from MATLAB to Python, language and organization"). Here, we propose two options: a step-by-step process to follow in a Jupyter/IPython notebook or a Docker container ready to be built and run.},
author = {Kim, Yang-Min and Poline, Jean-Baptiste and Dumas, Guillaume},
doi = {10.1093/gigascience/giy077},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Poline, Dumas - 2018 - Experimenting with reproducibility a case study of robustness in bioinformatics.pdf:pdf},
issn = {2047-217X},
journal = {GigaScience},
month = {jul},
number = {7},
pmid = {29961842},
publisher = {Oxford University Press},
title = {{Experimenting with reproducibility: a case study of robustness in bioinformatics}},
url = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giy077/5046609},
volume = {7},
year = {2018}
}
@article{Marin-Ortega2014,
abstract = {Current work presents new approach for designing business intelligence solutions. In the era of big data, former and robust analytical concepts and utilities needs to adopt themselves to changed market circumstances. Main focus of the work is to address acceleration of building process of a “data-centric” business intelligence solution on one side, and also prepare business intelligence solutions for big data utilization. Research is addressing following goals: (a) reduce time spent during business intelligence solution designing phase; (b) achieve flexibility of business intelligence solution by removing problems with adding new data sources; (c) prepare business intelligence solution for utilization of big data concepts. Research proposes extension of existing Extract, Load and Transform (ELT) approach to the new one Extract, Load, Transform and Analyse (ELTA).},
author = {Mar{\'{i}}n-Ortega, Pablo Michel and Dmitriyev, Viktor and Abilov, Marat and G{\'{o}}mez, Jorge Marx},
doi = {10.1016/J.PROTCY.2014.10.015},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mar{\'{i}}n-Ortega et al. - 2014 - ELTA New Approach in Designing Business Intelligence Solutions in Era of Big Data.pdf:pdf},
issn = {2212-0173},
journal = {Procedia Technology},
month = {jan},
pages = {667--674},
publisher = {Elsevier},
title = {{ELTA: New Approach in Designing Business Intelligence Solutions in Era of Big Data}},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314002424?via{\%}3Dihub},
volume = {16},
year = {2014}
}
@article{Yao1999,
abstract = {In this paper, we argue that granular computing may have many potential applications in knowledge discovery and data mining. Three related basic oper- ations of granular computing are examined: granula- tion of the universe, characterization of granules, and relationships between granules. Their connections to the tasks of knowledge discovery and data mining are analyzed.},
author = {Yao, Y Y and Zhong, Ning},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yao, Zhong - 1999 - Potential Applications of Granular Computing in Knowledge Discovery and Data Mining.pdf:pdf},
journal = {Proceedings of World Multiconference on Systemics, Cybernetics and Informatics},
keywords = {Big Data,Concept formation,data analysis,data mining,granular computing,knowledge discovery},
mendeley-tags = {Big Data},
pages = {573--580},
title = {{Potential Applications of Granular Computing in Knowledge Discovery and Data Mining}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.5364{\&}rep=rep1{\&}type=pdf},
volume = {5},
year = {1999}
}
@article{Abdi2010,
abstract = {Principal component analysis (pca) is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the pca model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. Pca can be generalized as correspondence analysis (ca) in order to handle qualitative variables and as multiple factor analysis (mfa) in order to handle heterogenous sets of variables. Mathematically, pca depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition (svd) of rectangular matrices.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
doi = {10.1002/wics.101},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Abdi, Williams - 2010 - Principal component analysis.pdf:pdf},
isbn = {1939-0068},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
number = {4},
pages = {433--459},
pmid = {20931840},
title = {{Principal component analysis}},
url = {http://staff.ustc.edu.cn/{~}zwp/teach/MVA/abdi-awPCA2010.pdf},
volume = {2},
year = {2010}
}
@article{Kim1991,
abstract = {The proliferation of file systems, navigational database systems (hierarchi- cal and network). and relational database systems during the past three decades has created difficult problems arising from the need to access heterogeneous files and databases through a single data definition and query language designed under a single data model. This has been the primary motiva- tion for research into multidatabase systems'.' during the past 15 years. An MDBS is a federation of independently developed component database systems (CDBSs). The MDBS provides a homogenizing layer on top of the CDBSs, thus giving users the illusion of a homogeneous system. Because CDBSs operate independently (that is, without central control or distributed coordination), the component databases (CDBs) may include structural and representational dis- crepancies. or conflicts. called schematic and data heterogeneity. These conflicts mu$\backslash$t hc rLwlvccl (homogcnized) so MDRS users c;in access the underlying C'DIh -- with a sinylc uniform clatabasc language rather than ;1 dillcrcnt database laiiguagc {\~{}}. for each CDB. Schematic},
annote = {This paper acknowledges problems with combining data from many sources using examples very similar to the ones tht I would use.

This paper proposes a classification, but not a solution.},
author = {Kim, W. and Seo, J.},
doi = {10.1109/2.116884},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Seo - 1991 - Classifying schematic and data heterogeneity in multidatabase systems.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {dec},
number = {12},
pages = {12--18},
title = {{Classifying schematic and data heterogeneity in multidatabase systems}},
url = {http://ieeexplore.ieee.org/document/116884/},
volume = {24},
year = {1991}
}
@article{Zar1972,
abstract = {A table of critical values of the Spearman rank correlation coefficient, r8 is given for n = 4(1)50(2) 100, for nine levels of significance: $\alpha$ = 0.50, 0.20, 0.10, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001. {\textcopyright} Taylor {\&} Francis Group, LLC.},
author = {Zar, Jerrold H.},
doi = {10.1080/01621459.1972.10481251},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zar - 1972 - Significance testing of the spearman rank correlation coefficient.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {339},
pages = {578--580},
title = {{Significance testing of the spearman rank correlation coefficient}},
volume = {67},
year = {1972}
}
@misc{Greiner2017,
abstract = {This document provides Best Practices related to the publication and usage of data on the Web designed to help support a self-sustaining ecosystem. Data should be discoverable and understandable by humans and machines. Where data is used in some way, whether by the originator of the data or by an external party, such usage should also be discoverable and the efforts of the data publisher recognized. In short, following these Best Practices will facilitate interaction between publishers and consumers.},
author = {Greiner, A and Isaac, A and Iglesias, C and Laufer, C and Gu{\'{e}}ret, C and Lee, D and Schepers, D and Stephan, E and Kauz, E and Atemezing, G and Beeman, H and Bittencourt, I and Almeida, J and Dekkers, M and Winstanley, P and Archer, P and Albertoni, R and Purohit, S and {C{\'{o}}rdova. Y}},
booktitle = {W3C Recommendation},
month = {jan},
title = {{Data on the Web Best Practices}},
url = {https://www.w3.org/TR/dwbp/},
urldate = {2018-07-10},
year = {2017}
}
@book{Wilson2013,
abstract = {From personal experience and also from lurking on the $\backslash$textttcomp.text.tex newsgroup the major problems with using $\backslash$latex are related to document design. Some years ago most questions on $\backslash$textttctt were answered by someone providing a piece of code that solved a particular problem, and again and again. More recently these questions are answered along the lines of `Use the --------- package', and again and again. I have used many of the more common of these packages but my filing system is not always well ordered and I tend to mislay the various user manuals, even for the packages I have written. The $\backslash$textttmemoir class is an attempt to integrate some of the more design-related packages with the $\backslash$latex $\backslash$textttbook class. I chose the $\backslash$textttbook class as the $\backslash$textttreport class is virtually identical to $\backslash$textttbook, except that $\backslash$textttbook does not have an abstract environment while $\backslash$textttreport does; however it is easy to fake an abstract if it is needed. With a little bit of tweaking, $\backslash$textttbook class documents can be made to look just like $\backslash$textttarticle class documents, and the $\backslash$textttmemoir class is designed with tweaking very much in mind. The $\backslash$textttmemoir class effectively incorporates the facilties that are usually accessed by using external packages. In most cases the class code is new code reimplementing package functionalities. The exceptions tend to be where I have cut and pasted code from some of my packages. I could not have written the $\backslash$textttmemoir class without the excellent work presented by the implementors of $\backslash$latex and its many packages. Apart from packages that I happen to have written I have gained many ideas from the other packages listed in the Bibliography. One way or another their authors have all contributed, albeit unknowingly. The participants in the $\backslash$textttcomp.text.tex newsgroup have also provided valuable input, partly by questioning how to do something in $\backslash$latex, and partly by providing answers. It is a friendly and educational forum.},
author = {Wilson, Peter and Madsen, Lars},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wilson, Madsen - 2013 - The Memoir Class for Configurable Typesetting - User Guide.pdf:pdf},
pages = {611},
title = {{The Memoir Class for Configurable Typesetting - User Guide}},
url = {http://texdoc.net/texmf-dist/doc/latex/memoir/memman.pdf},
year = {2013}
}
@inproceedings{Whitworth2006,
address = {Cambridge, UK, US DOD.},
author = {Whitworth, I R and Smith, S J and Hone, G N and Macleod, I},
title = {{How do we know that a scenario is “appropriate”? }},
url = {https://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=1{\&}cad=rja{\&}uact=8{\&}ved=0CB0QFjAAahUKEwiW8e7k4J7HAhXLWx4KHes{\_}Bt8{\&}url=http{\%}3A{\%}2F{\%}2Fwww.dodccrp.org{\%}2Fevents{\%}2F11th{\_}ICCRTS{\%}2Fhtml{\%}2Fpapers{\%}2F072.pdf{\&}ei=PrnIVZbaJcu3eev{\_}mPgN{\&}usg=AFQjCNEmizp{\_}nyyx0WOs1},
volume = {11th ICCRT},
year = {2006}
}
@article{Allwood2011,
abstract = {For most materials used to provide buildings, infrastructure, equipment and products, global stocks are still sufficient to meet anticipated demand, but the environmental impacts of materials production and processing, particularly those related to energy, are rapidly becoming critical. These impacts can be ameliorated to some extent by the ongoing pursuit of efficiencies within existing processes, but demand is anticipated to double in the next 40 years, and this will lead to an unacceptable increase in overall impacts unless the total requirement for material production and processing is reduced. This is the goal of material efficiency, and this paper aims to stimulate interest in the area. Four major strategies for reducing material demand through material efficiency are discussed: longer-lasting products; modularisation and remanufacturing; component re-use; designing products with less material. In industrialised nations, these strategies have had little attention, because of economic, regulatory and social barriers, which are each examined. However, evidence from waste management and the pursuit of energy efficiency suggests that these barriers might be overcome, and an outline of potential mechanisms for change is given. In bringing together insights into material efficiency from a wide range of disciplines, the paper presents a set of 20 open questions for future work.},
annote = {Paper in T-AREA-SoS. Note first excerpt below; summarisies the question. See page 368 for ways to improve material efficiency; equations are instructive.

Discussion:
"In delivering material services we will not run out of (low cost) materials, but if we are concerned about the environmental impacts of materials production, this paper has shown that there is a rationale to analyse the contribution that material efficiency can provide beyond energy-related efficiency.
The tension between ‘what can we do' and ‘what must we do' has been a constant theme in the narrative of this paper. According to Bleischwitz et al. (2009), the guiding question for economics is ‘can companies and industries spur their competitiveness, and can countries as a whole enhance their prosperity through improving material efficiency?' The equivalent back-casting question is ‘given what we know about the capacity of ecosystems for absorbing the effects of production, what level of material efficiency must we achieve, and how do we bring it about?' We have demonstrated that opportunities exist to improve the efficiency with which we use materials, and despite the obstacles to adopting this behaviour, there are many mechanisms that might be deployed to promote it. Yet material efficiency within the field of climate change mitigation has received limited attention since the first studies in the mid 1990s. {\ldots}"


Intro:
"Engineered materials are abundant and life as we currently live it would be impossible without them. Since the industrial revolution, we have processed these materials in an industry operating mainly as an open system, transforming resources to products that are eventually discarded. However as a result of growing demand, mankind now dominates the global flows of many elements of the periodic table (Klee and Graedel, 2004), selected materials have become scarce, and access to materials affects the security of many nations. An expanding population living on finite resources is always in danger of consuming all its resources and according to Diamond (2006), resource expiry may account for the collapse of several past civilisations. In addition, materials production and processing have dramatic impacts on the environment, including land use patterns, the use of water, undesirable emissions to air, water and land and the consumption of other important environmental resources. The risk of catastrophic climate change due to emission of greenhouse gases (GHGs) is currently seen as an urgent threat, and the basis of industrial development in its current form is challenged by the need to reduce GHG emissions by 55–85{\%} by 2050 (Fisher and Nakicenovic, 2007, Table 3.10, p. 229). 
{\ldots}
Material efficiency was normal practice prior to the industrial revolution, as the relatively high value of materials compared to labour ensured that buildings and products were maintained, repaired and upgraded. However, since concerns over the environmental impacts of post-industrial revolution production have risen to prominence, material efficiency has received limited attention in contemporary analysis and policy."
[and let's not forget Tainter]
Peak oil has occurred:
"Aleklett et al. (2010) in a detailed critique of the ‘2008 World Energy Outlook' (IEA, 2008b) estimate that ‘peak oil' has now occurred, that production from conventional fields will decline, and even with increasing output from new and unconventional sources, total production will decline from ∼80 Giga-barrels (Gb)/day now to ∼75 Gb/day by 2030. The impact of this on future polymer production is difficult to estimate: demand for oil for transport would grow if not supply-constrained, so declining production will drive up prices. However, the supply of oil for conversion to polymers is secure for the foreseeable future, albeit at increased cost."

Biomass won't deliver much:
"Bio-plastics, although their production is increasing, are made in only very small volumes compared to plastics made from hydrocarbons (Chadha, 2010), so the key use of biomass as an engineering material is for timber and paper. Can we significantly increase the annual supply of timber and paper? Table 2.1 demonstrates that average global annual production is 1 kg of dry biomass per square metre of land. 40{\%} of this is currently appropriated by humans, and it is difficult to increase total output: there is limited land for future expansion, yields can only be increased by small percentages, and at least some of the yield must be composted to provide nutrients for future growth. In a detailed study of biomass production from existing forests in New York State, Castellano et al. (2009) show available production of just 0.15 kg/m2 per year. Furthermore, the use of biomass for liquid fuels constrains its use for fuel, food and as a material. Eide (2008) reports that in 2007, 23{\%} of US coarse grain production was used to produce ethanol and 47{\%} of EU vegetable oil used for biodiesel, yet in total liquid biofuels provide only 0.36{\%} of global energy supply. The use of biomaterials for material services is thus in competition with other uses, and total biomass appropriation cannot rise significantly in future."

Energy saving:
"IEA (2008a) provides the breakdown of global CO2 emissions (from energy and processes) in Fig. 2.4 demonstrating that 56{\%} of industrial CO2 emissions, or 20{\%} of all energy and process related emissions, arise from production and processing of just five materials: steel, cement, plastic, paper and aluminium. In assessing the threat of unwanted climate change being driven by these emissions, the Intergovernmental Panel on Climate Change (IPCC) recommends a global reduction in GHG emissions by 55–85{\%} by 2050 (Fisher and Nakicenovic, 2007). Can this level of reduction be achieved for the five key materials in Fig. 2.4, in the light of anticipated demand growth?
Four options exist to reduce CO2 emissions per unit output within existing production processes:
• Energy efficiency:IEA (2008a) presents a review of known energy efficiency options for the five materials prioritised in Fig. 2.4, showing that for steel, 34{\%} of CO2 emissions per unit output could be saved by a combination of raising all plant to current best practice (18{\%}) and through global adoption of technologies ‘beyond best practice' (16{\%}). Equivalent figures for cement (40{\%}), plastic (22{\%}), paper (38{\%}, using further options from Worrell et al., 2000) and aluminium (24{\%}, using further options from Choate and Green, 2003) suggest a limit to improvement in existing process chains of 23–40{\%} reduction in CO2 emissions per unit output. Operational improvements and supply chain re-design may give further gains if, for instance, the number of thermal cycles in a supply chain can be reduced by co-location, or thermal inertia can be reduced to improve the speed of process start-up.• Yield improvement: More primary material is made than ends up in final goods, and this loss of material between its liquid form and use in a final product is termed the ‘yield loss'. For most materials, primary production is the most energy intensive stage, so any yield loss implies an increase in the total energy used per unit of final goods. Yield losses can arise from start-up losses, trimming and scalping during processing, subtractive processing, quality problems, high purity requirements, mismatches between batch and order volumes, and over-ordering. In producing sheet metal components, up to 50{\%} of cast metal is discarded through ingot scalping, rolling trim and blanking skeletons (Mulero and Layton, 2007); in aerospace manufacturing where final product weight dominates all other concerns, the ‘buy-to-fly' ratio for material in the product compared to material purchased can be as poor as 10:1 (Boyer, 2010); in construction, over-ordering to minimise the risk of shortages on-site can lead to waste of up to 25{\%} of actual requirements (Navon and Berkovich, 2005).
• Increased recycling rates: Except for cement, where there is currently no route to create new cement from old, recycling (in which used material is reduced to liquid form) is significantly less energy intensive than primary production so already has strong commercial motivation. Current rates of recycling (the fraction of annual discarded material that becomes part of future production) for steel (65{\%}, from Wang et al., 2007), paper (43{\%}, from IEA, 2007) and aluminium (39{\%}, from IAI, 2007) show how important this route is, even at current relatively low energy prices.
{\ldots} 
In an analysis of 20 distinctly different products, Dahmus and Gutowski (2007) show that products with higher material values are currently recycled provided their mixture is not too complicated. However, they also show by looking at the history of cars, refrigerators and computers that over time, products tend to become more mixed – and hence less economically attractive to recycle. In addition the operational challenge of recycling is great: uncertainty over the availability of recycled material streams, dictates that large stocks of recycled materials are required to match supply to demand; the logistics and infrastructure of material collection and sorting are complex; the time delay between production and disposal creates problems in material characterisation; if demand for goods is growing and is the sum of replacement and new demand, the supply of recycled material can never match demand until new demand ceases.• Decarbonisation of the global energy system: The CO2 emissions from materials production and processing could be reduced if the processes were powered by less carbon intensive energy. However, renewable energy supplies require large land commitment (Mackay, 2009), Carbon Capture and Storage (CCS) and nuclear power installations are expensive (Rubin et al., 2007), and the transition from one energy system to another has historically takenmanydecades (Smil, 2010). Accordingly, most energy agencies are predicting only modest substitution of the energy mix by non carbon-emitting sources by 2050 – for example, the scenarios of IEA (2008a) predict industrial emissions reduction by 17–37{\%} by 2050 due to CCS. In the US, only 20{\%} of industrial energy use is currently supplied as electricity, so the potential for decarbonisation throughnewelectricity supply is further limited, unless novel electrically powered processing routes are widely adopted."

Effects on water:
"Another exception is concern over the availability of fresh-water, which is already becoming critical in several regions. Fig. 2.7 suggests that extraction and processing of minerals are not major driver of water use, although locally, water stress may constrain some processing. The major impact of current materials processing on water is through toxic releases."

p.368 Ways to improve material efficiency - see paper for equations; see fig 3.1:
"The options already discussed in Section 2.2 were:
• Reducing Co or Cr – improving the energy efficiency or decarbonising existing processes for creating liquid material.
• Reducing Ms/Md – improving the yield ratio.
• Increasing fr – increasing the recycling rate.
The other options revealed by (2) are:
• Reducing N – which is uncomfortable if taken to mean impeding growth in developing countries, but could imply supporting a less materially intensive path to prosperity.
• Increasing L – either by using products more intensely and for longer, or by providing means to repair, upgrade or remanufacture products when discarded by their first owner.
• ReducingMp/D– designing ‘lightweight' products with less material input.
• Increasing fu – the fraction of material supplied by re-use when, as is often the case, Cu is small."

Lengthening product retention and use:
"Ashby (2009) describes product life as the shortest of: the physical life (when the product breaks beyond economic repair); the functional life (the need for the product ceases); the technical life (the product is obsolete); the economical life; the legal life; the life in which the product remains desirable.
{\ldots}
Cooper (2005) describes the design of a product as a combination of ‘shape and surfaces' (physical content and appearance) its ‘signs and scripts' (the signals it conveys within human relationships) and the ‘sales and services' by which it is bought. Based on the first two of these, Cooper proposes that the desire to replace goods might be contained if consumers had better information on durability or if they can be involved in personalising production, so they will not discard goods so easily. Based on the third, he identifies several alternative means to delay product replacement: adding value during the product life cycle (through extended warranties for instance); avoiding purchasing through leasing (as described in the literature on ‘product service systems' for instance Behrendt et al., 2003); use of products through a service provider (such as a laundrette); shared ownership."

See Table 3.3. for ways of re-use.Regulatory, legal and social barriers to material re-use:
"If markets reflected all costs accurately and immediately, then in response either to material scarcity or to harmful impacts created by material production and processing, prices would rise and this would curtail demand and stimulate a search for substitutes and technology improvements. For goods which are privately owned and traded without ‘externalities', markets have proved effective at achieving this, but for ‘public' goods – such as the atmosphere, oceans or biodiversity – the costs of harm do not affect prices unless the costs are added due to policy. Historically, according to Pearce (2005), rather than identifying a target standard of performance, policy makers typically identify a current best technology and set in place measures to promote its adoption. This approach, also known as ‘picking winners' has the attraction of feasibility, but will lead to slow change if a stimulus is needed to develop technologies to a higher standard.
{\ldots}
{\ldots}but in the area of material efficiency it is possible to identify some specific barriers within existing policy:

• Changes to UK health and safety regulations so that demolition employees work mainly at ground level have significantly reduced the availability of construction steel for re-use in the past 10 years (Allwood et al., 2010a).
• Governments may give support to materially inefficient practices for manufacturing, energy or resource extraction sectors for instance through scrappage schemes or favourable tax reductions. van Beers and de Moor (2001) claim that these sectors receive over {\pounds}1 trillion in subsidies.
• Standards bodies assume that all materials are new – and there is in general a lack of government certification for reused materials. 
• Lack of information and poor system design places a high burden on individuals to identify optimum disposal routes for every item being discarded. (Different solutions are currently required in the UK for kettles, clothing, fridges, batteries, televisions etc.).
• Standards which prescribe a certain material composition instead of a material performance inhibit material substitution or reuse. This is for example, a barrier for blended cement in road construction in large parts in the US.
• Legislation on producer responsibility persists beyond first ownership, so that, for example UK charity shops now do not sell electrical goods. Clarification on this responsibility is required to support extensions to product or component life.
{\ldots}
De Vries (2008) argues that in fact the transition to consumerism began 100 years earlier, from the mid-seventeenth century, when members of households increasingly spent time working in employment outside the home, in order to purchase external services, such as children's education, that would previously have been provided within the home. This dependence on purchased goods, and hence monetary income, provides a beginning for consumerism, but its manifestation in wealthy economies where basic needs for security, health and comfort have been met is now far broader:

• Fashion rather than form or function determines the end of life of many goods, and while technical reasons for the end of physical life can generally be overcome by design, it is difficult to change the symbolic role of a product without replacing its material content. This focus on ‘conspicuous consumption' suggests that goods made from re-used material, or designed for future re-use, may be seen as less desirable if they symbolise thrift.
• Convenience has become a major driver of consumption, leading to considerable excess in capacity for service provision: individual car or washing machine ownership leads to national capacity for personal transport and clothes washing far in excess of requirements.
• Cultural attitudes to waste have moved from moral disapproval to complete acceptance, so the ‘throw-away' society treats as normal the discard of materials with re-use value.
• De Vries (2008) reviews work arguing that the pervasiveness of marketing and media images of idealised lifestyles ensures that it is the anticipation of consumption that is at the core of today's hedonism, and the fact that the reality cannot live up to the dream drives the immediate craving for further consumption.

The use by governments of GDP figures as an indicator of economic well-being directly supports the pursuit of policies that aim to increase spending, and hence consumerism. In turn this drives the production and processing of much more material than is necessary to meet basic human needs, and leads to awaste stream rich in perfectly functioning products discarded for a lack of desirability rather than any lack of function."

Incentives for material efficiency:
"All developed governments seek to ensure by R{\&}D investment that they are well-placed in future technology races, so fear of being left behind is a powerful motivator – and this applies at individual, business and governmental levels. This section aims to collect evidence of other potential drivers of change.

5.1. Business opportunities
The case studies described earlier in this paper suggest that new business opportunities related to material efficiency may occur through:
• New revenue streams, such as primary metals producers developing a ‘second-hand' supply chain (for instance reconditioning, re-certifying and re-selling used I-beams) exactly as car makers aim to control their re-sale chains.
• Leasehold as a new business model – taking the example of Rolls Royce ‘power by the hour' contracts for aero-engines, or Xerox's leasing of copiers, to retain materials on the balance sheet and hence nurture their value.
• Brand benefits of environmental leadership, as currently being pursued by large UK retail chains for example (Jones et al., 2007).
• Vertical integration providing the ability to draw value from business streams other than growth in physical output.
• Embodied energy becoming a higher priority as use-phase energy efficiency improves – for instance as buildings become more passive and vehicles more efficient, so their production energy becomes a higher priority.
• Learning lessons from developing countries – where the ratio between labour and energy/material costs is different.
• New supply chain partnerships – for instance between design and demolition in buildings, or design, repair and end-of-life in
appliances.

5.2 Government interventions
Table 5.1 summarises policy instruments that have been used to promote material efficiency around the world. Generally, these are relatively conservative measures – aiming to support change more than banning or taxing unwanted behaviour.
{\ldots}
• Resource taxes and waste taxes have been fiercely opposed by the industrial establishment although landfill taxes are now
widespread and rising in Europe.
• Finland is now moving towards a resource tax and Norway has a tax on packaging.
• Where regulations banning certain behaviour have been implemented, they mainly concern waste regulation, such as: reduced permits for landfill sites and permits for new incinerators; restrictions
on waste disposal – for instance on lead acid batteries in landfill; banning free distribution of plastic bags in shops; aluminium
cans and PET bottles in Sweden must hit 90{\%} recycling or be banned; CAF{\'{E}} regulations on fleet average consumption."




References
Akerlof GA. The market for ‘lemons': quality uncertainty and the market mechanism.
The Quarterly Journal of Economics 1970;84(3):488–500.
Aleklett K, Hook M, Jakobsson K, et al. The peak of the oil age – analyzing the world
oil production Reference Scenario in World Energy Outlook 2008. Energy Policy
2010;38(3):1398–414.
Alexander C, Smaje C. Evaluating third sector reuse organisations in the UK: casestudies
and analysis of furniture reuse schemes. Resources, Conservation and
Recycling 2008;52:719–30.
Allwood JM, Cullen JM, Milford RL. Options for achieving a 50{\%} cut in industrial
carbon emissions by 2050. Environmental Science and Technology
2010;44(6):1888–94.
Allwood JM, Cullen JM, Cooper DR, Milford RL, Patel ACH, Carruth MA, et al. Conserving
our metal energy: avoiding melting steel and aluminium scrap to save
energy and carbon. University of Cambridge; 2010a, ISBN 978-0-903428-30-9.},
author = {Allwood, J M and Ashby, M F and Gutowski, T G and Worrell, E},
doi = {10.1016/j.resconrec.2010.11.002},
journal = {Resources, Conservation and Recycling},
keywords = {BPR,Governance,accounting practices,benchmarking,business process re-engineering,competitive challenges,complexity,consequences,corporate governance,cost models,culture,design process,engineering,federated control,green,knowledge,learning organisation,maintenance,measurement,metrics/metrication,networks,organisational design,philosophy,product introduction process,production control,quality,quotes,reference architectures,strategic planning,strategy,supply chain,systems,user needs},
pages = {362--381},
title = {{Material efficiency:  a white paper}},
volume = {55},
year = {2011}
}
@article{Ritchey2018,
abstract = {General Morphological Analysis (GMA) is a method for structuring a conceptual problem space – called a morphospace – and, through a process of existential combinatorics, synthesising a solution space. As such, it is a basic modelling method, on a par with other scientific modelling methods including System Dynamics Modelling, Bayesian Networks and various types graph-based “influence diagrams”. The purpose of this article is 1) to present the theoretical and methodology basics of morphological modelling; 2) to situate GMA within a broader modelling theoretical framework by developing a (morphological) model representing different modelling methods, and 3) to demonstrate some of the basic modelling techniques that can be carried out with GMA using dedicated computer support.},
author = {Ritchey, Tom},
doi = {10.1016/j.techfore.2017.05.027},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 2018 - General morphological analysis as a basic scientific modelling method.pdf:pdf},
issn = {00401625},
journal = {Technological Forecasting and Social Change},
keywords = {Analysis and synthesis,Existential combinatorics,General Morphological Analysis,Modelling theory,Morphological modelling},
month = {jan},
pages = {81--91},
publisher = {North-Holland},
title = {{General morphological analysis as a basic scientific modelling method}},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517307242},
volume = {126},
year = {2018}
}
@misc{Council2014,
address = {European Union},
annote = {In BSI 8611},
author = {{EU Council}},
booktitle = {P-EC2014},
keywords = {RRI},
publisher = {Italian Presidency of the Council of the European Union},
shorttitle = {RRI@Rome},
title = {{Rome Declaration on Responsible Research and Innovation in Europe}},
year = {2014}
}
@article{Augustin1996,
abstract = {1. A new method for estimating the geographical distribution of plant and animal species from incomplete field survey data is developed. 2. Wildlife surveys are often conducted by dividing a study region into a regular grid and collecting data on abundance or on presence/absence from some or all of the squares in the grid. Generalized linear models (GLMs) can be used to model the spatial distribution of a species within such a grid by relating the response variable (abundance or presence/absence) to spatially referenced covariates. 3. Such models ignore or at best indirectly model dependence on unmeasured covariates, and the intrinsic spatial autocorrelation arising for example in gregarious populations. 4. We describe a procedure for use with presence/absence data in which spatial autocorrelation is modelled explicitly. We achieve this by extending a logistic model to include an extra covariate which is derived from the responses at neighbouring squares. The extended model is known as an autologistic model. 5. To allow fitting of the autologistic model when only a random sample of squares is surveyed, we use the Gibbs sampler to predict presence/absence at unsurveyed squares. 6. We compare the autologistic model with the ordinary logistic model using red deer census data. Both models are fitted to a subsample of 20{\%} of the data and results are compared with the 'true' abundance and spatial distribution indicated by the full census. We conclude that the autologistic model is superior for estimating the spatial distribution of the deer, whereas the ordinary logistic model yields more precise estimates of the overall number of squares occupied by deer at the time of the survey.},
author = {Augustin, N. H. and Mugglestone, M. A. and Buckland, S. T.},
doi = {10.2307/2404755},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Augustin, Mugglestone, Buckland - 1996 - An Autologistic Model for the Spatial Distribution of Wildlife.pdf:pdf},
issn = {00218901},
journal = {The Journal of Applied Ecology},
month = {apr},
number = {2},
pages = {339},
publisher = {JSTOR},
title = {{An Autologistic Model for the Spatial Distribution of Wildlife}},
volume = {33},
year = {1996}
}
@inproceedings{hutt2005overmouldingrecovery,
author = {Hutt, D A and Teh, N J and Sarvar, F and Whalley, D C and Palmer, P J and Anderson, P and Kandasubramanian, B and Morris, P and Prosser, S and O'Brien, P},
booktitle = {Proceedings of the 5th IEEE CPMT International Conference on Polymers and Adhesives in Microelectronics and Photonics (Polytronic)},
isbn = {0-7803-9553-0},
month = {oct},
organization = {Wroclaw, Poland},
pages = {133},
title = {{Overmoulding of Electronics for End-of-Life Recovery}},
year = {2005}
}
@misc{Ann,
abstract = {The Cambridge Digital Library has finished preparing the first showing of the Oliver Rackham web pages: http://cudl.lib.cam.ac.uk/collections/rackham You can now view online some of Oliver's notebooks. The red notebooks form a chronological sequence, and were kept continuously by Rackham from his youth right up until his death. They record observations on plants seen on his travels as well as in his home surroundings, as well as other kinds of information, for example about weather and college duties. They are paginated continuously and include some sketches. A label on the outside usually lists the locations covered in each notebook, with page numbering, thus serving as a kind of contents page. The blue notebooks are more location-specific than the red ones. They are divided into separate sub-groups according to location, with an abbreviated key on the spine. Within each sub-group (for example the Hayley Wood books) pagination is continuous from notebook to notebook. They tend to contain more raw data than the red notebooks, for example tally charts showing frequency of particular plant species in particular areas of woodland, with photocopied maps of woodland areas often pasted in. Although sequence is roughly chronological, information is entered in a more content-led fashion than in the red notebooks.},
author = {Rackham, Oliver},
title = {{Oliver Rackham digital archive | Corpus Christi College University of Cambridge}},
url = {https://www.corpus.cam.ac.uk/articles/oliver-rackham-digital-archive},
urldate = {2020-04-18}
}
@article{Zhang2016,
abstract = {The ongoing rapid expansion of the Word Wide Web (WWW) greatly increases the information of effective transmission from heterogeneous individuals to various systems. Extensive research for information diffusion is introduced by a broad range of communities including social and computer scientists, physicists, and interdisciplinary researchers. Despite substantial theoretical and empirical studies, unification and comparison of different theories and approaches are lacking, which impedes further advances. In this article, we review recent developments in information diffusion and discuss the major challenges. We compare and evaluate available models and algorithms to respectively investigate their physical roles and optimization designs. Potential impacts and future directions are discussed. We emphasize that information diffusion has great scientific depth and combines diverse research fields which makes it interesting for physicists as well as interdisciplinary researchers.},
author = {Zhang, Zi-Ke and Liu, Chuang and Zhan, Xiu-Xiu and Lu, Xin and Zhang, Chu-Xu and Zhang, Yi-Cheng},
doi = {10.1016/j.physrep.2016.07.002},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Dynamics of information diffusion and its applications on complex networks.pdf:pdf},
issn = {03701573},
journal = {Physics Reports},
month = {sep},
pages = {1--34},
publisher = {North-Holland},
title = {{Dynamics of information diffusion and its applications on complex networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0370157316301600?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email http://linkinghub.elsevier.com/retrieve/pii/S0370157316301600},
volume = {651},
year = {2016}
}
@inproceedings{webb2005polymerintegration,
author = {Webb, D P and Hsu, C C and Hutt, D A and Hopkinson, N and Conway, P P and Palmer, P J},
booktitle = {Proceedings of the 5th International Conference on Polymers and Adhesives in Microelectronics and Photonics},
isbn = {0-7803-9553-0},
month = {oct},
organization = {Wroclaw, Poland},
pages = {134--139},
title = {{Polymer overmoulding for microfluidic device packaging and system integration}},
year = {2005}
}
@misc{Coia2018,
abstract = {Introduction to writing R packages},
author = {Coia, Vincenzo},
booktitle = {Stat545},
title = {{Write your own R package}},
url = {http://stat545.com/packages06{\_}foofactors-package.html},
urldate = {2018-11-08},
year = {2018}
}
@article{Ritchey2007,
abstract = {Morphological analysis is a method for rigorously structuring and investigating the total set of relationships in inherently non-quantifiable socio-technical problem complexes (variously called “wicked problems” and “social messes”1 ). The method is carried out by developing a discrete parameter space of the problem complex to be investigated, and defining relationships between the parameters on the basis of internal consistency. Such an internally linked parameter space is called a morphological field. With proper computer support, a morphological field can be treated as an inference model},
author = {Ritchey, Tom},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 2007 - Futures Studies using Morphological Analysis.pdf:pdf},
journal = {UN University Millennium Project: Futures Research Methodology Series},
pages = {2005--2007},
title = {{Futures Studies using Morphological Analysis}},
url = {www.swemorph.com http://www.swemorph.com/pdf/futures.pdf},
year = {2007}
}
@article{Maier1998,
annote = {This is the paper in which the five characteristics of, or criteria for, Systems of Systems are defined. It is the most frequently cited paper for SoS definition.

Managerial independence of the elements (component systems are created by different, independent teams, and maybbe owned by different organisations)
Operational independence of the elements (component systems interconnected, but each system can operate without the rest)
Stable intermediate forms (a varietyof stable forms, in both time and space, are explicit in the design and in operation)
Policy triage (enterprise environment determines how component systems are created and controlled)
Leverage at the interfaces (not in the component systems, which are operated to be efficient and efficacious)
Ensuring collaboration (largely achieved through socio-technical methods of command and control)

See file attachment for example of his criteria

Definition of SoS:
"The elements of the system are themselves sufficiently complex to be considered systems

Operating together the systems produce functions and fulfil purposes not produced or fulfilled by the elements alone

The elements possess operational independence. Each element fulfils useful purposes whether or nor connected to the assemblage. If disconnected the element continues to fulfil useful purposes

The elements possess managerial independence. Each element is managed, at least in part, for its own purposes rather than the purposes of the collective."

• Policy triage. Classic examples of good triage choice are in technical standards. For example, the Motion Picture Experts Group (MPEG) chose to only standardize the information needed to decompress a digital video
stream [Chiariglione, 1998]. The standard defines the format of the data stream, and the operations required to reconstruct the stream of moving picture frames. However, the compression process is deliberately left
undefined. By standardizing decompression the usefulness of the standard for interoperability was assured. By not standardizing compression the standard leaves open a broad area for the firms collaborating on the standard
to continue to compete. Interoperability increases the size of the market, a benefit to the whole collaborative group, while retaining a space for competition eliminates a reason to not collaborate with the group. Broad collaboration was essential both to ensure a large market, and to ensure that the requisite intellectual property would be offered for license by the participants.

• Leverage at the Interfaces. The greatest leverage in system architecting is at the interfaces. The greatest dangers are also at the interfaces. When the components of a system-of-systems are highly independent, operationally and managerially, the architecture of the system-of-systems is the interfaces. There is nothing else to architect. The Internet is the interfaces, in this case the Internet Protocol (IP).

• Ensuring Cooperation. If a system requires voluntary collaboration, the mechanism and incentives for that collaboration must be designed in. Later, he specifically mentions the socio-technical approach.

Summarises these as (p.283):

1. Stable Intermediate Forms: A collaborative system designer must pay closer attention to the intermediate steps in a planned evolution. The collaborative system will take on intermediate forms dynamically and without direction, as part of its nature. Thus, careful attention must be paid to the existence and stability (in all suitable dimensions) of partial assemblages of components. [note that he doesn't asddress the issue of over-competence; components that can to some extent do other components' work as a resilience measure. But this could be catastrophic if security prevents information being moved to other components.]
2. Policy Triage: The collaborative system designer will not have coercive control over the systems
configuration and evolution. This makes choosing the points at which to influence the design more important. In communication-centric systems, this means that design leverage will frequently be found in relatively abstract
components (like data standards and network protocols). [Doesn't discuss meta-interfaces, where the actual interface is negotiated at the meta-level at the time.]
3. Leverage at the Interfaces: A collaborative system is defined by its interfaces. The interfaces, whether thought of as the actual physical interconnections or as higher level service abstractions, are the primary points at which the designer can exert control.
4. Ensuring Cooperation: A collaborative system exists because the partially independent elements decide to collaborate. The designer must consider why they will choose to collaborate and foster those reasons in the design. This is not a consideration in the design of monolithic systems where the components can operate only as part of the whole.

Discusses some examples of these principles in action. E.g. the Internet (p.277):
The Internet and System-of-Systems Properties
Discriminating Factor - Applicability
• Managerial independence of the elements - Component systems as acquired and operated by independent users. Component systems
are developed (largely) by commercial firms following market dictates
• Operational independence of the elements - Operational coordination is through voluntary adherence to technical standards. The standard setting process is also voluntary. The systems defense against noncooperators
is only to exclude them. In the Internets earlier stages of development it was more
deliberately run by the U.S Government. Government sponsored projects continue to
be important to the Internets development
• Stable intermediate forms - The structure of the Internet is dynamic, with nodes being added and removed
continuously and on their own volition. The main protocols are designed to allow
evolution through replacement. The core protocol, IP, is now at version 4 with
migration to version 6 beginning
• Policy triage - The oversight bodies exercise very limited control, and carefully restrict their control to
the network. Applications and underlying physical interconnects are controlled
separately, if at all
• Leverage at the interfaces - The architecture of the Internet is its interfaces. Nothing else is constant
• Ensuring collaboration - The system fosters collaboration through low entry costs and benefits to cooperation. However, it is much weaker at excluding deliberate noncooperators, to the detriment of
the system. This is a byproduct of its original development environment
• Classification - Collaborative The system began with a directed purpose, but now follows purposes imposed upon it by its users. Operation and development is through the collaboration (largely voluntary) of its participants

From these examples he draws guidance (p.281):
• From leverage at the interfaces we conclude that interfaces are the architecture. If components are procured semi-independently then the standards of communication are more important than any particular component system.
• From policy triage we conclude that not everything can be standardized or defined. The points of leverage must be discerned and the architects resources applied sparingly.
• From stable intermediate forms we conclude that the interfaces must support severability in either vertical or horizontal directions. Vertical severability means the ability to remove or add a physical component to the system-of-systems. Horizontal severability means the ability to add or remove applications or functions to the system-of-systems independently of the physical components.
• From ensuring collaboration we conclude that attention must be paid to how the participating components derive value from participation.

Then discusses communication stacks; have a feeling he's missed part of the point here. While he discusses standards for the semantic content of mesages, this dosn't cover what happens to the semantics after the message has arrived. Doesn't cover the ned to ensure that the semantics of what is in the database of System A remains the same when the contents are transferred round the rest of the SoS, when conflation, elaboration, condensation, rearrangement and clipping might occur on the way.},
author = {Maier, M W},
journal = {Systems Engineering},
keywords = {definitions,reference architectures,software,systems},
number = {4},
pages = {267--284},
title = {{Architecting principles for systems-of-systems}},
volume = {1},
year = {1998}
}
@article{Wit2018,
abstract = {Despite the ubiquity of Big Data in the modern scientific discourse, most references describe storage and query considerations and rarely full-flexed analyses. In this article, we propose another definition with particular relevance to biometrics. We argue that the complexity of the generating measure of biological process means that the model complexity of any statistical model will have to be smaller. Only, when the model is used for prediction can we have any hope that the number of available features reasonably outnumbers the desired complexity of the model.},
author = {Wit, Ernst C.},
doi = {10.1016/J.SPL.2018.02.039},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wit - 2018 - Big data and biostatistics The death of the asymptotic Valhalla.pdf:pdf},
issn = {0167-7152},
journal = {Statistics {\&} Probability Letters},
month = {may},
pages = {30--33},
publisher = {North-Holland},
title = {{Big data and biostatistics: The death of the asymptotic Valhalla}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300841?dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {136},
year = {2018}
}
@article{Phaal2011,
annote = {id: 1; issn: print 00401625; publication{\_}type: full{\_}text},
author = {Phaal, R and O'Sullivan, E and Routley, M and Ford, S and Probert, D},
doi = {10.1016/j.techfore.2010.06.018},
issn = {00401625},
journal = {Technological Forecasting and Social Change},
number = {2},
pages = {217 {\textless}last{\_}page{\textgreater} 230},
title = {{A framework for mapping industrial emergence }},
url = {http://dx.doi.org/10.1016/j.techfore.2010.06.018},
volume = {78},
year = {2011}
}
@misc{Hochachka2012,
abstract = {Identifying ecological patterns across broad spatial and temporal extents requires novel approaches and methods for acquiring, integrating and modeling massive quantities of diverse data. For example, a growing number of research projects engage continent-wide networks of volunteers ('citizen-scientists') to collect species occurrence data. Although these data are information rich, they present numerous challenges in project design, implementation and analysis, which include: developing data collection tools that maximize data quantity while maintaining high standards of data quality, and applying new analytical and visualization techniques that can accurately reveal patterns in these data. Here, we describe how advances in data-intensive science provide accurate estimates in species distributions at continental scales by identifying complex environmental associations. {\textcopyright} 2011 Elsevier Ltd.},
author = {Hochachka, Wesley M. and Fink, Daniel and Hutchinson, Rebecca A. and Sheldon, Daniel and Wong, Weng Keen and Kelling, Steve},
booktitle = {Trends in Ecology and Evolution},
doi = {10.1016/j.tree.2011.11.006},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hochachka et al. - 2012 - Data-intensive science applied to broad-scale citizen science.pdf:pdf},
isbn = {0169-5347},
issn = {01695347},
month = {feb},
number = {2},
pages = {130--137},
pmid = {22192976},
publisher = {Elsevier Current Trends},
title = {{Data-intensive science applied to broad-scale citizen science}},
url = {https://www.sciencedirect.com/science/article/pii/S0169534711003296},
volume = {27},
year = {2012}
}
@article{DeMeyer2002,
annote = {Paper in Important Documents and in T-AREA-SoS; this is the marketing version of the more mathematical paper Pich, Loch, DeMeyer, q.v. Quote both in future. 

Compare with Hoffman 2016 for alliance styles; overlaps with this.

See also Wojcik for an analgous discussion in context of SoS. And Daw 2007 for another. Note that later categories correspond to wicked problems. See Vicente, Dekker as well

Distinguishes between 4 project environments:

• variation (Gantt charts and PERT charts are appropriate - aim is to resore 'normality'. variation tends to be in delivery dates, or minor quality variations)
• foreseen uncertainty (you know your competitors are going to bring out a new product and you will have to react to whatever it is - but you can guess likely attributes. requires risk management techniques and decision trees, to enable you quickly to adjust to whatever happens. agility required)
• unforeseen uncertainty (i.e. there is no Plan B. Viagra is an example; originally a blood pressure drug, but unexpected opportunity arose, requiring radical shift in testing and development programme. Project managers need to be flexible orchestrators, networkers, and ambassadors. Trust is critical. Scan horizons continuously, and sign flexible contracts.)
• chaos (usually due to complexity of links into the environment, extending internal complexity. Project manager must be an entrepreneur and knowledge manager. depends critically on having long-term relationships beyond individual projects. project relationships must be based on trust and handshakes. Continuous, ruthless concentration on go/nogo decisions. plan only to next decision point. prototype rapidly. )

Nothing startling in this, but codifies the field quite nicely. Perhaps need to build this into the CDDP},
author = {DeMeyer, A and Loch, C H and Pich, M T},
journal = {Sloan Management Review},
keywords = {Clever,SIMPLOFI,competencies,competitive challenges,complexity,control,core competences,corporate governance,culture,design process,empowerment,ethics,federated control,harvard business school,innovation,knowledge configuration,knowledge management,knowledge structures,learning organisation,measurement,methods,networks,org. trust,organisational change,product introduction process,projects,prototyping,readiness,strategic planning,strategy,supply chain,systems,teams,theory},
number = {2},
pages = {60--67},
title = {{Managing project uncertainty: from variation to chaos}},
volume = {43},
year = {2002}
}
@inproceedings{Gubanov2017,
abstract = {The big data era brought us petabytes of data together with the challenges of storing and efficiently accessing large-scale datasets. However, it unexpectedly surprised everyone with an enormous variety of data sources and types, and corresponding different data models. Data integration, a mature field addressing problems of accessing and fusing data residing in more than one data source, over the years came up with feasible semi-automatic solutions, most of which efficiently handle a handful of data sources represented in one or two different data models. Most of the solutions do not easily scale up, since they usually require some sort of human assistance, infeasible at scale. Unified Famous Object (UFO) Having trained many such UFOs, the system would get more and more powerful, as it learned to recognize and access data objects across many sources without supervision. While UFO was definitely progress towards scaling up data fusion, it was not a "silver bullet", because it was built mostly for relational data. Hence, there is a need for a new large-scale data integration system that could handle many types of data at scale. This paper sketches its architecture, and envisions potential research challenges. First, a few key principles that such a system should follow are described, then follows the discussion on architecture and research avenues.},
author = {Gubanov, M},
booktitle = {Data Engineering (ICDE), 2017 IEEE 33rd International Conference on},
doi = {0.1109/ICDE.2017.230},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gubanov - 2017 - Polyfuse A large-scale hybrid data fusion system.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {1575--1578},
publisher = {IEEE},
title = {{Polyfuse: A large-scale hybrid data fusion system}},
url = {http://ieeexplore.ieee.org/abstract/document/7930127/},
year = {2017}
}
@article{Aris2009,
abstract = {Gaining a rapid overview of an emerging scientific topic, sometimescalled researchfronts, is anincreasinglycom- mon task due to the growing amount of interdisciplinary collaboration. Visual overviews that show temporal pat- terns of paper publication and citation links among papers can help researchers and analysts to see the rate of growth of topics, identify key papers, and understand influences across subdisciplines. This article applies a novel network-visualization tool based on meaningful layouts of nodes to present research fronts and show citation links that indicate influences across research fronts.To demonstrate the value of two-dimensional lay- outs with multiple regions and user control of link visi- bility, we conducted a design-oriented, preliminary case study with 6 domain experts over a 4-month period. The main benefits were being able (a) to easily identify key papers and see the increasing number of papers within a research front, and (b) to quickly see the strength and direction of influence across related research fronts.},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Aris, Aleks and Shneiderman, Ben and Qazvinian, Vahed and Radev, Dragomir},
doi = {10.1002/asi.21160},
eprint = {0803.1716},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Aris et al. - 2009 - Visual overviews for discovering key papers and influences across research fronts.pdf:pdf},
isbn = {1532-2890},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
month = {nov},
number = {11},
pages = {2219--2228},
pmid = {502955140},
publisher = {Wiley-Blackwell},
title = {{Visual overviews for discovering key papers and influences across research fronts}},
url = {http://doi.wiley.com/10.1002/asi.21160},
volume = {60},
year = {2009}
}
@article{Kruis2016,
abstract = {Statistical models that analyse (pairwise) relations between variables encompass assumptions about the underlying mechanism that generated the associations in the observed data. In the present paper we demonstrate that three Ising model representations exist that, although each proposes a distinct theoretical explanation for the observed associations, are mathematically equivalent. This equivalence allows the researcher to interpret the results of one model in three different ways. We illustrate the ramifications of this by discussing concepts that are conceived as problematic in their traditional explanation, yet when interpreted in the context of another explanation make immediate sense.},
author = {Kruis, Joost and Maris, Gunter},
doi = {10.1038/srep34175},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kruis, Maris - 2016 - Three representations of the Ising model.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
month = {oct},
publisher = {Nature Publishing Group},
title = {{Three representations of the Ising model}},
volume = {6},
year = {2016}
}
@incollection{Siemieniuch2015a,
address = {London},
annote = {Resilience chapter; based around examples. One of our better chapters.

Aisin Seiki
Muzafferabad arthquake
Nokia-Erecsson
East Barnet electricity

Generic Lessons:
1 The exemplars above all show that resilience efforts have people at their core. It is true that these people may be supported by a vast array of tech-nology and technical services, but it is the people that drive the processes. The U.S. Air force long ago developed the ‘OODA Loop'; Observe, Orient, Decide, Act, and this decision-making loop is a key to resilience activi-ties. It is because people are the repositories of much formal and tacit knowledge, both foreground and background, and are the only sources of authority and responsibility over resources; this places them at the centre of the effort, executing the OODA loop. In most cases there will be a team involved (because only for very small crises will one person have all the requisite knowledge and authority); since agility is likely to be important, it follows that trust is a fundamental property for such teams. Trust here refers to trust within the team (i.e., that people will be honest about what they can and cannot do, and that when people promise to do something, they will deliver on that promise. It also refers to trust between people and the technology and technical resources that they employ (Blomqvist 1997, Whitworth and Moor 2003).

2 Recovering from a complex, disruptive, and likely distributed event may have the characteristics of a ‘wicked' problem (Rittel and Webber 1973, Daw 2007, Siemieniuch and Sinclair 2014). Two characteristics of wicked problems are that firstly, the class of solution that restores ‘fitness for pur-pose' in the definition of resilience quoted above, depends on the initial starting point; the recognition that an event either has occurred or is about to occur, and the identification of this event. This requires continuous, holistic situation awareness, perhaps best summarised by a business quota-tion from the 1990s: ‘Be afraid', attributed to the INTEL™ CEO, Andy Grove. Secondly, an appropriate, resilient solution may not be evident until it is achieved. This problem may be curbed by the efficient and timely han-dling and classification of disaggregated data, coupled with a focus on local teams, the training of these teams and the provision of support, all of which will reduce many of the confusions characteristic of wicked problems and will reduce them to a more manageable scope.

3 Tracking and maintenance of disaggregated data. Aggregated data is of little use when systems or components are in danger of failing or have failed because aggregation removes the detail in which many little warnings of imminent, or actual, failure will be found. Timely, organised, informative, disaggregated data is what is essential, but it needs information and com-munication systems for its collection, organisation and presentation for dis-aggregated data to be available and useful.

4 There is a need for local daily assessment of the needs of distributed, localised teams with local knowledge working to deliver their part of a resilient solution, and then to deliver the support that is needed to them. This creates a requirement for a flexible, ‘extreme programming' kind of approach to finding a solution that turns out to be ‘fit for purpose'; an example is the SCRUM approach, applicable to resilience (Schwaber 2004).

5 For local teams to be effective, they have to know a priori what to do, where they can call on what types of information and what support they can expect. From a local perspective, the SCRUM approach (or similar) needs to be embedded (Beck and Andres 2004). But for this to work, trained people, in positions of authority and responsibility and with resources to exercise them, are essential.

6 The internet is vital for coordination, but only with appropriate protocols, procedures and information security provision. Furthermore, the realisation of the Internet of Things (more accurately the Internet of Things, People and Services) may become a significant resource for resil-ience activities, given that the enterprise may have difficulties in mobil-ising its own resources quickly enough. Few organisations, in whatever field they operate, can afford to have spare resources always available for emergencies.

7 Feedback is essential to people, teams and organisations involved in resilience activities, so open access and reports on status are necessary. Note that this enables transparency, accountability and participation; however, security will then become a significant issue. Feedback is also necessary for any external organisations that become involved in the resilience effort; donors, governments and other capability suppliers will require reassurance that their efforts are being utilised appropriately, and are not being appropriated for other purposes.},
author = {Siemieniuch, C E and Sinclair, M A and Henshaw, M J de C and Hubbard, E.-M.},
booktitle = {Organisational Resilience: Concepts, Integration, and Practice},
chapter = {9},
editor = {Bhamra, R S},
isbn = {9781482233568},
keywords = {Governance,agility,communications,complexity,engineering,ergonomics,ergonomics self-help,evaluation,federated control,green,information systems,metrics/metrication,network-centric warfare,networks,reliability,resilience},
pages = {175--200},
publisher = {CRC Press},
title = {{Designing both systems and systems of systems to exhibit resilience}},
year = {2015}
}
@misc{Wettenhall2017,
abstract = {In terface for producing message boxes and other dialoges within R.},
author = {Wettenhall, James and Grosjean, Philippe},
title = {{Message boxes in R tcltk – SciViews}},
url = {http://www.sciviews.org/recipes/tcltk/TclTk-message-boxes/},
urldate = {2018-02-27},
year = {2017}
}
@misc{mustache2019,
abstract = {Logic-less templates},
author = {Annon},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Annon - 2019 - mustache(5) - Logic-less templates.pdf:pdf},
pages = {6},
title = {{mustache(5) - Logic-less templates.}},
url = {https://mustache.github.io/mustache.5.html},
urldate = {2019-11-04},
year = {2019}
}
@article{Greenland2016,
abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
doi = {10.1007/s10654-016-0149-3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Greenland et al. - 2016 - Statistical tests, P values, confidence intervals, and power a guide to misinterpretations.pdf:pdf},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Confidence intervals,Hypothesis testing,Null testing,P value,Power,Significance tests,Statistical testing},
month = {apr},
number = {4},
pages = {337--350},
publisher = {Springer Netherlands},
title = {{Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations}},
volume = {31},
year = {2016}
}
@article{Abbott2016,
abstract = {On September 14, 2015 at 09:50:45 UTC the two detectors of the Laser Interferometer Gravitational-Wave Observatory simultaneously observed a transient gravitational-wave signal. The signal sweeps upwards in frequency from 35 to 250 Hz with a peak gravitational-wave strain of 1.0 × 10 − 21 . It matches the waveform predicted by general relativity for the inspiral and merger of a pair of black holes and the ringdown of the resulting single black hole. The signal was observed with a matched-filter signal-to-noise ratio of 24 and a false alarm rate estimated to be less than 1 event per 203 000 years, equivalent to a significance greater than 5.1 $\sigma$ . The source lies at a luminosity distance of 41 0 + 160 − 180 Mpc corresponding to a redshift z = 0.0 9 + 0.03 − 0.04 . In the source frame, the initial black hole masses are 3 6 + 5 − 4 M ⊙ and 2 9 + 4 − 4 M ⊙ , and the final black hole mass is 6 2 + 4 − 4 M ⊙ , with 3. 0 + 0.5 − 0.5 M ⊙ c 2 radiated in gravitational waves. All uncertainties define 90{\%} credible intervals. These observations demonstrate the existence of binary stellar-mass black hole systems. This is the first direct detection of gravitational waves and the first observation of a binary black hole merger.},
author = {Abbott, B. P. and Abbott, R. and Abbott, T. D. and And others},
doi = {10.1103/PhysRevLett.116.061102},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Abbott et al. - 2016 - Observation of Gravitational Waves from a Binary Black Hole Merger.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {feb},
number = {6},
pages = {061102},
title = {{Observation of Gravitational Waves from a Binary Black Hole Merger}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.116.061102},
volume = {116},
year = {2016}
}
@inproceedings{palmer2000anindustry,
author = {Palmer, P J and Williams, D J},
booktitle = {Microelectronics International},
isbn = {0953585808},
issn = {1356-5362},
number = {(1)},
organization = {Harrogate, Yorkshire},
pages = {13--16},
title = {{An Analysis of Technology Trends within the Electronics Industry}},
volume = {17},
year = {2000}
}
@article{phaal2010technologydialogue,
abstract = {Successful innovation requires effective communication within and between technical and nontechnical communities, which can be challenging due to different educational backgrounds, experience, perceptions, and attitudes. Roadmapping has emerged as a method that can enable effective dialogue between these groups, and the way in which information is structured is a key feature that enables this communication. This is an area that has not received much attention in the literature, and this article seeks to address this gap by describing in detail the structures that have been successfully applied in roadmapping workshops and processes, from which key learning points and future research directions are identified.},
author = {Phaal, R and Palmer, P J},
issn = {1042-9247},
journal = {EMJ - Engineering Management Journal},
month = {mar},
number = {1},
pages = {64--74},
title = {{Technology management - Structuring the strategic dialogue}},
volume = {22},
year = {2010}
}
@article{Medvedev2017,
abstract = {The conventional technologies and methods are not able to store and analyse recent data that come from different sources: various devices, sensors, networks, transactional applications, the web, and social media. Due to a complexity of data, data mining methods should be implemented using the capabilities of the Cloud technologies. In this paper, a new web-based solution named DAMIS, inspired by the Cloud, is proposed and implemented. It allows making massive data mining simpler, effective, and easily understandable for data scientists and business intelligence professionals by constructing scientific workflows for data mining using a drag and drop interface. The usage of scientific workflows allows composing convenient tools for modelling data mining processes and for simulation of real-world time- and resource-consuming data mining problems. The solution is useful to solve data classification, clustering, and dimensionality reduction problems. The DAMIS architecture is designed to ensure easy accessibility, usability, scalability, and portability of the solution. The proposed solution has a wide range of applications and allows to get deep insights into the data during the process of knowledge discovery.},
author = {Medvedev, Viktor and Kurasova, Olga and Bernatavi{\v{c}}ienė, Jolita and Treigys, Povilas and Marcinkevi{\v{c}}ius, Virginijus and Dzemyda, Gintautas},
doi = {10.1016/j.simpat.2017.03.001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Medvedev et al. - 2017 - A new web-based solution for modelling data mining processes.pdf:pdf},
issn = {1569190X},
journal = {Simulation Modelling Practice and Theory},
keywords = {Cloud computing,Data mining,Dimensionality reduction,High-performance computing,Modelling data mining process,Scientific workflow},
month = {aug},
pages = {34--46},
publisher = {Elsevier},
title = {{A new web-based solution for modelling data mining processes}},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X17300485?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {76},
year = {2017}
}
@article{Ritchey2012,
abstract = {The purpose of this article is to classify and compare – in essence to model – a number of different types of modelling methods employed within Operations Research and the Management Sciences (OR/MS). The classification of these methods is based on a selected number of generally recognised modelling proper- ties. On the basis of this meta-model, requirements for the successful application of different modelling meth- ods – for the study of given systems or objects of scientific enquiry – can be examined. The method employed for this meta-modelling task is General Morphological Analysis (GMA). The problem of a General Theory of Modelling (GTM) is also discussed. Keywords:},
author = {Ritchey, Tom},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 2012 - Outline for a Morphology of Modelling Methods Contribution to a General Theory of Modelling.pdf:pdf},
isbn = {2001-2241},
issn = {2001-2241},
journal = {Acta Morphologica Generalis AMG Vol},
keywords = {OR methods,general theory of modelling,modelling methods,morphological analysis},
number = {1},
pages = {1--20},
title = {{Outline for a Morphology of Modelling Methods: Contribution to a General Theory of Modelling}},
url = {http://www.amg.swemorph.com/pdf/amg-1-1-2012.pdf http://www.swemorph.com/pdf/amg-1-1-2012.pdf},
volume = {1},
year = {2012}
}
@article{Berger2007,
author = {Berger, Roger L},
journal = {Computational Statistics {\&} Data Analysis},
number = {6},
pages = {2788--2791},
publisher = {Elsevier},
title = {{Nonstandard operator precedence in Excel}},
volume = {51},
year = {2007}
}
@inbook{Cavanillas2016,
abstract = {Big data is expected to impact all sectors, from healthcare to media, from energy to retail. The ability to effectively manage information and extract knowledge is now seen as a key competitive advantage for organizations. This chapter explores the value potential of big data with a particular focus on the European context. The chapter identifies the positive transformational potential of big data within a number of key sectors and highlights the need for a clear strategy to increase the competitiveness of European industries in order to drive innovation and competitiveness. Europe needs to foster the development and wide adoption of big data technologies, value adding use cases, and sustainable business models through a Big Data Ecosystem. Finally the chapter describes the key dimensions, including skills, legal, business, and social, that need to be addressed in a European Big Data Ecosystem.},
address = {Cham},
author = {Cavanillas, Jos{\'{e}} Mar{\'{i}}a and Curry, Edward and Wahlster, Wolfgang},
booktitle = {New Horizons for a Data-Driven Economy: A Roadmap for Usage and Exploitation of Big Data in Europe},
doi = {10.1007/978-3-319-21569-3_1},
editor = {Cavanillas, Jos{\'{e}} Mar{\'{i}}a and Curry, Edward and Wahlster, Wolfgang},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cavanillas, Curry, Wahlster - 2016 - The Big Data Value Opportunity.pdf:pdf},
isbn = {978-3-319-21569-3},
pages = {3--11},
publisher = {Springer International Publishing},
title = {{The Big Data Value Opportunity}},
url = {https://doi.org/10.1007/978-3-319-21569-3{\_}1},
year = {2016}
}
@article{Briscoe2001,
abstract = {We review the physiological, molecular, and neural mechanisms of insect color vision. Phylogenetic and molecular analyses reveal that the basic bauplan, UV-blue-green-trichromacy, appears to date back to the Devonian ancestor of all pterygote insects. There are variations on this theme, however. These concern the number of color receptor types, their differential expression across the retina, and their fine tuning along the wavelength scale. In a few cases (but not in many others), these differences can be linked to visual ecology. Other insects have virtually identical sets of color receptors despite strong differences in lifestyle. Instead of the adaptionism that has dominated visual ecology in the past, we propose that chance evolutionary processes, history, and constraints should be considered. In addition to phylogenetic analyses designed to explore these factors, we suggest quantifying variance between individuals and populations and using fitness measurements to test the adaptive value of traits identified in insect color vision systems. },
address = {Department of Molecular and Cellular Biology, University of Arizona, Tucson, Arizona 85721, USA. abriscoe@u.arizona.edu},
annote = {id: 3; LR: 20101118; JID: 0372367; 0 (Retinal Pigments); RF: 172; ppublish},
author = {Briscoe, A D and Chittka, L},
doi = {10.1146/annurev.ento.46.1.471 [doi]},
issn = {0066-4170; 0066-4170},
journal = {Annual Review of Entomology},
keywords = {Adaptation,Animals,Biological Evolution,Color Perception/physiology,Humans,Insects/classification/physiology,Invertebrate,Photoreceptor Cells,Phylogeny,Physiological/physiology,Retinal Pigments/physiology},
pages = {471--510},
title = {{The evolution of color vision in insects }},
volume = {46},
year = {2001}
}
@techreport{Zeileis,
abstract = {MOB is a generic algorithm for model-based recursive partitioning (Zeileis, Hothorn, and Hornik 2008). Rather than fitting one global model to a dataset, it estimates local models on subsets of data that are "learned" by recursively partitioning. It proceeds in the following way: (1) fit a parametric model to a data set, (2) test for parameter instability over a set of partitioning variables, (3) if there is some overall parameter instability, split the model with respect to the variable associated with the highest instability, (4) repeat the procedure in each of the resulting subsamples. It is discussed how these steps of the conceptual algorithm are translated into computational tools in an object-oriented manner , allowing the user to plug in various types of parametric models. For representing the resulting trees, the R package partykit is employed and extended with generic infrastructure for recursive partitions where nodes are associated with statistical models. Compared to the previously available implementation in the party package, the new implementation supports more inference options, is easier to extend to new models, and provides more convenience features. To implement the model-based recursive partitioning (MOB) algorithm of Zeileis et al. (2008) in software, infrastructure for three aspects is required: (1) statistical "models", (2) recursive "party"tions, and (3) "mobsters" carrying out the MOB algorithm. Along with Zeileis et al. (2008), an implementation of all three steps was provided in the party package (Hothorn, Hornik, Strobl, and Zeileis 2015) for the R system for statistical computing (R Core Team 2013). This provided one very flexible mob() function combining party's S4 classes for representing trees with binary splits and the S4 model wrapper functions from modeltools (Hothorn, Leisch, and Zeileis 2013). However, while this supported many applications of interest, it was somewhat limited in several directions: (1) The S4 wrappers for the models were somewhat cumbersome to set up. (2) The tree infrastructure was originally designed for ctree() and somewhat too narrowly focused on it. (3) Writing new "mobster" interfaces was not easy because of using unexported S4 classes. Hence, a leaner and more flexible interface (based on S3 classes) is now provided in partykit (Hothorn and Zeileis 2015): (1) New models are much easier to provide in a basic version and customization does not require setting up an additional S4 class-and-methods layer anymore. (2) The trees are built on top of partykit's flexible 'party' objects, inheriting many useful},
author = {Zeileis, Achim},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zeileis - Unknown - Parties, Models, Mobsters A New Implementation of Model-Based Recursive Partitioning in R Torsten Hothorn.pdf:pdf},
keywords = {object-orientation,parametric models,recursive partitioning},
title = {{Parties, Models, Mobsters: A New Implementation of Model-Based Recursive Partitioning in R Torsten Hothorn}}
}
@misc{Poblet2017,
abstract = {Mobile technologies, web-based platforms, and social media have transformed the landscape of disaster management by enabling a new generation of digital net-works to produce, process, and analyse georeferenced data in real time. This unprecedented convergence of geomobile technologies and crowdsourcing methods is opening up multiple forms to participate in disaster man-agement tasks. Based on empirical research, this paper first proposes a conceptualisation of crowdsourcing roles and then analyses methods and tools based on a combi-nation of two variables: (i) types of data being processed; (ii) involvement of the crowds. The paper also surveys a number of existing platforms and mobile apps leveraging crowdsourcing in disaster and emergency management with the aim to contribute to the discussion on the advan-tages and limits of using crowdsourcing methods and tools in these areas.},
author = {Poblet, Marta and Garca-Cuesta, Esteban and Casanovas, Pompeu},
booktitle = {Information Systems Frontiers},
doi = {10.1007/s10796-017-9734-6},
issn = {15729419},
keywords = {Crowdsourcing,Data management,Disaster management,Microtasking,Mobile technologies,Online platforms,Ontologies},
month = {jan},
pages = {1--17},
title = {{Crowdsourcing roles, methods and tools for data-intensive disaster management}},
url = {http://link.springer.com/10.1007/s10796-017-9734-6},
year = {2017}
}
@article{Hulme2011,
abstract = {Most researchers working in applied ecology are aware that much of what is published in leading ecological and environmental science journals makes little difference to the day-to-day management of species and ecosystems (Nature 2007). In recent years, several editorials have made this point and attempted to identify a way forward to bridge ‘The Great Divide' (Born, Boreux {\{}{\&}{\}} Lawes 2009; Milner-Gulland et al. 2010; Memmott et al. 2010). The onus has largely been on the scientific community to communicate the value of its science more clearly, become increasingly involved in extension activities and, heaven forbid, step down from their ivory towers and get their hands dirty. Yet the reality is that many scientists are doing this already, several very successfully (Possingham 2009). A more serious concern is that academic journals are simply not the best medium to communicate practical messages to a wide audience who need specific solutions to particular problems that have to be delivered on a tight budget. We should not be surprised; much academic research aims to be innovative, internationally competitive and globally relevant – aims which are not always congruent with finding practical solutions. It is against these criteria that many editorial decisions regarding whether or not to accept a paper for publication are made. Irrespective of how much hand-wringing might take place among editors this situation is unlikely to change as publishers judge the viability of journals using bibliometric indices and numbers of institutional subscriptions. Yet, potentially there is another way. Communication is a two-way street, even if much of the academic traffic is heading in one direction with no clear destination. So how do scientific researchers hear about the concerns and needs of those tackling problems in the field? Surveys of stakeholders are certainly one way to confirm that they feel the scientific community is not listening to them (e.g. Andreu, Vil{\{}{\`{a}}{\}} {\{}{\&}{\}} Hulme 2009) but questionnaires do not address the problem. An alternative is to provide an opportunity within the pages of academic journals for non-standard pieces to be written by individuals who have a different perspective on what is needed in applied ecology research and whether the papers published in academic journals get anywhere near it. With this aim, the Journal of Applied Ecology launches its first ‘Practitioner's Perspective'. These ‘prick our conscience' pieces can be contributed by anyone who has a strong opinion on the current state of applied ecology research, whether academic or not, as long as they can provide an original perspective and a constructive way forward. Although practitioners have been identified as a distinct group of actors in applied ecology that ‘buy land, put up fences, set fires, put out fires, lobby politicians, negotiate with farmers, spray invasive weeds, poison rats and guard against poachers' (Nature 2007), we are not placing restrictions on who is or is not a ‘practitioner'. Thus, we welcome pieces from academics (at least those with a bit of dirt under their fingernails) as well as civil servants, environmental consultants, park managers and environmental lobbyists. The truth is we are unsure what to expect in terms of submissions under this new feature, hopefully provocative pieces from writers whose voices are rarely heard in our journal. To kick-start this initiative we have commissioned a few articles that might give a flavour of the pieces we would like to see published in the future. Our greatest challenge to date has been to prevent these pieces from becoming advertorials for the activities of NGOs, conservation groups or consultancies. This is certainly not what we want, but we do welcome examples of best practice that may not have made it into the wider academic literature. The first Practitioner's Perspective appears in this issue (Goulson et al. 2011) and illustrates the viewpoint of the Bumblebee Conservation Trust, although the lead author is a senior academic at Stirling University, UK. Hopefully, in addition to highlighting how science informs the conservation of bumblebees it will challenge readers to consider what more needs to be done. We encourage future submissions under Practitioner's Perspectives but please be sure to contact the Editors to discuss your piece beforehand. There is no prescribed structure to Practitioner's Perspectives apart from our hope that they will be thought-provoking and challenge the science community to consider the perspectives of those individuals addressing applied ecological issues. However, authors may wish to consider covering the activities of the individual or organization with regard to ecological management, the key issues they are addressing (see Sutherland et al. 2006, 2009 for a range of key questions), the extent to which applied ecological research has supported their activities (if at all), how future research might assist them to address ecological problems more effectively and how this might best be achieved (e.g. through greater dialogue, joint projects, new research techniques etc.). Jump to{\{}{\ldots}{\}} References 0. 0. Andreu, J., Vil{\{}{\`{a}}{\}}, M. {\{}{\&}{\}} Hulme, P.E. (2009) An assessment of stakeholder perceptions and management of alien plants in Spain. Environmental Management, 43, 1244–1255. CrossRef, PubMed, Web of Science{\{}{\textregistered}{\}} Times Cited: 19, ADS Born, J., Boreux, V. {\{}{\&}{\}} Lawes, M.J. (2009) Synthesis: sharing ecological knowledge – the way forward. Biotropica, 41, 586–588. Direct Link: Abstract Full Article (HTML) PDF(57K) References Web of Science{\{}{\textregistered}{\}} Times Cited: 3 Goulson, D., Rayner, P., Dawson, R. {\{}{\&}{\}} Darvill, B. (2011) Translating research into action; bumblebee conservation as a case study. Journal of Applied Ecology, 48, 3–8. Direct Link: Abstract Full Article (HTML) PDF(171K) References Web of Science{\{}{\textregistered}{\}} Times Cited: 4 Memmott, J., Cadotte, M., Hulme, P.E., Kerby, G., Milner-Gulland, E.J. {\{}{\&}{\}} Whittingham, M.J. (2010) Editorial: putting applied ecology into practice. Journal of Applied Ecology, 47, 1–4. Direct Link: Abstract Full Article (HTML) PDF(140K) References Web of Science{\{}{\textregistered}{\}} Times Cited: 10 Milner-Gulland, E.J., Fisher, M., Browne, S., Redford, K.H., Spencer, M. {\{}{\&}{\}} Sutherland, W.J. (2010) Do we need to develop a more relevant conservation literature?Oryx, 44, 1–2. CrossRef, Web of Science{\{}{\textregistered}{\}} Times Cited: 7 Nature (2007) The great divide. Nature, 450, 135–136. CrossRef, CAS, Web of Science{\{}{\textregistered}{\}}, ADS Possingham, H. (2009) Dealing with ‘The great divide'. Decision Point, 28, 2. Sutherland, W.J., Armstrong-Brown, S., Armsworth, P. R., Brereton, T., Brickland, J., Campbell, C.D., Chamberlain, D. E., Cooke, A.I., Dulvy, N.K., Dusic, N.R., Fitton, M., Freckleton, R.P., Godfray, H.C., Grout, N., Harvey, H.J., Hedley, C., Hopkins, J.J., Kift, N.B., Kirby, J., Kunin, W.E., MacDonald, D.W., Markee, B., Naura, M., Neale, A.R., Oliver, T., Osborn, D., Pullin, A.S., Shardlow, M.E.A., Showler, D.A., Smith, P.L., Smithers, R.J., Solandt, J.-L., Spencer, J., Spray, C.J., Thomas, C.D., Thompson, J., Webb, S.E., Yalden, D.W. {\{}{\&}{\}} Watkinson, A.R. (2006) The identification of 100 ecological questions of high policy relevance in the UK. Journal of Applied Ecology, 43, 617–627. Direct Link: Abstract Full Article (HTML) PDF(112K) References Web of Science{\{}{\textregistered}{\}} Times Cited: 133 Sutherland, W.J., Adams, W.M., Aronson, R.B., Aveling, R., Blackburn, T.M., Broad, S., Ceballos, G., C{\{}{\^{o}}{\}}t{\{}{\'{e}}{\}}, I.M., Cowling, R.M., da Fonseca, G.A.B., Dinerstein, E., Ferraro, P.J., Fleishman, E., Gascon, C., Hunter Jr, M., Hutton, J., Kareiva, P., Kuria, A., Macdonald, D.W., MacKinnon, K., Madgwick, F.J., Mascia, M.B., McNeely, J., Milner-Gulland, E.J., Moon, S., Morley, C.G., Nelson, S., Osborn, D., Pai, M., Parsons, E.C.M., Peck, L.S., Possingham, H., Prior, S.V., Pullin, A.S., Rands, M.R.W., Ranganathan, J., Redford, K.H., Rodriguez, J.P., Seymour, F., Sobel, F., Sodhi, N.S., Stott, A., Vance-Borland, K. {\{}{\&}{\}} Watkinson, A.R. (2009) One hundred questions of importance to the conservation of global biological diversity. Conservation Biology, 23, 557–567. Direct Link: Abstract Full Article (HTML) PDF(211K) References Web of Science{\{}{\textregistered}{\}} Times Cited: 87},
annote = {id: 1; issn: print 00218901; publication{\_}type: full{\_}text},
author = {Hulme, Philip E},
doi = {10.1111/j.1365-2664.2010.01938.x},
isbn = {0021-8901},
issn = {00218901},
journal = {Journal of Applied Ecology},
number = {1},
pages = {1--2},
title = {{Practitioner's perspectives: Introducing a different voice in applied ecology}},
url = {http://dx.doi.org/10.1111/j.1365-2664.2010.01938.x},
volume = {48},
year = {2011}
}
@article{Beck1999,
abstract = {Extreme Programming turns the conventional software process sideways. Rather than planning, analyzing, and designing for the far-flung future, XP programmers do all of these activities—a little at a time—throughout development.},
author = {Beck, Kent},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Beck - 1999 - Embracing Change with Extreme Programming.pdf:pdf},
journal = {IEEE Computer},
number = {10},
pages = {70--77},
title = {{Embracing Change with Extreme Programming}},
url = {https://www.computer.org/csdl/mags/co/1999/10/rx070.pdf http://faculty.salisbury.edu/{~}xswang/Research/Papers/SERelated/XP/rx070.pdf},
volume = {32},
year = {1999}
}
@article{Cuccurullo2016,
abstract = {Our paper analyses 25 years of performance management research published in the English-language journals, included in SSCI database, separating the business domain from public sector one. We used a content analysis for showing the relationships between the subfields of performance management and the time evolution. Through a multiple correspondence analysis based on keywords we provide a framework to track this literature over the 25-year period. We conclude the paper with a discussion on future pathways in the performance management literature.},
author = {Cuccurullo, Corrado and Aria, Massimo and Sarto, Fabrizia},
doi = {10.1007/s11192-016-1948-8},
isbn = {1588-2861},
issn = {15882861},
journal = {Scientometrics},
keywords = {Big Data,Historiograph,Multiple correspondence analysis,Performance management,Scientometrics},
mendeley-tags = {Big Data},
month = {aug},
number = {2},
pages = {595--611},
title = {{Foundations and trends in performance management. A twenty-five years bibliometric analysis in business and public administration domains}},
url = {http://link.springer.com/10.1007/s11192-016-1948-8},
volume = {108},
year = {2016}
}
@article{Ibm2013,
abstract = {Michael Schroeck, Rebecca Shockley, Dr. Janet Smart, Professor Dolores Romero-Morales and Professor Peter Tufano},
author = {Ibm},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ibm - 2013 - Analytics The real-world use of big data in financial services.pdf:pdf},
journal = {IBM Institute for Business Value},
keywords = {Keywords: Keywords: analytics,banking,big data,business analytics and optimization,data sources,financial markets,financial services,innovation,technologies,variety,velocity,veracity,volume},
pages = {16},
title = {{Analytics : The real-world use of big data in financial services}},
url = {www.sbs.ox.ac.uk},
year = {2013}
}
@article{Sutherland2009,
abstract = {We identified 100 scientific questions that, if answered, would have the greatest impact on conservation practice and policy. Representatives from 21 international organizations, regional sections and working groups of the Society for Conservation Biology, and 12 academics, from all continents except Antarctica, compiled 2291 questions of relevance to conservation of biological diversity worldwide. The questions were gathered from 761 individuals through workshops, email requests, and discussions. Voting by email to short-list questions, followed by a 2-day workshop, was used to derive the final list of 100 questions. Most of the final questions were derived through a process of modification and combination as the workshop progressed. The questions are divided into 12 sections: ecosystem functions and services, climate change, technological change, protected areas, ecosystem management and restoration, terrestrial ecosystems, marine ecosystems, freshwater ecosystems, species management, organizational systems and processes, societal context and change, and impacts of conservation interventions. We anticipate that these questions will help identify new directions for researchers and assist funders in directing funds.},
address = {Conservation Science Group, Department of Zoology, University of Cambridge, Downing Street, Cambridge CB2 3EJ, United Kingdom. w.sutherland@zoo.cam.ac.uk},
annote = {id: 3; LR: 20141120; CI: (c)2009; JID: 9882301; 2009/04/22 [aheadofprint]; ppublish},
author = {Sutherland, W J and Adams, W M and Aronson, R B and Aveling, R and Blackburn, T M and Broad, S and Ceballos, G and C??t??, I. M. and Cowling, R M and {Da Fonseca}, G. A B and Dinerstein, E and Ferraro, P J and Fleishman, E and Gascon, C and Hunter, M. and Hutton, J and Kareiva, P and Kuria, A and MacDonald, D. W. and MacKinnon, K. and Madgwick, F J and Mascia, M B and McNeely, J and Milner-Gulland, E J and Moon, S and Morley, C G and Nelson, S and Osborn, D and Pai, M and Parsons, E C M and Peck, L S and Possingham, H and Prior, S V and Pullin, A S and Rands, M R W and Ranganathan, J and Redford, K H and Rodriguez, J P and Seymour, F and Sobel, J and Sodhi, N S and Stott, A and Vance-Borland, K and Watkinson, A R},
doi = {10.1111/j.1523-1739.2009.01212.x},
isbn = {1523-1739 (Electronic)$\backslash$n0888-8892 (Linking)},
issn = {08888892},
journal = {Conservation Biology},
keywords = {Biodiversity,Conservation,Horizon scanning,Policy,Priority setting,Research agenda,Research questions},
month = {jun},
number = {3},
pages = {557--567},
pmid = {19438873},
publisher = {Society for Conservation Biology},
title = {{One hundred questions of importance to the conservation of global biological diversity}},
volume = {23},
year = {2009}
}
@inproceedings{stokes3dreview,
abstract = {We review the state of the art in micro-scale fabrication processes capable of producing three-dimensional parts and features. Their number and heterogeneity makes an exhaustive review impossible. We selected processes according to how well they satisfied the following criteria. They should ideally have made the leap from research laboratory to industry or appear likely to do so in the near- to mid-term. Resolution should he high. Throughput should be high. The variety of materials from which parts can be fabricated should be large. Finally, constraints on the geometries of parts that can be realized should be few. The review is based on the literature in the field. We have done our best to track down and include references to as many published reviews of technologies and technology families within our broad scope, though we have been a little disappointed not to find more. {\textcopyright} The Institute of Engineering and Technology.},
author = {Stokes, C A and Palmer, P J},
booktitle = {MEMS Sensors and Actuators, 2006: The Institution of Engineering and Technology Seminar},
isbn = {0-86341-627-6},
month = {dec},
organization = {London, UK},
pages = {289--298},
title = {{3D micro-fabrication processes: A review}},
year = {2006}
}
@misc{NBNAtlas2020,
abstract = {The NBN Atlas is an online tool that provides a platform to engage, educate and inform people about the natural world. It will help improve biodiversity knowledge, open up research possibilities and change the way environmental management is carried out in the UK.},
author = {{National Biodiversity Network}},
title = {{NBN Atlas}},
url = {https://nbnatlas.org/},
urldate = {2020-01-11},
year = {2020}
}
@misc{Buckley2017,
abstract = {Additional opportunities for public to visit city centre dig site on Saturday 20 and Sunday 21 May Issued by University of Leicester Press Office on 12 May 2017 Images taken during the dig available to download at: https://www.dropbox.com/sh/xw5egbtlgl2usfl/AACL5KIYBa2eJqlhHAFtmD3ia?dl=0 A 3D model of the mosaic: https://skfb.ly/6ps9y Due to huge demand, University of Leicester archaeologists have announced further opportunities for members of the public to view the largest archaeological excavation in Leicester in over a decade on Saturday 20 and Sunday 21 May 2017. A limited number of free tickets will be made available at 8am on Sunday 14 May via Eventbrite. Over 3,000 visitors have flocked to see the work being carried out by University of Leicester Archaeological Services (ULAS) on the former Stibbe factory site, between Great Central Street and Highcross Street in central Leicester. The original open days proved so popular it prompted the team to extend the public viewing sessions for an additional five days. These new dates on Saturday 20 and Sunday 21 May 2017 will be the final chance for people to see the largest Roman mosaic in Leicester found in over 150 years (before it is lifted and conserved), along with Roman buildings, streets, and artefacts. Over 300 schoolchildren will be visiting the site during the week to learn more about the city's past from the archaeologists. The land is owned by Charles Street Buildings group, which has made the site available and financially supported the archaeological excavation ahead of a major planned development of the site. A series of free 15 minute tours by the archaeologists have been organised throughout the weekend of the 20 and 21 May for which tickets must be booked in advance. Tours will be given in batches of 20. Entry is by advanced ticket only so please bring the printed tickets with you on the day. The site entrance is on Great Central Street, off Vaughan Way.},
author = {Buckley, Richard and Speed, Gavin},
booktitle = {Press Releases},
title = {{Final chances to view Leicester's largest archaeological excavation in over a decade — University of Leicester}},
url = {http://www2.le.ac.uk/offices/press/press-releases/2017/may/final-chances-to-view-leicester2019s-largest-archaeological-excavation-in-over-a-decade},
urldate = {2017-08-24},
year = {2017}
}
@article{Matthews1924,
author = {Matthews, Janice. R.},
doi = {10.1155/1924/76545},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Matthews - 1924 - Early History of the Cambridge Entomological Club.pdf:pdf},
issn = {16877438},
journal = {Psyche (New York)},
number = {1},
pages = {1--6},
title = {{Early History of the Cambridge Entomological Club}},
volume = {31},
year = {1924}
}
@misc{Smith2018,
abstract = {The 18th of March 2018, was the day tech insiders had been dreading. That night, a new moon added almost no light to a poorly lit four-lane road in Tempe, Arizona, as a specially adapted Uber Volvo XC90 detected an object ahead. Part of the modern gold rush to develop self-driving vehicles, the SUV had been driving autonomously, with no input from its human backup driver, for 19 minutes. An array of radar and light-emitting lidar sensors allowed onboard algorithms to calculate that, given their host vehicle's steady speed of 43mph, the object was six seconds away – assuming it remained stationary. But objects in roads seldom remain stationary, so more algorithms crawled a database of recognisable mechanical and biological entities, searching for a fit from which this one's likely behavior could be inferred.},
address = {Lomdon},
author = {Smith, Andrew},
booktitle = {The Guardian},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Smith - 2018 - Franken-algorithms the deadly consequences of unpredictable code Technology The Guardian.pdf:pdf},
keywords = {AI,Artificial intelligence,Silicon Valley},
mendeley-tags = {AI,Artificial intelligence,Silicon Valley},
month = {aug},
pages = {11},
title = {{Franken-algorithms: the deadly consequences of unpredictable code | Technology | The Guardian}},
url = {https://www.theguardian.com/technology/2018/aug/29/coding-algorithms-frankenalgos-program-danger},
year = {2018}
}
@article{Quarteroni2018,
abstract = {In their modern implementation, computational models based on first principles from Physics can dramatically benefit from the recent explosion of Data Science. In fact, these two branches of applied mathematics can virtuously interplay, and at a large extent they already do.},
author = {Quarteroni, Alfio},
doi = {10.1016/j.spl.2018.02.047},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Quarteroni - 2018 - The role of statistics in the era of big data A computational scientist' perspective.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Computational methods,Data analysis,Data assimilation,Numerical models,Uncertainty quantification},
month = {may},
pages = {63--67},
publisher = {North-Holland},
title = {{The role of statistics in the era of big data: A computational scientist' perspective}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300920},
volume = {136},
year = {2018}
}
@misc{Ritchey2015,
annote = {id: 1},
author = {Ritchey, T},
number = {9/21/2015},
title = {{General Morphological Analysis }},
url = {http://www.swemorph.com/ma.html},
volume = {2015},
year = {2015}
}
@article{Pebesma2018,
abstract = {Simple features are a standardized way of encoding spatial vector data (points, lines, polygons) in computers. The sf package implements simple features in R, and has roughly the same capacity for spatial vector data as packages sp, rgeos, and rgdal. We describe the need for this package, its place in the R package ecosystem, and its potential to connect R to other computer systems. We illustrate this with examples of its use.},
author = {Pebesma, Edzer},
doi = {10.32614/rj-2018-009},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pebesma - 2018 - Simple features for R Standardized support for spatial vector data.pdf:pdf},
issn = {20734859},
journal = {R Journal},
month = {jul},
number = {1},
pages = {439--446},
publisher = {Technische Universitaet Wien},
title = {{Simple features for R: Standardized support for spatial vector data}},
volume = {10},
year = {2018}
}
@inproceedings{Dahmann2014,
address = {Las Vegas, USA},
annote = {Document in TAMS4CPS - Note; only a presentation. It appears there was no paper.

Pain [oints listed below. Worth going to Claus Nielsen's paper with John FitzGerald. Many of the issues might be addressed by a discussion of contracts between constituent systems, at all 9 levels of the NCOIC interoperability Framework. Not an easy task, though. "If the user meets the interface preconditions, then the system owner guarantees to deliver the interface post-conditions."[I think this is where broker architectures come into play; could be complex for a PnP network over a long evolving time]Systems of Systems Engineering
The process of planning, analyzing, organizing, and integrating the capabilities of a mix of existing and new systems into a system-of-systems capability that is greater than the sum of the capabilities of the constituent parts.
[From: US DoD Guide for Systems Engineering of Systems of Systems]SoS Pain Points (headings):

• SoS Authority. What are effective collaboration patterns in SoS?

• Leadership What are the roles and characteristics of effective SoS leaders?

• Capabilities {\&} Requirements How can SE address SoS capabilities and requirements?

• Autonomy, Interdependencies {\&} Emergence How can SE address the complexities of SoS interdependencies and emergent behaviors?

• Constituent Systems What are effective approaches to integrating constituent systems?

• Testing, Validation {\&} Learning How can SE approach SoS validation, testing, and continuous learning in SoS?

• SoS Principles What are the key SoS thinking principles?

These are explicated in more detail in later slides.},
author = {Dahmann, J},
booktitle = {INCOSE International Symposium},
doi = {10.1002/j.2334-5837.2014.tb03138.x},
keywords = {Cyber-physical systems,Governance,IoT,agents,architecture,certification:,consequences,contracts,control,definitions,design process,engineering,evaluation,federated control,green,intranets,knowledge nets,methods,metrics/metrication,mobile,models,networks,org. trust,reference architectures,requirements,simulation},
number = {1.4.3},
pages = {108--121},
publisher = {Wiley},
title = {{Systems of systems: a systems engineering perspective}},
year = {2014}
}
@techreport{VeerleVandenEyndenLouiseCorti2011,
abstract = {The information provided in this guide is designed to help researchers and data managers, across a wide range of research disciplines and research environments, produce highest quality research data with the greatest potential for long-term use. All these initiatives involve close liaising with numerous researchers spanning the natural and social sciences and humanities. Author-supplied keywords},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Veerle Van den Eynden, Louise Corti}, Matthew and Woollard, Libby Bishop and Laurence Horton},
booktitle = {UK Data Archive University of Essex},
doi = {10.1007/978-1-84996-226-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Veerle Van den Eynden, Louise Corti, Woollard - 2011 - Managing and Sharing Data - Best Practice For Researchers.pdf:pdf},
isbn = {1904059783},
issn = {0964704X},
keywords = {2011 05 03,best practice,confidentiality,consent,copyright,managing data,sharing data,uk data archive},
number = {May},
pages = {1--40},
pmid = {21581},
title = {{Managing and Sharing Data - Best Practice For Researchers}},
url = {http://www.data-archive.ac.uk/media/2894/managingsharing.pdf},
year = {2011}
}
@article{Pocock2019,
abstract = {• Species records from volunteers are a vast and valuable source of information on biodiversity for a wide range of taxonomic groups. Although these citizen science data are opportunistic and unstructured, occupancy analysis can be used to quantify trends in distribution. However, occupancy analysis of unstructured data can be resource-intensive and requires substantial expertise. It is valuable to have simple 'rules of thumb' to efficiently assess the suitability of a dataset for occupancy analysis prior to analysis. • Our analysis was possible due to the production of trends, from our Bayesian occupancy analysis, for 10 967 species from 34 multi-species recording schemes in Great Britain. These schemes had an average of 500 visits to sites per year, and an average of 20{\%} of visited sites received a revisit in a year. Occupancy trend outputs varied in their precision and we used expert elicitation on a subset of outputs to determine a precision threshold above which trends were suitable for further consideration. We then used classification trees with seven metrics to define simple rules explaining when the data would result in outputs that met the precision threshold.. CC-BY 4.0 International license It is made available under a (which was not peer-reviewed) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint. http://dx.doi.org/10.1101/813626 doi: bioRxiv preprint first posted online Oct. 22, 2019; 2 • We found that the suitability of a species' data was best described by (i) the number of records of the focal species in the 10{\%} best-recorded years, and (ii) the proportion of recording visits for that taxonomic group with non-detections of the focal species. Surprisingly few data were required to be predicted to meet the precision threshold. Specifically, for 98{\%} confidence that our Bayesian occupancy models would produce outputs meeting the precision threshold, there needed to be ≥29 records of the focal species in the 10{\%} best-recorded years (equivalent to an average of 12.5 records per year in our dataset), although only ≥10 records (equivalent to 4.5 records per year) were required for species recorded in less than 1 in 25 visits. • We applied these rules to regional species data for Great Britain. Data from 32{\%} of the species:region combinations met the precision threshold with 80{\%} confidence, and 14{\%} with 98{\%} confidence. There was great variation between taxonomic groups (e.g. butterflies, moths and dragonflies were well recorded) and region (e.g. southeast England was best recorded). • These simple criteria provide no indication of the accuracy or representativeness of the trend outputs: this is vital, but needs to be assessed individually. However our criteria do provide a rapid, quantitative assessment of the predicted suitability of existing data for occupancy analysis and could be used to inform the design and implementation of multi-species citizen science recording projects elsewhere in the world.},
author = {Pocock, Michael J O and Logie, M. and Isaac, Nick J B and Powney, G. D. and Outhwaite, C.L. and August, T. A.},
doi = {10.1101/813626},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pocock et al. - 2019 - A rapid assessment of multi-species citizen science datasets for occupancy trend analysis to identify data gaps i.pdf:pdf},
journal = {bioRxiv},
pages = {36},
title = {{A rapid assessment of multi-species citizen science datasets for occupancy trend analysis to identify data gaps in coverage}},
url = {http://dx.doi.org/10.1101/813626},
year = {2019}
}
@article{Bizer2009,
abstract = {The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of Data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.7071v2},
author = {Bizer, Christian and Lehmann, Jens and Kobilarov, Georgi and Auer, S{\"{o}}ren and Becker, Christian and Cyganiak, Richard and Hellmann, Sebastian},
doi = {10.1016/j.websem.2009.07.002},
eprint = {arXiv:1407.7071v2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bizer et al. - 2009 - DBpedia - A crystallization point for the Web of Data.pdf:pdf},
isbn = {15708268 (ISSN)},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Knowledge extraction,Linked Data,RDF,Web of Data,Wikipedia},
month = {sep},
number = {3},
pages = {154--165},
pmid = {12014913},
publisher = {Elsevier},
title = {{DBpedia - A crystallization point for the Web of Data}},
url = {https://www.sciencedirect.com/science/article/pii/S1570826809000225},
volume = {7},
year = {2009}
}
@article{Vetro2016,
abstract = {The diffusion of Open Government Data (OGD) in recent years kept a very fast pace. However, evidence from practitioners shows that disclosing data without proper quality control may jeopardize dataset reuse and negatively affect civic participation. Current approaches to the problem in literature lack a comprehensive theoretical framework. Moreover, most of the evaluations concentrate on open data platforms, rather than on datasets.In this work, we address these two limitations and set up a framework of indicators to measure the quality of Open Government Data on a series of data quality dimensions at most granular level of measurement. We validated the evaluation framework by applying it to compare two cases of Italian OGD datasets: an internationally recognized good example of OGD, with centralized disclosure and extensive data quality controls, and samples of OGD from decentralized data disclosure (municipality level), with no possibility of extensive quality controls as in the former case, hence with supposed lower quality.Starting from measurements based on the quality framework, we were able to verify the difference in quality: the measures showed a few common acquired good practices and weaknesses, and a set of discriminating factors that pertain to the type of datasets and the overall approach. On the basis of this evaluation, we also provided technical and policy guidelines to overcome the weaknesses observed in the decentralized release policy, addressing specific quality aspects.},
author = {Vetr{\`{o}}, Antonio and Canova, Lorenzo and Torchiano, Marco and Minotas, Camilo Orozco and Iemma, Raimondo and Morando, Federico},
doi = {10.1016/j.giq.2016.02.001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vetr{\`{o}} et al. - 2016 - Open data quality measurement framework Definition and application to Open Government Data.pdf:pdf},
isbn = {0740-624x},
issn = {0740624X},
journal = {Government Information Quarterly},
keywords = {Big Data,Data quality measurement,Empirical assessment,Government information quality,Open Government Data,Open data quality},
mendeley-tags = {Big Data},
number = {2},
pages = {325--337},
title = {{Open data quality measurement framework: Definition and application to Open Government Data}},
volume = {33},
year = {2016}
}
@inproceedings{Pivarski2016,
abstract = {We introduce a new language for deploying analytic models into products, services and operational systems called the Portable Format for Analytics (PFA). PFA is an example of what is sometimes called a model interchange format, a language for describing analytic models that is independent of specific tools, applications or systems. Model interchange formats allow one application (the model producer) to ex-port models and another application (the model consumer or scoring engine) to import models. The core idea behind PFA is to support the safe execution of statistical func-tions, mathematical functions, and machine learning algo-rithms and their compositions within a safe execution envi-ronment. With this approach, the common analytic models used in data science can be implemented, as well as the data transformations and data aggregations required for pre-and post-processing data. PFA compliant scoring engines can be extended by adding new user defined functions described in PFA. We describe the design of PFA. A Data Mining Group (DMG) Working Group is developing the PFA stan-dard. The current version is 0.8.1 and contains many of the commonly used statistical and machine learning models, in-cluding regression, clustering, support vector machines, neu-ral networks, etc. We also describe two implementations of Hadrian, one in Scala and one in Python. We discuss four case studies that use PFA and Hadrian to specify analytic models, including two that are deployed in operations at client sites.},
author = {Pivarski, Jim and Bennett, Collin and Grossman, Robert L},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
doi = {10.1145/2939672.2939731},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pivarski, Bennett, Grossman - 2016 - Deploying Analytics with the Portable Format for Analytics (PFA).pdf:pdf},
isbn = {9781450342322},
keywords = {PFA,PMML,Portable Format for An-alytics,deploying analytics,model producers,scoring engines},
pages = {579--588},
title = {{Deploying Analytics with the Portable Format for Analytics (PFA)}},
url = {http://dx.doi.org/10.1145/2939672.2939731 http://dl.acm.org/citation.cfm?doid=2939672.2939731},
year = {2016}
}
@article{Hoenig2001,
abstract = {It is well known that statistical power calculations can be valuable in planning an experiment. There is also a large literature advocating that power calculations be made whenever one performs a statistical test of a hypothesis and one obtains a statistically nonsignificant result. Advocates of such post-experiment power calculations claim the calculations should be used to aid in the interpretation of the experimental results. This approach, which appears in various forms, is fundamentally flawed. We document that the problem is extensive and present arguments to demonstrate the flaw in the logic.},
author = {Hoenig, John M. and Heisey, Dennis M.},
doi = {10.1198/000313001300339897},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hoenig, Heisey - 2001 - The abuse of power The pervasive fallacy of power calculations for data analysis.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Bioequivalence testing,Burden of proof,Observed power,Retrospective power analysis,Statistical power,Type II error},
month = {feb},
number = {1},
pages = {19--24},
title = {{The abuse of power: The pervasive fallacy of power calculations for data analysis}},
volume = {55},
year = {2001}
}
@techreport{Ngo2019,
abstract = {Unedited advance version.
Nature embodies different concepts for different people, including biodiversity, ecosystems,
Mother Earth, systems of life and other analogous concepts. Nature's contributions to people
embody different concepts such as ecosystem goods and services, and nature's gifts. Both nature
and nature's contributions to people are vital for human existence and good quality of life
(human well-being, living in harmony with nature, living well in balance and harmony with
Mother Earth, and other analogous concepts).While more food, energy and materials than ever
before are now being supplied to people in most places, this is increasingly at the expense of
nature's ability to provide such contributions in the future and frequently undermines nature's
many other contributions, which range from water quality regulation to sense of place. The
biosphere, upon which humanity as a whole depends, is being altered to an unparalleled degree
across all spatial scales. Biodiversity – the diversity within species, between species and of
ecosystems – is declining faster than at any time in human history.},
address = {Bonn, Germany},
author = {Ngo, Hien T and Gu{\`{e}}ze, Maximilien and {Agard Trinidad}, John and Arneth, Almut and Balvanera, Patricia and Brauman, Kate and Watson, Robert and Baste, Ivar and Larigauderie, Anne and Leadley, Paul and Pascual, Unai and Baptiste, Brigitte and Demissew, Sebsebe and Dziba, Luthando and Erpul, G{\"{u}}nay and Fazel, Asghar and Fischer, Markus and {Mar{\'{i}}a Hern{\'{a}}ndez}, Ana and Karki, Madhav and Mathur, Vinod and Pataridze, Tamar and {Sousa Pinto}, Isabel and Stenseke, Marie and T{\"{o}}r{\"{o}}k, Katalin and Vil{\'{a}}, Bibiana and {Carneiro da Cunha}, Manuela and Mace, Georgina and Mooney, Harold},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ngo et al. - 2019 - Summary for policymakers of the global assessment report on biodiversity and ecosystem services.pdf:pdf},
institution = {UN},
pages = {39},
title = {{Summary for policymakers of the global assessment report on biodiversity and ecosystem services}},
url = {https://www.ipbes.net/sites/default/files/downloads/summary{\_}for{\_}policymakers{\_}ipbes{\_}global{\_}assessment.pdf},
year = {2019}
}
@article{Sanchez-Bayo2019,
abstract = {Biodiversity of insects is threatened worldwide. Here, we present a comprehensive review of 73 historical reports of insect declines from across the globe, and systematically assess the underlying drivers. Our work reveals dramatic rates of decline that may lead to the extinction of 40{\%} of the world's insect species over the next few decades. In terrestrial ecosystems, Lepidoptera, Hymenoptera and dung beetles (Coleoptera) appear to be the taxa most affected, whereas four major aquatic taxa (Odonata, Plecoptera, Trichoptera and Ephemeroptera) have already lost a considerable proportion of species. Affected insect groups not only include specialists that occupy particular ecological niches, but also many common and generalist species. Concurrently, the abundance of a small number of species is increasing; these are all adaptable, generalist species that are occupying the vacant niches left by the ones declining. Among aquatic insects, habitat and dietary generalists, and pollutant-tolerant species are replacing the large biodiversity losses experienced in waters within agricultural and urban settings. The main drivers of species declines appear to be in order of importance: i) habitat loss and conversion to intensive agriculture and urbanisation; ii) pollution, mainly that by synthetic pesticides and fertilisers; iii) biological factors, including pathogens and introduced species; and iv) climate change. The latter factor is particularly important in tropical regions, but only affects a minority of species in colder climes and mountain settings of temperate zones. A rethinking of current agricultural practices, in particular a serious reduction in pesticide usage and its substitution with more sustainable, ecologically-based practices, is urgently needed to slow or reverse current trends, allow the recovery of declining insect populations and safeguard the vital ecosystem services they provide. In addition, effective remediation technologies should be applied to clean polluted waters in both agricultural and urban environments.},
author = {S{\'{a}}nchez-Bayo, Francisco and Wyckhuys, Kris A.G.},
doi = {10.1016/J.BIOCON.2019.01.020},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/S{\'{a}}nchez-Bayo, Wyckhuys - 2019 - Worldwide decline of the entomofauna A review of its drivers.pdf:pdf},
issn = {0006-3207},
journal = {Biological Conservation},
month = {apr},
pages = {8--27},
publisher = {Elsevier},
title = {{Worldwide decline of the entomofauna: A review of its drivers}},
url = {https://www.sciencedirect.com/science/article/pii/S0006320718313636},
volume = {232},
year = {2019}
}
@book{Ayto,
abstract = {With thousands of contemporary words and phrases and a wide selection of entries on the cultural preoccupations of our times, Brewer's Dictionary of Modern Phrase {\&} Fable is an invaluable guide to modern language and culture. Focusing on the 20th and 21st centuries, it applies the trademark Brewer's treatment to a fascinating selection of buzzwords, catchphrases, slang, nicknames, fictional characters and intriguing cultural phenomena from pop culture to politics, literature to technology.},
author = {Ayto, John and Crofton, Ian and Brewer, Ebenezer Cobham},
isbn = {9780199916108},
title = {{Brewer's dictionary of modern phrase and fable}}
}
@misc{Kassambara,
abstract = {The Multiple correspondence analysis (MCA) is an extension of the simple correspondence analysis (chapter @ref(correspondence-analysis)) for summarizing and visualizing a data table containing more than two categorical variables. It can also be seen as a generalization of principal component analysis when the variables to be analyzed are categorical instead of quantitative (Abdi and Williams 2010).

MCA is generally used to analyse a data set from survey. The goal is to identify:

A group of individuals with similar profile in their answers to the questions
The associations between variable categories},
author = {Kassambara, Alboukadel},
title = {{MCA - Multiple Correspondence Analysis in R: Essentials - Articles - STHDA}},
url = {http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/},
urldate = {2018-02-13}
}
@incollection{Madureira2010,
abstract = {Interoperability refers to the ability of two or more systems or components to exchange information and to use the information that has been exchanged. The importance of interoperability has grown together with the adoption of Digital Information Networks (DINs). DINs refer to information networks supported by telecommunication infrastructures and terminated by microprocessors. With an upcoming interest in services science and transsector business models, a stronger driver arises to further break the interoperability barriers across sectors. In this paper, we propose a novel model to address trans-sector digital interoperability, which by definition involves interoperability across different economic sectors connected by DINs. Particularly, we specify how a well known interoperability framework, the ATHENA framework, should be adapted for the economic sector plane. Based on data from the Eurostat survey on ICT usage and e-Commerce in enterprises, we illustrate how conclusions about trans-sector interoperability can be extracted and technological implications can be derived.},
author = {Madureira, Ant{\'{o}}nio and den Hartog, Frank and Silva, Eduardo and Baken, Nico},
booktitle = {Enterprise Interoperability IV},
doi = {10.1007/978-1-84996-257-5_12},
editor = {Popplewell, Keith},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Madureira et al. - 2010 - Model for Trans-sector Digital Interoperability.pdf:pdf},
keywords = {ATHENA,EWI-18098,IR-72272,Interoperability,MDA,Model,Service science,digital information network},
pages = {123--133},
publisher = {Springer London},
title = {{Model for Trans-sector Digital Interoperability}},
year = {2010}
}
@inproceedings{Glanzel2006,
abstract = {The objective of the present study is twofold: (1) to show the aims and means of quantitative interpretation of bibliographic features in bibliometrics and their re-interpretation in research policy, and (2) to summarise the state-of-art in self-citation research. The authors describe three approaches to the role of author self-citations and possible conflicts arising from the different perspectives. From the bibliometric viewpoint we can conclude that that there is no reason for condemning self-citations in general or for removing them from macro or meso statistics; supplementary indicators based on self-citations are, nonetheless, useful to understand communication patterns.},
author = {Gl{\"{a}}nzel, Wolfgang and Debackere, Koenraad and Thijs, Bart and Schubert, Andr{\'{a}}s},
booktitle = {Scientometrics},
doi = {10.1007/s11192-006-0098-9},
isbn = {0138-9130},
issn = {01389130},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {may},
number = {2},
pages = {263--277},
title = {{A concise review on the role of author self-citations in information science, bibliometrics and science policy}},
url = {http://link.springer.com/10.1556/Scient.67.2006.2.8},
volume = {67},
year = {2006}
}
@incollection{Abney2012,
address = {Cambridge, MA},
annote = {Book on shelf

Important for raising the psychopath problem, p. 46 - robot without emotions is potentially a psychopath. Let loose a commuicating swarm, and potentially you have an army of sociopaths. Add in Wagner {\&} Arkin on duplicity, and neef{\&} vander Vecht on disconnection to command{\ldots}.

Relate to Haidt, W.D. Ross, Beavers, Calo


Discusses:
1 What is morality or ethics: the right, or the good?
2 What are moral rights? What is their relationship to moral duties? And who or what can be rights holders?
3 What are the major contemporary moral theories? How do theybear on robot ethics?
4 What is a person, in the moral sense? Can a robot be a person?

1 What is morality or ethics: the right, or the good?
• Concerns sentences about behaviour that include the word 'ought'
• Deontological approach says they are rules governing behaviour - quote Kant and his categorical principle
• Consequentialist approach - the ends justify the means - utilitarianism
• Virtue ethics - Aristotle - 'what should I be', not 'what should I do'. Claims this is a bottom-up approach to ethics, you learn what to be by case law. Example he discusses is that I must not carve you open with a knife because it is a wrong action for me; it has no virtue attached. She can, because she is a surgeon and is performing a life-enhancing activity, acceptable and legal in our society. 'Virtue' should be seen as a 'disposition to act in a certain way' - "ideally, to know by practical wisdom the right thing to do, in the right way, at the right time." Or, what would a role model do in my situation? [don't see that this helps; implies rules]

Then goes into a discussion about whether morality is about rights or about doing good. Basis is that moral is equated to legal, and hence immoral is equated to illegal. But this allows amoral behaviour if it is legal, and all philosophers argue against this equation. The existence of a rule doesn't give you a right.

2 What are moral rights? What is their relationship to moral duties? And who or what can be rights holders?
Two basic theories:
• The 'will' approach - "it asserts the right to liberty is the foundation of all other rights claims, and a rights claim is understood as the entitlement to a particular kind of choice - a rights claim entitles me to claim or perform something, or not - it is up to me (and nobody else." Thus, a rights claim entails no duties for the claimant, BUT the correlative thesis means that a rights claim does entail duties for other people. If I have a right, then you have a correlative duty to allow or enable me to exercise that right. Rights are guaranteed freedoms, which then guarantee duties for everyone else. BUT note that everyone else implies they are moral agents; non-moral agents (trees, dogs, stones) are exempt; they have no corresponding duties. This has an unfortunate consequence; in a scenario in which I have a right, the process is to establish all other rights belonging to the moral agents, and if there is no clash, then I exercise my right. If this entails the mutilation of non-moral agents e.g. laboratory monkeys, dogs, etc., is this OK?

• The 'interest' approach. This says that rights "correlate with interests (or welfare) - everything that has interests (or a 'welfare') has rights. All persons have a duty to respect the rights of everything that has interests (including, perhaps?)."robots"

Then goes on to point out that the correlativity thesis is flawed, by the arguments above, Can only ressurrect it by introducing the concept of 'moral patients' - agents to whom we owe a moral duty.even though they have no moral right.

3 What are the major contemporary moral theories? How do theybear on robot ethics?
Discusses deontological theories first Asimov and Kant are the heroes, here. Note, there are deontological logics to explore this domain. Deontological theories see ethics as a programmable set of rules to follow, corresponding to the approaches of Arkin and of Bringsjord.

Kant Categorical Imperative:
CI(1) also Formula of universal Law (FUL):
Act only in accordance with that maxim through which you can at the same time will that it become a universal law

CI(2) also Means-End Principle:
So act that you use humanity, whether in your own person or in the person of any other, always at the same time as an end, never merely as a means.Asimov 4 laws:
0 A robot may not harm all humanity nor, through inaction, allow humanity to come to harm
1 A robot may not injure a human being nor, through inaction, allow a human being to come to harm
2 A robot must obey orders give to it by human beings, except where such orders would conflict with the first law
3 A robot must protecrt its own existence as ong as such protection does notconflict with the preceding laws

Both of these laws fail, because they can be got round; for instance the subdivided sequential process concept where e.g. a bomb can be planted and exploded by having a series of robots carry out parts of the task in ignorance of what theothers are doing before the current robot and after it has completed its task. Also, the robot may have to continually intervene in activities by a group, and never get its own tasks accomplished.

Utilitarianism - Jeremy Bentham, John Stuart Mill. Came up with Greatest happiness Principle (GHP). Demands equality of happiness, impartiality (implausible w.r.t. individuals notions of happiness). Can't quantify this, and cost-benefit approach has practical difficulties in metrics. Keeps on about the calculation problem - doesn't realise this is not necessary. Also the frame problem - what data is relevant? Completely miss the point that decisions are sequential and so long as you don't make a terminal decision it doesn't much matter.

4 What is a person, in the moral sense? 
p.46 raises the psychopath question:
"Perhaps robots will one day have emotions; but our legal system assumes that moral agency does not require a normal, properly-functioning emotional 'inner' life. Psychopaths/sociopaths, rational agents with dysfunctional or missing emotional affect, are still morally and legally responsible for their crimes; whereas those who have emotional responses, but cannot exercise rational control (like the severey mentally disabled or infants) are not. But psychopaths, while emotionally dysfunctional, plausibly still have emotions. Would an emotionless robot possibly be a person?"
[doesn't spot the army of psychopaths problem]

"Even primate researcher Frans de Waal (2010) writes: 'I am reluctant to call a chimpanzee a 'moral being'. This is because sentiments do not suffice. {\ldots} This is what sets human morality apart: a move towards universal standards combined with an elaborate system of justification, monitoring and punishment."

Goes on to argue we have two types of decision-making systems; an emotion-based one, responsible for instant decisions (e.g. to do with safety, etc.) amd a more deliberative, rstional system for planning and cost-benefit decisions. He says the latter is essential for moral agency. Hence infants cannot be held responsible.

5 Can emotionless robots be moral persons?
"Yes, they could. {\ldots} the key to moral responsibility and personhood is the posession of moral agency, which requires the capacity for rational deliberation - but not the capacity for fuctional emotional states, per psychopaths{\ldots}"
[I think this argument is wrong, following the sequence of decisions argument, and the do no lasting harm argument - when you don't have all the data, or have to make a quick decision, emnotions are important - humanitarian decisions in the course of executing commands might well be based on emotional response to the circumstances. - see Haidt, W.D. Ross]},
author = {Abney, K},
booktitle = {Robot ethics},
chapter = {3},
editor = {Lin, P and Abney, K and Bekey, G A},
isbn = {978-0-262-01666-7},
keywords = {agents,definitions,ethics,philosophy},
pages = {35--54},
title = {{Robots, ethical theory and metaethics:  a guide for the perplexed}},
year = {2012}
}
@article{Kelling2009,
abstract = {The increasing availability of massive volumes of scientific data requires new synthetic analysis techniques to explore and identify interesting patterns that are otherwise not apparent. For biodiversity studies, a "data-driven" approach is necessary because of the complexity of ecological systems, particularly when viewed at large spatial and temporal scales. Data-intensive science organizes large volumes of data from multiple sources and fields and then analyzes them using techniques tailored to the discovery of complex patterns in high-dimensional data through visualizations, simulations, and various types of model building. Through interpreting and analyzing these models, truly novel and surprising patterns that are "born from the data" can be discovered. These patterns provide valuable insight for concrete hypotheses about the underlying ecological processes that created the observed data. Data-intensive science allows scientists to analyze bigger and more complex systems efficiently, and complements more traditional scientific processes of hypothesis generation and experimental testing to refine our understanding of the natural world. {\textcopyright} 2009 by American Institute of Biological Sciences.},
author = {Kelling, Steve and Hochachka, Wesley M. and Fink, Daniel and Riedewald, Mirek and Caruana, Rich and Ballard, Grant and Hooker, Giles},
doi = {10.1525/bio.2009.59.7.12},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kelling et al. - 2009 - Data-intensive Science A New Paradigm for Biodiversity Studies.pdf:pdf},
issn = {0006-3568},
journal = {BioScience},
month = {jul},
number = {7},
pages = {613--620},
publisher = {Oxford University Press (OUP)},
title = {{Data-intensive Science: A New Paradigm for Biodiversity Studies}},
volume = {59},
year = {2009}
}
@book{Friendly2016a,
abstract = {"A Chapman {\&} Hall book."},
author = {Friendly, Michael and Meyer, David},
isbn = {9781498725835},
publisher = {Chapman {\&} Hall/CRC Texts in Statistical Science},
title = {{Discrete data analysis with R : visualization and modeling techniques for categorical and count data}},
year = {2016}
}
@article{Gupta2016,
abstract = {The era of big data has begun such that organizations in all industries have been heavily investing in big data initiatives. We know from prior studies that investments alone do not generate competitive advantage; instead, firms need to create capabilities that rival firms find hard to match. Drawing on the resource-based theory of the firm and recent work in big data, this study (1) identifies various resources that in combination build a big data analytics (BDA) capability, (2) creates an instrument to measure BDA capability of the firm, and (3) tests the relationship between BDA capability and firm performance. Results empirically validate the proposed theoretical framework of this study and provide evidence that BDA capability leads to superior firm performance.},
archivePrefix = {arXiv},
arxivId = {10.1016/j.im.2016.07.004},
author = {Gupta, Manjul and George, Joey F.},
doi = {10.1016/j.im.2016.07.004},
eprint = {j.im.2016.07.004},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, George - 2016 - Toward the development of a big data analytics capability.pdf:pdf},
isbn = {0378-7206},
issn = {03787206},
journal = {Information and Management},
keywords = {Big data analytics,Capability,Firm performance,Formative constructs,Instrument development,Resource-based theory},
month = {dec},
number = {8},
pages = {1049--1064},
primaryClass = {10.1016},
publisher = {North-Holland},
title = {{Toward the development of a big data analytics capability}},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616300787?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {53},
year = {2016}
}
@inproceedings{Johnson2012,
abstract = {A futurist for Intel shows how geotags, sensor outputs, and big data are changing the future. He argues that we need a better understanding of our relationship with the data we produce in order to build the future we want.},
author = {Johnson, David Brian},
booktitle = {World Future Society},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Johnson - 2012 - The Secret Life of Data in the Year 2020.pdf:pdf},
isbn = {00163317},
number = {August},
pages = {20--23},
pmid = {1024140667},
title = {{The Secret Life of Data in the Year 2020}},
volume = {46},
year = {2012}
}
@misc{DeMauro2016,
abstract = {Purpose – The purpose of this paper is to identify and describe the most prominent research areas connected with “Big Data” and propose a thorough definition of the term. Design/methodology/approach – The authors have analysed a conspicuous corpus of industry and academia articles linked with Big Data to find commonalities among the topics they treated. The authors have also compiled a survey of existing definitions with a view of generating a more solid one that encompasses most of the work happening in the field. Findings – The main themes of Big Data are: information, technology, methods and impact. The authors propose a new definition for the term that reads as follows: “Big Data is the Information asset characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value.” Practical implications – The formal definition that is proposed can enable a more coherent development of the concept of Big Data, as it solely relies on ...},
archivePrefix = {arXiv},
arxivId = {aimag.v17i3.1230},
author = {{De Mauro}, Andrea and Greco, Marco and Grimaldi, Michele},
booktitle = {Library Review},
doi = {10.1108/LR-06-2015-0061},
eprint = {aimag.v17i3.1230},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/De Mauro, Greco, Grimaldi - 2016 - A formal definition of Big Data based on its essential features.pdf:pdf},
isbn = {1756139121},
issn = {00242535},
keywords = {Digital libraries,Information science,Information systems,Information technology,Knowledge management},
number = {3},
pages = {122--135},
pmid = {84487513},
title = {{A formal definition of Big Data based on its essential features}},
volume = {65},
year = {2016}
}
@article{Davies1983,
abstract = {Because of the increased cost-effectiveness of com- puter technology and its subsequent acceptance into the business world, computer-based message systems are likely to become the principal carriers of business cor- respondence. Unfortunately with the efficiency of these systems come new possibilities for crime based on in- terference with digital messages. But the same technology that poses the threat can be used to resist and perhaps entirely frustrate potential crimes.},
author = {Davies, Donald W.},
doi = {10.1109/MC.1983.1654301},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Davies - 1983 - Applying the RSA Digital Signature to Electronic Mail.pdf:pdf},
issn = {00189162},
journal = {Computer},
month = {feb},
number = {2},
pages = {55--62},
title = {{Applying the RSA Digital Signature to Electronic Mail}},
url = {http://ieeexplore.ieee.org/document/1654301/},
volume = {16},
year = {1983}
}
@inproceedings{Dadzie2009,
abstract = {Sensemaking is the process of analysing complex situations in order to make informed decisions. Semantic Web technology can be effectively used to create new sensemaking systems that focus on concepts and knowledge instead of documents. We demonstrate how this is achieved using information extraction to acquire knowledge and create a semantic repository that can then be semantically searched. A domain ontology is used to support the creation of an analysis tree; the semantic visualisation enables knowledge discovery, a core aspect of sensemaking. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Dadzie, Aba Sah and Iria, Jos{\'{e}} and Petrelli, Daniela and Xia, Lei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02121-3_61},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dadzie et al. - 2009 - The XMediaBox Sensemaking through the use of knowledge lenses.pdf:pdf},
isbn = {3642021204},
issn = {03029743},
keywords = {Knowledge acquisition,Semantic visualisation,Semantic web,Sensemaking},
pages = {811--815},
publisher = {Springer-Verlag},
title = {{The XMediaBox: Sensemaking through the use of knowledge lenses}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-02121-3{\_}61},
volume = {5554 LNCS},
year = {2009}
}
@book{Zuur2007,
abstract = {Data management and software -- Advice for teachers -- Exploration -- Linear regression -- Generalised linear modelling -- Additive and generalised additive modelling -- to mixed modelling -- Univariate tree models -- Measures of association -- Ordination -- First encounter -- Principal component analysis and redundancy analysis -- Correspondence analysis and canonical correspondence analysis -- to discriminant analysis -- Principal coordinate analysis and non-metric multidimensional scaling -- Time series analysis -- Introduction -- Common trends and sudden changes -- Analysis and modelling of lattice data -- Spatially continuous data analysis and modelling -- Univariate methods to analyse abundance of decapod larvae -- Analysing presence and absence data for flatfish distribution in the Tagus estuary, Portugal -- Crop pollination by honeybees in Argentina using additive mixed modelling -- Investigating the effects of rice farming on aquatic birds with mixed modelling -- Classification trees and radar detection of birds for North Sea wind farms -- Fish stock identification through neural network analysis of parasite fauna -- Monitoring for change: Using generalised least squares, non-metric multidimensional scaling, and the Mantel test on western Montana grasslands -- Univariate and multivariate analysis applied on a Dutch sandy beach community -- Multivariate analyses of South-American zoobenthic species -- spoilt for choice -- Principal component analysis applied to harbour porpoise fatty acid data -- Multivariate analyses of morphometric turtle data -- size and shape -- Redundancy analysis and additive modelling applied on savanna tree data -- Canonical correspondence analysis of lowland pasture vegetation in the humid tropics of Mexico -- Estimating common trends in Portuguese fisheries landings -- Common trends in demersal communities on the Newfoundland-Labrador Shelf -- Sea level change and salt marshes in the Wadden Sea: A time series analysis -- Time series analysis of Hawaiian waterbirds -- Spatial modelling of forest community features in the Volzhsko-Kamsky reserve.},
annote = {In lboro library.},
author = {Zuur, Alain F. and Ieno, Elena N. and Smith, Graham M.},
isbn = {0387459723},
pages = {672},
publisher = {Springer},
title = {{Analysing ecological data}},
year = {2007}
}
@article{FossoWamba2015,
abstract = {Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.},
author = {{Fosso Wamba}, Samuel and Akter, Shahriar and Edwards, Andrew and Chopin, Geoffrey and Gnanzou, Denis},
doi = {10.1016/J.IJPE.2014.12.031},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fosso Wamba et al. - 2015 - How ‘big data' can make big impact Findings from a systematic review and a longitudinal case study(3).pdf:pdf},
issn = {0925-5273},
journal = {International Journal of Production Economics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jul},
pages = {234--246},
publisher = {Elsevier},
title = {{How ‘big data' can make big impact: Findings from a systematic review and a longitudinal case study}},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004253},
volume = {165},
year = {2015}
}
@article{Bivand2018,
abstract = {Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.},
author = {Bivand, Roger and Krivoruchko, Konstantin},
doi = {10.1016/j.spl.2018.02.012},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bivand, Krivoruchko - 2018 - Big data sampling and spatial analysis “which of the two ladles, of fig-wood or gold, is appropriate to.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Change of support,Data transformation,Prediction standard error,Sampling design},
month = {may},
pages = {87--91},
publisher = {North-Holland},
title = {{Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300579},
volume = {136},
year = {2018}
}
@article{Franke2015,
abstract = {We describe a platform for smart, city-wide crowd management based on participatory mobile phone sensing and location/situation specific information delivery. The platform supports quick and flexible deployments of end-to-end applications for specific events or spaces that include four key functionalities: (1) Mobile phone based delivery of event/space specific information to the users, (2) participatory sensor data collection (from app users) and flexible analysis, (3) location and situation specific message multicast instructing people in different areas to act differently in case of an emergency and (4) post mortem event analysis. This paper describes the requirements that were derived through a series of test deployments, the system architecture, the implementation and the experiences made during real life, large scale deployments. Thus, until today it has been deployed at 14 events in three European countries (UK, Netherlands, Switzerland) and was used by well over 100,000 people.},
author = {Franke, Tobias and Lukowicz, Paul and Blanke, Ulf},
doi = {10.1186/s13174-015-0040-6},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Franke, Lukowicz, Blanke - 2015 - Smart crowds in smart cities real life, city scale deployments of a smartphone based participatory cro.pdf:pdf},
isbn = {1869-0238},
issn = {18690238},
journal = {Journal of Internet Services and Applications},
keywords = {Crowd management,Crowd sourcing,Large scale cooperative sensing,Smartphones},
month = {aug},
number = {1},
pages = {1--19},
publisher = {Springer London},
title = {{Smart crowds in smart cities: real life, city scale deployments of a smartphone based participatory crowd management platform}},
url = {http://www.jisajournal.com/content/6/1/27},
volume = {6},
year = {2015}
}
@article{Sengupta1992,
abstract = {Bibliometrics, informetrics, scientometrics and librametrics are four measuring techniques in library and information science. These are analogous, or rather synonymous, terms with intermingled aims and objectives and as such they need some elucidation. This paper discusses their scope, application, development and potential in solving various problems pertaining to library and information sciences.},
author = {Sengupta, I. N.},
doi = {10.1515/libr.1992.42.2.75},
isbn = {0024-2667},
issn = {18658423},
journal = {Libri},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {2},
pages = {75--98},
title = {{Bibliometrics, informetrics, scientometrics and librametrics: An overview}},
url = {https://www.degruyter.com/view/j/libr.1992.42.issue-2/libr.1992.42.2.75/libr.1992.42.2.75.xml},
volume = {42},
year = {1992}
}
@incollection{Fitzsimons2011,
abstract = {Previous chapters have looked at leisure choices. In the models associated with this, interaction with others is for mainly instrumental purposes. For example, to play a competitive sport, even in a friendly manner, requires other participants. Obviously, additional utility may arise from social interaction with fellow players. In this chapter, we consider situations where social interaction in itself can become sufficient as an activity. This social interaction has been facilitated by the expansion of information technology which greatly reduces the transactions costs of interacting with another person. It is not without costs, however, mainly in the form of risks to individual safety and well-being, although there may also be „hidden‟ pecuniary costs in the way that entry to leisure networks are priced.},
address = {Cheltenham, UK},
author = {Fitzsimons, Vincent G.},
booktitle = {The Handbook on the Economics of Leisure},
chapter = {22},
doi = {10.2139/ssrn.1952708},
editor = {Cameron, S},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fitzsimons - 2011 - The Impact of New Technology on Leisure Networks.pdf:pdf},
isbn = {978-1848444041},
keywords = {Leisure,Networks,Work},
month = {jun},
pages = {752--798},
publisher = {Edward Elgar Publishing Ltd},
title = {{The Impact of New Technology on Leisure Networks}},
url = {http://www.ssrn.com/abstract=1952708},
year = {2011}
}
@misc{Smith2008,
abstract = {Although the fields of bibliometrics and citation analysis have existed for many years, relatively few studies have specifically focused on the dermatological literature. This article reviews citation-based research in the dermatology journals, with a particular interest in manuscripts that have included Contact Dermatitis as part of their analysis. Overall, it can be seen that the rise of bibliometrics during the mid-20th century and its subsequent application to dermatology has provided an interesting insight into the progression of research within our discipline. Further investigation of citation trends and top-cited papers in skin research periodicals would certainly help complement the current body of knowledge.},
author = {Smith, Derek R.},
booktitle = {Contact Dermatitis},
doi = {10.1111/j.1600-0536.2008.01405.x},
isbn = {1600-0536 (Electronic) 0105-1873 (Linking)},
issn = {01051873},
keywords = {Bibliometrics,Big Data,Citation analysis,Contact dermatitis,Dermatology,Impact factor},
mendeley-tags = {Big Data},
month = {sep},
number = {3},
pages = {133--136},
pmid = {18759892},
title = {{Bibliometrics, dermatology and contact dermatitis}},
url = {http://doi.wiley.com/10.1111/j.1600-0536.2008.01405.x},
volume = {59},
year = {2008}
}
@article{Dennis2019,
abstract = {Moths form an important part of Scotland's biodiversity and an up-to-date assessment of their status is needed given their value as a diverse and species-rich taxon, with various ecosystem roles, and the known decline of moths within Britain. We use long-term citizen-science data to produce species-level trends and multi-species indicators for moths in Scotland, to assess population (abundance) and distribution (occupancy) changes. Abundance trends for moths in Scotland are produced using Rothamsted Insect Survey count data, and, for the first time, occupancy models are used to estimate occupancy trends for moths in Scotland, using opportunistic records from the National Moth Recording Scheme. Species-level trends are combined to produce abundance and occupancy indicators. The associated uncertainty is estimated using a parametric bootstrap approach, and comparisons are made with alternative published approaches. Overall moth abundance (based on 176 species) in Scotland decreased by 20{\%} for 1975–2014 and by 46{\%} for 1990–2014. The occupancy indicator (based on 230 species) showed a 16{\%} increase for 1990–2014. Alternative methods produced similar indicators and conclusions, suggesting robustness of the results, although rare species may be under-represented in our analyses. Species abundance and occupancy trends were not clearly correlated; in particular species with negative population trends showed varied occupancy responses. Further research into the drivers of moth population changes is required, but increasing occupancy is likely to be driven by a warming summer climate facilitating range expansion, whereas population declines may be driven by reductions in habitat quality, changes in land management practices and warmer, wetter winters.},
author = {Dennis, E. B. and Brereton, T. M. and Morgan, B. J.T. and Fox, R. and Shortall, C. R. and Prescott, T. and Foster, S.},
doi = {10.1007/s10841-019-00135-z},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dennis et al. - 2019 - Trends and indicators for quantifying moth abundance and occupancy in Scotland.pdf:pdf},
issn = {15729753},
journal = {Journal of Insect Conservation},
keywords = {Abundance,Citizen science,Lepidoptera,Multi-species indicators,National Moth Recording Scheme,Occupancy},
month = {apr},
number = {2},
pages = {369--380},
publisher = {Springer International Publishing},
title = {{Trends and indicators for quantifying moth abundance and occupancy in Scotland}},
volume = {23},
year = {2019}
}
@book{Loo2018,
abstract = {The production of clean data is a complex and time-consuming process that requires both technical know-how and statistical expertise. Statistical Data Cleaning brings together a wide range of techniques for cleaning textual, numeric or categorical data. This book examines technical data cleaning methods relating to data representation and data structure. A prominent role is given to statistical data validation, data cleaning based on predefined restrictions, and data cleaning strategy.},
address = {Hoboken, New Jersey, United States},
author = {van der Loo, M and de Jonge, E},
isbn = {9781118897157},
keywords = {Big Data,Forthcoming book},
mendeley-tags = {Big Data,Forthcoming book},
pages = {320},
title = {{Statistical Data Cleaning with Applications in R}},
url = {https://www.wiley.com/en-gb/Statistical+Data+Cleaning+with+Applications+in+R-p-9781118897157 https://books.google.co.uk/books?hl=en{\&}lr={\&}id=3slJDwAAQBAJ{\&}oi=fnd{\&}pg=PP2{\&}dq=data+cleaning{\&}ots=PiHQ6zT4g2{\&}sig=0thuZh1zQVm8IPa7FDp5oHUseqM},
year = {2018}
}
@article{Asaro2011,
abstract = {Roboethics and Legal Jurisdictions of Tele-Agency},
annote = {Paper in Autonomy!! See also Bryson.

Interesting, but reflects his occupation as a philosopher. He points out the problems that arise from different jurisdictions:

1 Where I am, online gambling is not allowed. But in Antigua, it is. A WTO ruling says that if the servers that do the gambling are not in my jurisdiction, but are located in some place like Antigua, online gambling is legal.

2 Criminals would be able to hide their control through botnets, etc.

3 Note that extradition might not be possible to get hold of criminals; e.g. US insistence on death penalty means some countries in the EU will refuse extradition if a possible penalty is death.

Discusses the Texas businessman who tried to set up a robot hunting business 'for disabled people'. Banned in Texas and a number of other states; rationale for this is that the hunter has to be present at death. Could be set up in South Africa, though, and other African countries. Or vicarious sex, or the torture of slaves in other countries.

However, he doesn't discuss the issue of ethics within robots, which could reduce criminal activity at the point of the crime. Nor does he discuss the security issue of "where have you been, exactly".

Also note that certification is important here, as a means of ensuring that robots are indeed ethical. This won't stop criminals getting at their software, but it should be possible, through the support infrastructure, to know when this has happened. But we would still need legislation in place to ensure we (or the EU) have in place a legal framwork to take action against such crimes.},
author = {Asaro, P M},
doi = {10.1109/MRA.2010.940154},
journal = {IEEE Robotics {\&} Automation},
keywords = {Governance,UAV,agents,certification:,consequences,control,empowerment,ethics,federated control,legislation,network-centric warfare,philosophy,robots,safety,security,theory},
number = {1},
pages = {68--71},
title = {{Remote-control crimes}},
volume = {18},
year = {2011}
}
@techreport{Manual,
author = {Manual, User},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Manual - Unknown - My Passport {\textregistered} Wireless Portable Hard Drive.pdf:pdf},
isbn = {0508555639},
keywords = {WDBNEK0020BBK WDBTNH0010BBK WDBTPE5000ABK WDBDAF00},
title = {{My Passport {\textregistered} Wireless Portable Hard Drive}},
url = {http://www.wd.com/setup}
}
@techreport{Cinelli2019,
abstract = {We show how experimental results can be generalized across diverse populations by leveraging knowledge of mechanisms that produce the outcome of interest. We use Structural Causal Models (SCM) and a refined version of selection diagrams to represent such knowledge , and to decide whether it entails conditions that enable generalizations. We further provide bounds for the target effect when some of these conditions are violated. We conclude by demonstrating that the structural account offers a more reliable way of analyzing generalization than positing counterfactual consequences of the actual mechanisms.},
author = {Cinelli, Carlos and Pearl, Judea},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cinelli, Pearl - 2019 - Generalizing Experimental Results by Leveraging Knowledge of Mechanisms.pdf:pdf},
title = {{Generalizing Experimental Results by Leveraging Knowledge of Mechanisms}},
year = {2019}
}
@article{Pu2016,
abstract = {Check-in records are usually available in social services, which offer us the opportunity to capture and analyze users' spatial and temporal behaviors. Mining such behavior features is essential to social analysis and business intelligence. However, the complexity and incompleteness of check-in records bring challenges to achieve such a task. Different from the previous work on social behavior analysis, in this paper, we present a visual analytics system, Social Check-in Fingerprinting (Sci-Fin), to facilitate the analysis and visualization of social check-in data. We focus on three major components of user check-in data: location, activity, and profile. Visual fingerprints for location, activity, and profile are designed to intuitively represent the high-dimensional attributes. To visually mine and demonstrate the behavior features, we integrate WorldMapper and Voronoi Treemap into our glyph-like designs. Such visual fingerprint designs offer us the opportunity to summarize the interesting features and patterns from different check-in locations, activities and users (groups). We demonstrate the effectiveness and usability of our system by conducting extensive case studies on real check-in data collected from a popular microblogging service. Interesting findings are reported and discussed at last. {\textcopyright} 2016 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Pu, Jiansu and Teng, Zhiyao and Gong, Rui and Wen, Changjiang and Xu, Yang},
doi = {10.3390/s16122194},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pu et al. - 2016 - Sci-Fin Visual mining spatial and temporal behavior features from social media.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Big data analysis,Internet of things,Social media,Spatial and temporal behaviors,Visual mining},
month = {dec},
number = {12},
pages = {2194},
pmid = {27999398},
title = {{Sci-Fin: Visual mining spatial and temporal behavior features from social media}},
url = {http://www.mdpi.com/1424-8220/16/12/2194},
volume = {16},
year = {2016}
}
@article{DeFauw2018,
abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
author = {{De Fauw}, Jeffrey and Ledsam, Joseph R. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and van den Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'{i}}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
doi = {10.1038/s41591-018-0107-6},
isbn = {4159101801},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Diagnosis,Eye manifestations,Machine learning,Three,dimensional imaging},
month = {aug},
pages = {1},
pmid = {30104768},
publisher = {Nature Publishing Group},
title = {{Clinically applicable deep learning for diagnosis and referral in retinal disease}},
url = {http://www.nature.com/articles/s41591-018-0107-6},
year = {2018}
}
@book{Borchers2002,
abstract = {This is the first book to provide an accessible, comprehensive introduction to wildlife population assessment methods. It uses a new approach that makes the full range of methods accessible in a way that has not previously been possible. Traditionally, newcomers to the field have had to face the daunting prospect of grasping new concepts for almost every one of the many methods. In contrast, this book uses a single conceptual (and statistical) framework for all the methods. This makes understanding the apparently different methods easier because each can be seen to be a special case of the general framework. The approach provides a natural bridge between simple methods and recently developed methods. It also links closed population methods quite naturally with open population methods. The book is accompanied by free software on the web, in the form of an R library, allowing readers to get some "hands-on" experience with the methods and how they perform in different contexts - without the considerable effort and expense required to do this in the real world. It also provides a tool for teaching the methods, including a means for teachers to generate examples and exercises customised to the needs of their students. As the first truly up-to-date and introductory text in the field, this book should become a standard reference for students and professionals in the fields of statistics, biology and ecology. Part I. Introduction: Using Likelihood for Estimation -- Part II. Simple Methods: Building Blocks; Plot Sampling; Removal, Catch-Effort, and Change-in-Ratio; Simple Mark-Recapture; Distance Sampling; Nearest Neighbour and Point-to-Nearest-Object -- Part III. Advanced Methods: Further Building Blocks; Spatial/Temporal Models with Certain Detection; Dealing with Heterogeneity; Integrated Models; Dynamic and Open Population Models -- Part IV. Overview: Which Method? Notation and Glossary; Statistical Formulation for Observation Models; The Asymptotic Variance of MLEs; State Models for Mark-Recapture and Removal Methods -- References -- Index.},
author = {Borchers, D. L. and Buckland, S. T. and Zucchini, W.},
isbn = {1447137086},
pages = {314},
publisher = {Springer London},
title = {{Estimating Animal Abundance : Closed Populations}},
year = {2002}
}
@misc{Xie2018,
abstract = {R Package},
annote = {R package version 1.20},
author = {Xie, Yihui},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://yihui.name/knitr/},
year = {2018}
}
@techreport{Gide2013,
author = {Gide, L},
publisher = {ARTEMIS},
title = {{Embedded/ Cyber-Physical Systems ARTEMIS Major Challenges: 2014-2020}},
volume = {DRAFT Adde},
year = {2013}
}
@misc{Natio,
abstract = {The Vice County system for Great Britain was devised by an English botanist, Hewett Cottrell Watson, for the purposes of illustrating plant distributions.},
author = {{National Biodiversity Network}},
title = {{Mapping - Watsonian Vice Counties - National Biodiversity Network}},
url = {https://nbn.org.uk/tools-and-resources/nbn-toolbox/watsonian-vice-county-boundaries/},
urldate = {2019-02-11},
year = {2018}
}
@misc{Zeng2017,
abstract = {The science of science (SOS) is a rapidly developing field which aims to understand, quantify and predict scientific research and the resulting outcomes. The problem is essentially related to almost all scientific disciplines and thus has attracted attention of scholars from different backgrounds. Progress on SOS will lead to better solutions for many challenging issues, ranging from the selection of candidate faculty members by a university to the development of research fields to which a country should give priority. While different measurements have been designed to evaluate the scientific impact of scholars, journals and academic institutions, the multiplex structure, dynamics and evolution mechanisms of the whole system have been much less studied until recently. In this article, we review the recent advances in SOS, aiming to cover the topics from empirical study, network analysis, mechanistic models, ranking, prediction, and many important related issues. The results summarized in this review significantly deepen our understanding of the underlying mechanisms and statistical rules governing the science system. Finally, we review the forefront of SOS research and point out the specific difficulties as they arise from different contexts, so as to stimulate further efforts in this emerging interdisciplinary field.},
author = {Zeng, An and Shen, Zhesi and Zhou, Jianlin and Wu, Jinshan and Fan, Ying and Wang, Yougui and Stanley, H. Eugene},
booktitle = {Physics Reports},
doi = {10.1016/j.physrep.2017.10.001},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - 2017 - The science of science From the perspective of complex systems.pdf:pdf},
issn = {03701573},
keywords = {Complex networks,Scholarly data,Science of science},
month = {nov},
pages = {1--73},
publisher = {North-Holland},
title = {{The science of science: From the perspective of complex systems}},
url = {https://www.sciencedirect.com/science/article/pii/S0370157317303289?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {714-715},
year = {2017}
}
@misc{Inkscape2015,
annote = {id: 1},
author = {Inkscape},
number = {10/6/2015},
title = {{Inkscape Vector Graphics Editor}},
url = {https://inkscape.org/en/},
volume = {2015},
year = {2015}
}
@article{TOVEE1995,
annote = {id: 1; issn: print 01695347; publication{\_}type: full{\_}text},
author = {TOVEE, M},
doi = {10.1016/S0169-5347(00)89179-X},
issn = {01695347},
journal = {Trends in Ecology {\&} Evolution},
number = {11},
pages = {455 {\textless}last{\_}page{\textgreater} 460},
title = {{Ultra-violet photoreceptors in the animal kingdom: their distribution and function }},
url = {http://dx.doi.org/10.1016/s0169-5347(00)89179-x},
volume = {10},
year = {1995}
}
@article{Pearl2013,
abstract = {The direct effect of one eventon another can be defined and measured byholding constant all intermediate variables between the two.Indirect effects present conceptual andpractical difficulties (in nonlinear models), because they cannot be isolated by holding certain variablesconstant. This paper shows a way of defining any path-specific effectthat does not invoke blocking the remainingpaths.This permits the assessment of a more naturaltype of direct and indirect effects, one thatis applicable in both linear and nonlinear models. The paper establishesconditions under which such assessments can be estimated consistentlyfrom experimental and nonexperimental data,and thus extends path-analytic techniques tononlinear and nonparametric models.},
archivePrefix = {arXiv},
arxivId = {1301.2300},
author = {Pearl, Judea},
eprint = {1301.2300},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2013 - Direct and Indirect Effects.pdf:pdf},
journal = {Wiley StatsRef: Statistics Reference Online},
month = {jan},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Direct and Indirect Effects}},
url = {http://arxiv.org/abs/1301.2300},
year = {2013}
}
@article{Cornejo2018,
abstract = {The construction of reducts, that is, minimal sets of attributes containing the main information of a database, is a fundamental task in different frameworks, such as in Formal Concept Analysis (FCA) and Rough Set Theory (RST). This paper will be focused on a general fuzzy extension of FCA, called multi-adjoint concept lattice, and we present a study about the attributes generating meet-irreducible elements and on the reducts in this framework. From this study, we introduce interesting results on the cardinality of reducts and the consequences in the classical case.},
author = {Cornejo, M. Eugenia and Medina, Jes{\'{u}}s and Ram{\'{i}}rez-Poussa, Elo{\'{i}}sa},
doi = {10.1016/j.ins.2017.08.099},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Cornejo, Medina, Ram{\'{i}}rez-Poussa - 2018 - Characterizing reducts in multi-adjoint concept lattices.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Attribute reduction,Formal concept analysis,Reducts},
month = {jan},
pages = {364--376},
publisher = {Elsevier},
title = {{Characterizing reducts in multi-adjoint concept lattices}},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517302943?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {422},
year = {2018}
}
@book{Xie2017,
address = {London, UK},
author = {Xie, Yihui},
isbn = {9781138700109},
pages = {138},
publisher = {Chapman and Hall/CRC},
title = {{Bookdown: Authoring Books and Technical Documents with R Markdown}},
url = {https://bookdown.org/yihui/bookdown/get-started.html},
year = {2017}
}
@inproceedings{palmer1999thedesign,
author = {Palmer, P J and Williams, D J},
booktitle = {Circuit World},
isbn = {0953585808},
number = {(4)},
organization = {Harrogate, Yorkshire},
pages = {22--24},
title = {{The Constraints of Vias and Layers and Their Effect on PCB Design Strategy}},
volume = {25},
year = {1999}
}
@misc{RCoreTeam2016,
abstract = {This is a guide to extending R, describing the process of creating R add-on packages, writing R documentation, R's system and foreign language interfaces, and the R API.},
author = {{R Core Team}},
title = {{Writing R Extensions}},
url = {https://cran.r-project.org/doc/manuals/r-release/R-exts.html{\#}Creating-R-packages},
urldate = {2018-02-27},
year = {2016}
}
@article{Bornmann2013,
abstract = {According to current research in bibliometrics, percentiles (or percentile rank classes) are the most suitable method for normalizing the citation counts of individual publications in terms of the subject area, the document type, and the publication year. Up to now, bibliometric research has concerned itself primarily with the calculation of percentiles. This study suggests how percentiles (and percentile rank classes) can be analyzed meaningfully for an evaluation study. Publication sets from four universities are compared with each other to provide sample data. These suggestions take into account on the one hand the distribution of percentiles over the publications in the sets (universities here) and on the other hand concentrate on the range of publications with the highest citation impact—that is, the range that is usually of most interest in the evaluation of scientific performance. TS - RIS},
archivePrefix = {arXiv},
arxivId = {1206.1741},
author = {Bornmann, Lutz},
doi = {10.1002/asi.22792},
eprint = {1206.1741},
isbn = {1532-2890},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
keywords = {Big Data,bibliometrics},
mendeley-tags = {Big Data},
month = {mar},
number = {3},
pages = {587--595},
pmid = {502955140},
title = {{How to analyze percentile citation impact data meaningfully in bibliometrics: The statistical analysis of distributions, percentile rank classes, and top-cited papers}},
url = {http://doi.wiley.com/10.1002/asi.22792},
volume = {64},
year = {2013}
}
@article{Hanquinet2019,
abstract = {Do ticketing data and national survey data on attendance tell the same story? This question is particularly important in the context of debates over the power of new forms of data to supplant the “traditional” survey methods that have underpinned our understanding of the social stratification of culture. This paper compares three data sources on attendance: the Active Lives Survey, the Taking Part Survey, and Audience Finder. We first compare self-reported attendance at events in each English local authority from the Active Lives survey with ticket sales data, finding a close relationship. We follow up by comparing the distributions of ticket buyers across the Indices of Multiple Deprivation with those from Taking Part, finding that for widely-ticketed and widely-attended art forms they track closely together, providing support for existing trends. Ticketing data does not seem to offer more information on social stratification than traditional social science sources. However, we extend the comparison through more detailed analysis of subcategories within less well-researched forms – literature and dance events – where numbers of attendees are lower, with accompanying uncertainty in survey sources. We find that the audiences for dance vary widely, with ballet attendance being heavily socially stratified but attendance at contemporary dance much more similar to the general population. However, we find that audiences for literature events are more heavily socially stratified than almost any other art form, almost regardless of the subcategory. The power of new datasets is in offering specificity about artforms, rather than overturning what we know about culture and inequality.},
author = {Hanquinet, Laurie and O'Brien, Dave and Taylor, Mark},
doi = {10.1080/09548963.2019.1617941},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hanquinet, O'Brien, Taylor - 2019 - The coming crisis of cultural engagement Measurement, methods, and the nuances of niche activities.pdf:pdf},
issn = {14693690},
journal = {Cultural Trends},
keywords = {Active Lives,Audience Finder,Taking Part,cultural consumption,dance,literary events},
month = {may},
number = {2-3},
pages = {198--219},
publisher = {Routledge},
title = {{The coming crisis of cultural engagement? Measurement, methods, and the nuances of niche activities}},
volume = {28},
year = {2019}
}
@article{Giraldo2018,
abstract = {A literature review on spatial big data analysis is given. We show an application of Universal Kriging to a massive spatial dataset. We also present some perspectives of future work in this field.},
author = {Giraldo, Ram{\'{o}}n and Dabo-Niang, Sophie and Mart{\'{i}}nez, Sergio},
doi = {10.1016/j.spl.2018.02.025},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Giraldo, Dabo-Niang, Mart{\'{i}}nez - 2018 - Statistical modeling of spatial big data An approach from a functional data analysis perspective.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Functional data,Spatial big data,Universal kriging},
month = {may},
pages = {126--129},
publisher = {North-Holland},
title = {{Statistical modeling of spatial big data: An approach from a functional data analysis perspective}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300701?via{\%}3Dihub},
volume = {136},
year = {2018}
}
@article{Isaac2015,
abstract = {Biological recording is in essence a very simple concept in which a record is the report of a species at a physical location at a certain time. The collation of these records into a dataset is a powerful approach to addressing large-scale questions about biodiversity change. Records are collected by volunteers at times and places that suit them, leading to a variety of biases: uneven sampling over space and time, uneven sampling effort per visit and uneven detectability. These need to be controlled for in statistical analyses that use biological records. In particular, the data are ‘presence-only', and lack information on the sampling protocol or intensity. Submitting ‘complete lists' of all the species seen is one potential solution because the data can be treated as ‘presence–absence' and detectability of each species can be statistically modelled. The corollary of bias is that records vary in their ‘information content'. The information content is a measure of how much an individual record, or collection of records, contributes to reducing uncertainty in a parameter of interest. The information content of biological records varies, depending on the question to which the data are being applied. We consider a set of hypothetical ‘syndromes' of recording behaviour, each of which is characterized by different information content.We demonstrate how these concepts can be used to support the growth of a particular type of recording behaviour. Approaches to recording are rapidly changing, especially with the growth of mass participation citizen science. We discuss how these developments present a range of challenges and opportunities for biological recording in the future.},
author = {Isaac, Nick J.B. and Pocock, Michael J.O.},
doi = {10.1111/bij.12532},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Isaac, Pocock - 2015 - Bias and information in biological records.pdf:pdf},
isbn = {1095-8312},
issn = {10958312},
journal = {Biological Journal of the Linnean Society},
keywords = {Citizen science,GBIF,Human behaviour,Information content,Recording behaviour},
month = {jul},
number = {3},
pages = {522--531},
pmid = {16758113},
publisher = {Oxford University Press},
title = {{Bias and information in biological records}},
url = {https://academic.oup.com/biolinnean/article-lookup/doi/10.1111/bij.12532},
volume = {115},
year = {2015}
}
@book{Wickham2019,
address = {London, UK},
author = {Wickham, Hadley},
edition = {2},
isbn = {0815384572},
title = {{Advanced R, Second Edition (Chapman {\&} Hall/CRC The R Series)}},
url = {https://adv-r.hadley.nz/},
year = {2019}
}
@inproceedings{palmer1998understandingpackages,
author = {Palmer, P J and Williams, D J},
booktitle = {Proceedings of the IEEE Symposium on IC/Package Design Integration},
isbn = {0-8186-8433-X},
month = {feb},
organization = {Santa Cruz, USA},
pages = {58--63},
title = {{Understanding Models of Substrate Behaviour for the Routing of High I/O Packages}},
year = {1998}
}
@article{Colledge2014,
abstract = {The increase of data availability and computational advances has led to a plethora of metrics and indicators being developed for different levels of research evaluation. Whether at the individual, program, department or institution level, there are numerous methodologies and indicators offered to capture the impact of research output. These advances have also highlighted the fact that metrics must be applied appropriately depending on the goal and subject of the evaluation and should be used alongside qualitative inputs, such as peer review.},
author = {Colledge, Lisa and Halevi, Gali},
journal = {Research Trends},
number = {39},
pages = {1--6},
title = {{Standardizing research metrics and indicators – Perspectives {\&} approaches}},
url = {https://www.researchtrends.com/issue-39-december-2014/standardizing-research-metrics-and-indicators/},
year = {2014}
}
@article{Alawamleh2012,
abstract = {The purpose of this paper is to work towards a comprehensive study of the risk in collaborative network. The environment within which SMEs have to function in the 21st century is one that is increasingly competitive and dynamic and therefore, simply to cope in such a situation, SMEs have to seek methods to employ. Collaboration is necessary in order for enterprises to compete and to operate with speed and flexibility. However, there is some risk due to this relationship. In order that the challenges can be met successfully, it is important that enterprises should be helped to both recognise the risks and then surmount them.},
author = {Alawamleh, M and Popplewell, K},
doi = {10.1504/IJSOM.2012.047952},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Alawamleh, Popplewell - 2012 - Risk in collaborative networks relationships analysis.pdf:pdf},
journal = {International Journal of Services and Operations Management},
keywords = {SME,case study,collaboration,collaborative networks,commitment,communication,information sharing,risk,small and medium sized enterprise,trust},
number = {4},
pages = {431--446},
title = {{Risk in collaborative networks: relationships analysis}},
url = {https://www.researchgate.net/publication/264821596},
volume = {12},
year = {2012}
}
@misc{LRWT2018,
abstract = {Rutland Water Nature Reserve is unique in that it was declared a reserve before it existed. The wildlife potential of the proposed reservoir was recognised as early as 1969; reserve boundaries and the construction of lagoons were formulated in 1972 and in 1975 the Trust signed a management agreement with the Anglian Water Authority. In 2002 the areas managed by the Wildlife Trust were increased to include Barnsdale, Armley, and Hambleton Woods and Berrybutts Spinney.},
author = {LRWT},
title = {{Rutland Water - Leicestershire {\&} Rutland Wildlife Trust}},
url = {https://www.lrwt.org.uk/nature-reserves/rutland-water/},
urldate = {2018-11-09},
year = {2018}
}
@article{Hardwick1950,
abstract = {One of the most essential procedures in the identification of many Lepidop-tera is an examination of the genitalia, and for a most critical examination of these organs slide mounts must he prepared. Tn many cases, the preparation of slides is one of the most neglected arts in lepidopterology. This is to be deplored, for a well-prepared slide is not only hantnclsome but also much more valuable taxonomically than a messily prepared one. For large and medium-sized specimens, I have found the following technique very gratifying in producing slides of uniformly high quality. Certain phases of the technique are not particularly well adapted for working with smaller moths. Labelling The specimen whose genitalia are to be studied is first provided with a number by which cross-reference may be made between slide mount and specimen. Slide number labels are prepared in duplicate. One of these is affixed to the pin holding the specimen, and the other accompanies the genitalia through the various reagents. For the latter purpose the slide number should be written on white card with an HB pencil. Corrosion The abdomens of specimens to be studied are immersed in a ten per cent solution of potassium hydroxide overnight. A few hours more or less in the caustic does little to alter its action. Generally, the use of cold caustic over a more or less prolonged period gives much more uniform results than relatively short immersion in hot or boiling caustic. Dissection and Dehydration After removal from the caustic the abdomen is placed in a dissection dish containing 30 per cent alcohol, in which the genitalia are removed and prepared for subsequent treatment. Male Genitalia.-To remove male genitalia from the abdomen, the closed ends of a pair of curved forceps are placed on the lateral surface of the abdomen at the anterior end. This forces the contents toward the rear. A second pair of forceps.is then placed on the abdomen immediately posterior to the first pair. This process is continued backward along the length of the abdomen. The},
author = {Hardwick, D. F.},
doi = {10.4039/Ent82231-11},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hardwick - 1950 - Preparation of Slide Mounts of Lepidopterous Genitalia.pdf:pdf},
issn = {19183240},
journal = {The Canadian Entomologist},
number = {11},
pages = {231--235},
publisher = {Cambridge University Press},
title = {{Preparation of Slide Mounts of Lepidopterous Genitalia}},
url = {https://doi.org/10.4039/Ent82231-11},
volume = {82},
year = {1950}
}
@article{Vicente1997,
annote = {'Muddling through' is a technique for these kinds of problems suggested by CE Lindblom 1959 The science of muddling through, Public Admin Review, 19, 78-88.

• Use iterative strategy or nonlinear strategy
• Carry out both top-down and bottom-up studies
• Evolve, don't revolutionise
• Let decisions emerge from groups, not from individuals
• Largely guided by informal human judgement
• Guided by satisficing, not optimising, criteria

'Wicked problems' named by HWJ Rittel {\&} MM Webber, 1973, Dilemmas in a general theory of planning, Policy Sciences, 4, 155-169. They have the characteristcis:

• There is no definitive formulation of a wicked problem (unfixed parameters, and different perspectives)
• Wicked problems have no stopping rules (you stop when you run out of resources)
• Solutions to wicked problems are good/bad, not true/false (depending on your perspective - also later alternatives are better because there are more constraints considered)
• There is no immediate or ultimate test of a solution to a wicked problem (because of complexity, and slowness of relevant feedback on performance)
• Every solution to a wicked problem is a one-shot operation (no opportunity to learn by trial and error, and immense difficulties in changing a realised design)
• Wicked problems do not have an enumerable (or an exhaustively describable) set of potential solutions (i.e. not closed problems)
• Every wicked problem is esentially unique (e.g. because of context for the problem)},
author = {Vicente, K J and Burns, C M and Pawlak, W S},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vicente, Burns, Pawlak - 1997 - Muddling through wicked design problems.pdf:pdf},
journal = {Ergonomics in Design},
keywords = {SIMPLOFI,allocation of functions,automation,business process re-engineering,control rooms,core competences,design process,harvard business school,information systems,interface design,job design,learning organisation,philosophy,product introduction process,reference architectures,socio-tech,staffing,systems,workstation design},
number = {1},
pages = {25--30},
title = {{Muddling through wicked design problems}},
volume = {5},
year = {1997}
}
@article{Gandomi2015,
abstract = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95{\%} of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.},
author = {Gandomi, Amir and Haider, Murtaza},
doi = {10.1016/J.IJINFOMGT.2014.10.007},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gandomi, Haider - 2015 - Beyond the hype Big data concepts, methods, and analytics.pdf:pdf},
issn = {0268-4012},
journal = {International Journal of Information Management},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {apr},
number = {2},
pages = {137--144},
publisher = {Pergamon},
title = {{Beyond the hype: Big data concepts, methods, and analytics}},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214001066?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {35},
year = {2015}
}
@article{Aspnes2007,
abstract = {We give a formal model for systems that store data in entangled form. We propose a new notion of entanglement, called all-or-nothing integrity (AONI) that binds the users' data in a way that makes it hard to corrupt the data of any one user without corrupting the data of all users. AONI can be a useful defense against negligent or dishonest storage providers who might otherwise be tempted to discard documents belonging to users without much clout. We show that, if all users use a fixed standard recovery algorithm, we can implement AONI using a MAC, but, if some of the users adopt instead a non-standard recovery algorithm provided by the dishonest storage provider, AONI can no longer be achieved. However, even for the latter scenario, we describe a simple entangling mechanism that provides AONI for a restricted class of destructive adversaries. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Aspnes, James and Feigenbaum, Joan and Yampolskiy, Aleksandr and Zhong, Sheng},
doi = {10.1016/j.tcs.2007.07.021},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Aspnes et al. - 2007 - Towards a theory of data entanglement.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {All-or-nothing integrity,Data entanglement,Untrusted storage,Upgrade attacks},
month = {dec},
number = {1-2},
pages = {26--43},
publisher = {Elsevier},
title = {{Towards a theory of data entanglement}},
volume = {389},
year = {2007}
}
@misc{Rokach2009,
abstract = {Ensemble methodology, which builds a classification model by integrating multiple classifiers, can be used for improving prediction performance. Researchers from various disciplines such as statistics, pattern recognition, and machine learning have seriously explored the use of ensemble methodology. This paper presents an updated survey of ensemble methods in classification tasks, while introducing a new taxonomy for characterizing them. The new taxonomy, presented from the algorithm designer's point of view, is based on five dimensions: inducer, combiner, diversity, size, and members' dependency. We also propose several selection criteria, presented from the practitioner's point of view, for choosing the most suitable ensemble method. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Rokach, Lior},
booktitle = {Computational Statistics and Data Analysis},
doi = {10.1016/j.csda.2009.07.017},
isbn = {0167-9473},
issn = {01679473},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {12},
pages = {4046--4072},
title = {{Taxonomy for characterizing ensemble methods in classification tasks: A review and annotated bibliography}},
url = {https://www.sciencedirect.com/science/article/pii/S0167947309002631},
volume = {53},
year = {2009}
}
@article{Ekbia2015,
abstract = {The recent interest in Big Data has generated a broad range of new academic, corporate, and policy practices along with an evolving debate amongst its proponents, detractors, and skeptics. While the practices draw on a common set of tools, techniques, and technologies, most contributions to the debate come either from a particular disciplinary perspective or with an eye on a domain-specific issue. A close examination of these contributions reveals a set of common problematics that arise in various guises in different places. It also demonstrates the need for a critical synthesis of the conceptual and practical dilemmas surrounding Big Data. The purpose of this article is to provide such a synthesis by drawing on relevant writings in the sciences, humanities, policy, and trade literature. In bringing these diverse literatures together, we aim to shed light on the common underlying issues that concern and affect all of these areas. By contextualizing the phenomenon of Big Data within larger socio-economic developments, we also seek to provide a broader understanding of its drivers, barriers, and challenges. This approach allows us to identify attributes of Big Data that need to receive more attention — autonomy, opacity, and generativity, disparity, and futurity — leading to questions and ideas for moving beyond dilemmas.},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Ekbia, Hamid and Mattioli, Michael and Kouper, Inna and Arave, G. and Ghazinejad, Ali and Bowman, Timothy and Suri, Venkata Ratandeep and Tsou, Andrew and Weingart, Scott and Sugimoto, Cassidy R.},
doi = {10.1002/asi.23294},
eprint = {0803.1716},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ekbia et al. - 2015 - Big data, bigger dilemmas A critical review.pdf:pdf},
isbn = {2330-1643},
issn = {23301643},
journal = {Journal of the Association for Information Science and Technology},
keywords = {data,information policy,socioeconomic activities},
month = {aug},
number = {8},
pages = {1523--1545},
pmid = {502955140},
publisher = {Wiley-Blackwell},
title = {{Big data, bigger dilemmas: A critical review}},
url = {http://doi.wiley.com/10.1002/asi.23294},
volume = {66},
year = {2015}
}
@article{Jones2020,
abstract = {Abstract 1. Cultural Ecosystem Services (CES) encompass a range of social, cultural and health benefits to local communities, for example recreation, spirituality, a sense of place and local identity. However, these complex and place‐specific CES are often overlooked in rapid land management decisions and assessed using broad, top–down approaches. 2. We use the Toolkit for Ecosystem Service Site‐based Assessment (TESSA) to examine a novel approach to rapid assessment of local CES provision using inductive, participatory methods. We combined free‐listing and participatory geographic information systems (GIS) techniques to quantify and map perceptions of current CES provision of an urban green space. The results were then statistically compared with those of a proposed alternative scenario with the aim to inform future decision‐making. 3. By identifying changes in the spatial hotspots of CES in our study area, we revealed a spatially specific shift towards positive sentiment regarding several CES under the alternative state with variance across demographic and stakeholder groups. Response aggregations in areas of proposed development reveal previously unknown stakeholder preferences to local decision‐makers and highlight potential trade‐offs for conservation management. Free‐listed responses revealed deeper insight into personal opinion and context. 4. This work serves as a useful case study on how the perceptions and opinions of local people regarding local CES could be accounted for in the future planning of an urban greenspace and how thorough analysis of CES provision is important to fully inform local‐scale conservation and planning for the mutual benefit of local communities and nature.},
author = {Jones, Lizzie and Holland, Robert A. and Ball, Jennifer and Sykes, Tim and Taylor, Gail and Ingwall‐King, Lisa and Snaddon, Jake L. and {S.‐H. Peh}, Kelvin},
doi = {10.1002/pan3.10057},
editor = {Ladle, Richard},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jones et al. - 2020 - A place‐based participatory mapping approach for assessing cultural ecosystem services in urban green space.pdf:pdf},
issn = {2575-8314},
journal = {People and Nature},
keywords = {CES,TESSA,chalk stream,cultural benefits,decision‐making,perceptions},
month = {mar},
number = {1},
pages = {123--137},
publisher = {Wiley},
title = {{A place‐based participatory mapping approach for assessing cultural ecosystem services in urban green space}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pan3.10057},
volume = {2},
year = {2020}
}
@misc{GitHubCoreTeam2018,
abstract = {GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.

This tutorial teaches you GitHub essentials like repositories, branches, commits, and Pull Requests. You'll create your own Hello World repository and learn GitHub's Pull Request workflow, a popular way to create and review code},
author = {{GitHub Core Team}},
booktitle = {Web Page},
title = {{GitHub Guides}},
url = {https://guides.github.com/activities/hello-world/},
urldate = {2018-04-10},
year = {2018}
}
@article{Azzone2018,
abstract = {Big data are often presented as a strategic opportunity for the design of new public policies, improving the quality and effectiveness of public services and using resources more efficiently. The paper discusses such opportunities and identifies a few open questions.},
author = {Azzone, Giovanni},
doi = {10.1016/j.spl.2018.02.022},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Azzone - 2018 - Big data and public policies Opportunities and challenges.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Big data,Personal policies,Public policy design,Public value},
month = {may},
pages = {116--120},
publisher = {North-Holland},
title = {{Big data and public policies: Opportunities and challenges}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300671},
volume = {136},
year = {2018}
}
@book{Mecca2017,
abstract = {Schema mapping management is an important research area in data transformation, integration, and cleaning systems. The reasons for its success can be found in the declarative nature of its building block (thus enabling clean semantics and easy to use design tools) paired with the efficiency and modularity in the deployment step. In this chapter we cover the evolution of schema-mappings through what we identify as three main ages.We start presenting the foundations of schema mapping tools and the first tools aimed at translating data from a source to a target schema in the first, heroic age. We then discuss the silver age, when schema mapping tools have grown their way into complex systems and have been translated into both commercial and open-source tools. Finally, we show how recent results in schema-mapping are stimulating a third, golden age, with novel research opportunities and a new generation of systems capable of dealing with a significantly larger class of real-life applications.},
author = {Mecca, Giansalvatore and Papotti, Paolo and Santoro, Donatello},
booktitle = {Springer},
chapter = {Schema Map},
doi = {https://doi.org/10.1007/978-3-319-61893-7_12},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Mecca, Papotti, Santoro - 2017 - Schema Mappings from Data Translation to Data Cleaning.pdf:pdf},
isbn = {978-3-319-61893-7},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {203--217},
publisher = {Springer, Cham},
title = {{Schema Mappings: from Data Translation to Data Cleaning}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-61893-7{\_}12},
year = {2017}
}
@inproceedings{Spitalny1970,
abstract = {The rapidly improving capabilities of large scale integrated circuit (LSI) manufacturing technology provide the opportunity for dramatic advances in digital system economy, capacity and flexibility for those who are prepared to modify their design philosophy and procedures to take full advantage of it. This involves much more than the simple first step of replacing core memory with standard LSI memory arrays. With an effective computer-aided design system it is possible to combine logic, memory, shift registers, and other digital system circuitry on the same chip in a custom LSI design at a total design, test and initial production lot cost competitive with the design, development, packaging and initial production and test cost for equipment of comparable complexity using discrete components and standard integrated circuits. The resulting LSI custom unit would cost much less than its conventional counterpart in subsequent production as well as being smaller, more reliable, and consuming less power.},
address = {New York, New York, USA},
author = {Spitalny, Arnold},
booktitle = {Proceedings of the June 1970 design automation workshop on Design automation - DAC '70},
doi = {10.1145/800160.805142},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Spitalny - 1970 - Designing a system on a chip.pdf:pdf},
pages = {336--345},
publisher = {ACM Press},
title = {{Designing a system on a chip}},
url = {http://portal.acm.org/citation.cfm?doid=800160.805142},
year = {1970}
}
@misc{BBCNews2017,
abstract = {Archaeologists admit they have been surprised by the interest a dig that has uncovered a Roman street. Among the finds from the excavation at the old Stibbe factory site in central Leicester is the largest mosaic uncovered in the area for 150 years. Items including brooches, pottery and coins have also been unearthed. An open day held at the beginning of May saw hundreds of people queuing down the street, and the remains have already been seen by some 5,000 people. Two last open days are being held this weekend before the site closes to the public. The mosaic will then be conserved and probably placed on display.},
author = {{BBC News}},
booktitle = {BBCNews},
title = {{Roman mosaic wows history fans in Leicester - BBC News}},
url = {http://www.bbc.co.uk/news/uk-england-leicestershire-39981438},
urldate = {2017-08-24},
year = {2017}
}
@article{Pettersson2008,
abstract = {Franz{\'{e}}n, M.: Comparing wine-based and beer-based baits for moth trapping: a field experiment. [Vin-baserade och {\"{o}}l-baserade beten f{\"{o}}r nattfj{\"{a}}rilsf{\aa}ngst: ett f{\"{a}}ltexperiment.]-Entomologisk Tidskrift 129 (3): 129-134. Uppsala, Sweden 2008. ISSN 0013-886x. Bait traps for moth trapping are increasing in use but little is known about the relative performance of different baits. Here we describe recipes for two of the most commonly used bait types, a wine-based bait and a beer-based bait and evaluate them in a field experiment on Gotland in 2007. Jalas traps (commercially available in Finland) were used and ten traps, five with beer bait and five with wine bait were placed out in a pairwise design and retrieved after 48 hours. Both baits performed well and a total of 365 individuals from 35 different moth species were caught. There were no statistically significant differences in performance between the two baits, neither in terms of number of species caught, nor in terms of total number of individuals. We conclude that both bait types are well suited for moth trapping and that the choice of either is primarily a matter of taste, cost, and availability of ingredients.},
author = {Pettersson, Lars B and Franz{\'{e}}n, Markus},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pettersson, Franz{\'{e}}n - 2008 - Comparing wine-based and beer-based baits for moth trapping a field experiment.pdf:pdf},
journal = {Ent. Tidskr.},
pages = {129},
title = {{Comparing wine-based and beer-based baits for moth trapping: a field experiment}},
url = {http://www.viestipaino.},
year = {2008}
}
@article{Shimadzu2015,
abstract = {Quantifying biodiversity aspects such as species presence/ absence, richness and abundance is an important challenge to answer scientific and resource management questions. In practice, biodiversity can only be assessed from biological material taken by surveys, a difficult task given limited time and resources. A type of random sampling, or often called sub-sampling, is a commonly used technique to reduce the amount of time and effort for investigating large quantities of biological samples. However, it is not immediately clear how (sub-)sampling affects the estimate of biodiversity aspects from a quantitative perspective. This paper specifies the effect of (sub-)sampling as attenuation of the species abundance distribution (SAD), and articulates how the sampling bias is induced to the SAD by random sampling. The framework presented also reveals some confusion in previous theoretical studies.},
author = {Shimadzu, Hideyasu and Darnell, Ross},
doi = {10.1098/rsos.140219},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shimadzu, Darnell - 2015 - Attenuation of species abundance distributions by sampling.pdf:pdf},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {Biodiversity estimation,Marine surveys,Rarefaction,Richness,Sampling,Species presence/absence},
month = {apr},
number = {4},
pages = {140219--140219},
pmid = {26064626},
title = {{Attenuation of species abundance distributions by sampling}},
url = {http://rsos.royalsocietypublishing.org/cgi/doi/10.1098/rsos.140219},
volume = {2},
year = {2015}
}
@techreport{Srivastava2002,
abstract = {Introduction to data mining 9 Data mining process 9 Data Mining techniques Classification Clustering Topic Analysi s Concept Hierarchy Content Relevance},
author = {Srivastava, Jaideep},
booktitle = {National Science Foundation Workshop on Next Generation Data Mining (NGDM'02)},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava - 2002 - Web Mining Accomplishments {\&} Future Directions.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {1},
pages = {51--56},
title = {{Web Mining : Accomplishments {\&} Future Directions}},
url = {http://ieee.org.ar/downloads/Srivastava-tut-pres.Pdf http://users.atw.hu/ignatius/mining.pdf},
year = {2002}
}
@article{webb2009packagingthermoplastics,
abstract = {Successful innovation requires effective communication within and between technical and nontechnical communities, which can be challenging due to different educational backgrounds, experience, perceptions, and attitudes. Roadmapping has emerged as a method that can enable effective dialogue between these groups, and the way in which information is structured is a key feature that enables this communication. This is an area that has not received much attention in the literature, and this article seeks to address this gap by describing in detail the structures that have been successfully applied in roadmapping workshops and processes, from which key learning points and future research directions are identified.},
annote = {From Duplicate 6 (Statistical Optimisation of Thermoplastic Injection Moulding Process for the Encapsulation of Electronic Subassembly - Teh, N J; Conway, P P; Palmer, P J; Kioul, A; Prosser, S)

Encapsulated electronics is becoming a major technology theme within the research group with significant industrial applications.

From Duplicate 7 (Foresight Vehicle: Physical Media for Automotive Multiplex Networks Implemented on Large Area Flexible Printed Circuit Boards - Webb, D P; Cottrill, M C W; Jaggernauth, W A; West, A A; Palmer, P J; Conway, P P)

Invited participation in UK/DTI delegation at world leading Society of Automotive Engineers (SAE) Congress in Detroit, 2002. Paper was selected by an SAE committee to appear in Transactions of the SAE. (Reference Dr Graham Farmer, GTS Flexible Materials Gwent.)
Unmapped Imported Data : Isbn = ISBN 0-7680-1291-0

From Duplicate 8 (Electromagnetic Compatibility Performance of Large Area Flexible Printed Circuit Automotive Harness - Webb, D P; Jaggernauth, W A; Cottrill, M C W; West, A A; Conway, P P; Palmer, P J)

Results enabled project major industrial partner In2connect to offer EMC protection as part of product offering. No data in academic literature previously available. (In2connect, Long Eaton contact: J. Astill).

From Duplicate 9 (Framework for a Technology-watch Relay Station - Stokes, C A; Palmer, P J)

The technology watch station describe in this paper has been the core of the web presence for the PRIME Faraday partnership and now the Electronics Enabled Products KTN. It is in part responsible for the success of these activities and the continued high level of funding it has attracted since it was launched in 1999.

From Duplicate 10 (A software tool for estimation of PCB substrate utilisation efficiency statistics from scanned images - Hyslop, S M; Palmer, P J; Whalley, D C)

The tool described in this paper remains unique as far as the authors know. The knowledge and understand has been incorporated in other projects and short courses as a way of quantifying the limits to integration and size reduction.},
author = {Phaal, R and Palmer, P J and Webb, D P and Jaggernauth, W A and Cottrill, M C W and Palmer, P J and West, A A and Conway, P P and Hutt, D A and Hopkinson, N and Conway, P P and Palmer, P J and Sarvar, F and Whalley, D C and Hutt, D A and Palmer, P J and Teh, N J and Hyslop, S M and Palmer, P J and Whalley, D C and Stokes, C A and Palmer, P J and Webb, D P and Jaggernauth, W A and Cottrill, M C W and West, A A and Conway, P P and Palmer, P J and Jaggernauth, W A and West, A A and Palmer, P J and Conway, P P and Teh, N J and Conway, P P and Palmer, P J and Kioul, A and Prosser, S and Kioul, A and Williams, D J and Hughes, C and Edwards, T C and Palmer, P J},
doi = {10.1142/S0960313100000150},
issn = {0960-3131},
journal = {Journal of Electronics Manufacturing},
month = {sep},
number = {(3)},
pages = {171--179},
title = {{Technology Trends in Electronics and Photonics and their Effect on Manufacturing and Assembly}},
url = {http://dx.doi.org/10.1142/S0960313100000150 http://www.sae.org/technical/papers/2002-01-1130 http://dx.doi.org/10.1243/0954407041580102 http://dx.doi.org/10.1108/03056120510585018 http://dx.doi.org/10.1108/13565360710818439 http://dx.doi.org/10.1109/JMEMS},
volume = {18},
year = {2004}
}
@article{Vitolo2015,
abstract = {Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.},
author = {Vitolo, Claudia and Elkhatib, Yehia and Reusser, Dominik and Macleod, Christopher J.A. and Buytaert, Wouter},
doi = {10.1016/j.envsoft.2014.10.007},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vitolo et al. - 2015 - Web technologies for environmental Big Data.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Big Data,OGC standards,Web services,Web-based modelling},
pages = {185--198},
title = {{Web technologies for environmental Big Data}},
url = {http://creativecommons.org/licenses/by/3.0/},
volume = {63},
year = {2015}
}
@article{Bates2014,
abstract = {Moths are abundant and ubiquitous in vegetated terrestrial environments and are pollinators, important herbivores of wild plants, and food for birds, bats and rodents. In recent years, many once abundant and widespread species have shown sharp declines that have been cited by some as indicative of a widespread insect biodiversity crisis. Likely causes of these declines include agricultural intensification, light pollution, climate change, and urbanization; however, the real underlying cause(s) is still open to conjecture. We used data collected from the citizen science Garden Moth Scheme (GMS) to explore the spatial association between the abundance of 195 widespread British species of moth, and garden habitat and landscape features, to see if spatial habitat and landscape associations varied for species of differing conservation status. We found that associations with habitat and landscape composition were species-specific, but that there were consistent trends in species richness and total moth abundance. Gardens with more diverse and extensive microhabitats were associated with higher species richness and moth abundance; gardens near to the coast were associated with higher richness and moth abundance; and gardens in more urbanized locations were associated with lower species richness and moth abundance. The same trends were also found for species classified as increasing, declining and vulnerable under IUCN (World Conservation Union) criteria. However, vulnerable species were more strongly negatively affected by urbanization than increasing species. Two hypotheses are proposed to explain this observation: (1) that the underlying factors causing declines in vulnerable species (e.g., possibilities include fragmentation, habitat deterioration, agrochemical pollution) across Britain are the same in urban areas, but that these deleterious effects are more intense in urban areas; and/or (2) that urban areas can act as ecological traps for some vulnerable species of moth, the light drawing them in from the surrounding landscape into sub-optimal urban habitats. },
address = {The University of Birmingham, Birmingham, United Kingdom.; The University of Birmingham, Birmingham, United Kingdom.; The Garden Moth Scheme, Birmingham, United Kingdom.; The Garden Moth Scheme, Birmingham, United Kingdom.; The Garden Moth Schem(TRUNCATED},
annote = {id: 1; LR: 20150515; JID: 101285081; OID: NLM: PMC3903603; 2014 [ecollection]; 2012/11/22 [received]; 2013/12/20 [accepted]; 2014/01/27 [epublish]; epublish},
author = {Bates, A J and Sadler, J P and Grundy, D and Lowe, N and Davis, G and Baker, D and Bridge, M and Freestone, R and Gardner, D and Gibson, C and Hemming, R and Howarth, S and Orridge, S and Shaw, M and Tams, T and Young, H},
doi = {10.1371/journal.pone.0086925 [doi]},
issn = {1932-6203; 1932-6203},
journal = {PloS one},
keywords = {Animals,Biodiversity,Conservation of Natural Resources/methods,Ecosystem,Great Britain,Models, Statistical,Moths/physiology,Population Dynamics,Species Specificity,Urbanization/trends},
month = {jan},
number = {1},
pages = {e86925},
title = {{Garden and landscape-scale correlates of moths of differing conservation status: significant effects of urbanization and habitat diversity }},
volume = {9},
year = {2014}
}
@book{Wallach2009,
address = {Oxford, UK},
annote = {Whole emphasis tends to be ademic and researchy; not much about the conext in which these machines will operate, nor about the infrastructure to suport their operations. The emphasis is very much on stand-alone robots. Consequently, they fairly rapidly come to the cnclusion that ethics must be learnt, via top-down AND bottom-up approaches - i.e. they conceive of an hybrid system

Review from New Scientist:
"Six ways to build robots that do humans no harm

14:24 18 November 2008 by Tom Simonite
For similar stories, visit the Books and Art and Robots Topic Guides
With the relentless march of technological progress, robots and other automated systems are getting ever smarter. At the same time they are also being given greater responsibilities, driving cars, helping with childcare, carrying weapons, and maybe soon even pulling the trigger.

But should they be trusted to take on such tasks, and how can we be sure that they never take a decision that could cause unintended harm?

The latest contribution to the growing debate over the challenges posed by increasingly powerful and independent robots is the book Moral Machines: Teaching Robots Right from Wrong.

Authors Wendell Wallach, an ethicist at Yale University, and historian and philosopher of cognitive science Colin Allen, at Indiana University, argue that we need to work out how to make robots into responsible and moral machines. It is just a matter of time until a computer or robot takes a decision that will cause a human disaster, they say.

So are there things we can do to minimise the risks? Wallach and Allen take a look at six strategies that could reduce the danger from our own high-tech creations.

Keep them in low-risk situations

Make sure that all computers and robots never have to make a decision where the consequences can not be predicted in advance.

Likelihood of success: Extremely low. Engineers are already building computers and robotic systems whose actions they cannot always predict.

Consumers, industry, and government demand technologies that perform a wide array of tasks, and businesses will expand the products they offer in order to capitalise on this demand. In order to implement this strategy, it would be necessary to arrest further development of computers and robots immediately.

Do not give them weapons

Likelihood of success: Too late. Semi-autonomous robotic weapons systems, including cruise missiles and Predator drones, already exist. A few machine-gun-toting robots were sent to Iraq and photographed on a battlefield, though apparently were not deployed.

However, military planners are very interested in the development of robotic soldiers, and see them as a means of reducing deaths of human soldiers during warfare.

While it is too late to stop the building of robot weapons, it may not be too late to restrict which weapons they carry, or the situations in which the weapons can be used.

Give them rules like Asimov's 'Three Laws of Robotics'

Likelihood of success: Moderate. Isaac Asimov's famous rules are arranged hierarchically: most importantly robots should not harm humans or through inaction allow harm to them, of secondary importance is that they obey humans, while robotic self-preservation is the lowest priority.

However, Asimov was writing fiction, not building robots. In story after story he illustrates problems that would arise with even these simple rules, such as what the robot should do when orders from two people conflict.

Asimov's rules task robots with some difficult judgements. For example, how could a robot know that a human surgeon cutting into a patient was trying to help them? Asimov's robot stories in fact quite clearly demonstrate the limits of any rule-based morality. Nevertheless, rules can successfully restrict the behaviour of robots that function within very limited contexts.

Program robots with principles

Building robots motivated to create the "greatest good for the greatest number", or to "treat others as you would wish to be treated" would be safer than laying down simplistic rules.

Likelihood of success: Moderate. Recognising the limits of rules, some ethicists look for an over-riding principle that can be used to evaluate all courses of action.

But the history of ethics is a long debate over the value and limits of many proposed single principles. For example, it could seem logical to sacrifice the lives of one person to save the lives of five people. But a human doctor would not sacrifice a healthy person simply to supply organs to five people needing transplants. Would a robot?

Sometimes identifying the best option under a given rule can be extremely difficult. For example, determining which course of action leads to the greatest good would require a tremendous amount of knowledge, and an understanding of the effects of actions in the world. Making such calculations would require time and a great deal of computing power.

Educate robots like children

Machines that learn as they "grow up" could develop sensitivity to the actions that people consider to be right and wrong.

Likelihood of success: Promising, although this strategy requires a few technological breakthroughs. While researchers have created robots able to learn in similar ways to humans, the tools presently available are very limited.

Make machines master emotion

Human-like faculties such as empathy, emotions, and the capacity to read non-verbal social cues should give robots much greater ability to interact with humans. Work has already started on equipping domestic robots with such faculties.

Likelihood of success: Developing emotionally sensitive robots would certainly help implement the previous three solutions discussed. Most of the information we use to make choices and cooperate with others derives from our emotions, as well as our capacity to read gestures and intentions and imagine things from another person's point of view"

My notes:

p40 and elsewhere; TSD:

They talk about people being to challenge a Decision Support Tool that has some credibility. Note this might be a function of possible blame. Hence, TSD; operators, controllers, set a very conservative Beta.

p.45:
"The introduction of Trident submarines carrying nuclear weapons in the late 1980s was one of the factors that seriously jeopardised the fate of the Earth during the arms race between the United States and the Soviet Union. These ships broke the symbolic ten-minute barrier between the launch of weapons and weapon strikes that had been considered necessary for bringing leaders into the decision-making process to evaluate whether images that appeared on radar screens signalled an attack or were harmless. Without time to bring humans into the decision-making process, weapon systems in the Soviet Union would be forced to rely on computer analysis of the data and computers initiating retaliatory measures The future of humanity was about to be placed in the hands of 1980s Soviet computer technology. Luckily for all, the arms race collapsed. But even today, anyone who puts matters of life and death in the hands of computers has failed to understand the limits of current technology." 

p. 58 Ethics of deterministic systems"
Dennett: only kind of freedom humans have is being able to choose among alternatives; the more alternativs, the more freedom you have.
D.C. Dennett 2003 Freedom evolves New York: Viking


p61: (Some ethicists even suggest that it is not possible to act ethically if it is not also possible to act unethically). Note significance of this; if the constraints on behaviour are such as to compel ethical behaviour, we don't have to worry about ethics in the robot.
But, later; "The soldier's concern for his bomb-sniffing robot introduces new ethical possibilities;for example how he would rank the survival of the robot against that of, say, a dog." [Note that this is not a robotics problem, but a system problem].

"Young children and most animals have only a limited sense of the effects of their actions on the well-being of others. For them, the scope of ethical action is limited by the invisibility of the relevant options and outcomes. With increased cognitive sophistication, whether achieved evolutionarily, developmentally, or socially, comes greater awareness of the conflicting goals among agents. It also brings sensitivity to conflicts within an agent's own internal goals."

p.65 Useful quote:
"Nevertheless, realizing the importance of being embodied and embedded in the world provides two important insights. First, much of the information that agents need may already be built into the environments through which they move, making it unnecessary to reproduce or simulate this information internally; that is, one doesn't always need to build a mental model of the world. Second, people;s ability to react with such apparent understanding to their ohysical and social environments owe a lot to the structure of their bodies, limbs and senses, which allows them to process most responses with little or no need for conscious thought ..." 

p.85 Consequentialism:
Defines several versions:
• Utility is dfined in terms of well-being in the world.
• Egoism: consider consequences only for the self. [not a good way fowards]
• Act utilitarianism (each act is assessed for utility)
• Rule utilitarianism (rules for action are assessed in the light of their tendency to increase total utility).
[so, our 'plan fragments with consequences' approach would seem to be a version of Act Utilitarianism. May need to justify why we have gone this way rather than rule utilitarianism. Current answer is that we don't know how to handle rule utilitarianism]

p.87 et seq. Gips' approach:
"... the earliest attempt to outline the computational requirements for any consequentialist robot. [James Gips] outlined the necessary abilities:

1 A way of describing the situation in the world
2 A way of generating possible actions
3 A means of predicting the situation that would result if an action were taken given the current situation
4 A method of evaluating a situation in terms of its goodness or desirability"

p. 91 Asimov's Laws:

0: A robot may not harm humanity, or, by inaction, allow humanity t come to harm. (a much later addition)
1: A robot may not injure a humanbeing or, thrugh inaction, allow a human being to come to harm.
2: A robot mut obey orders given it by human beings except whre such orders conflict with the (Zeroth and) First Law
3: A robot must protect its own existence as long as such protection does not conflict with the (Zeroth,) First or second Law.

p. 120 top-down virtues:
Problem is how many, definitions, rigidity, and conflicts between them
"An artificial agent applying virtues in a top-down fashion would need to have considerable knowledge of psychology to figure out how to apply them in a given situation. For instance, what should one do when an action seems both to apply andviolate a virtue? Imagine that you - or your (ro)bot - have been asked by two people for a favour, but you can only help one of them. The other will perceive your action as unkind. One might feel that being unkind is unacceptable, but how does one determine which party's request to honour? A virtue-based AMA, like its deontological cohorts, could get stuck in endless looping when checking if ts actions are congruent with the prescribed virtues, then reflecting on the checking, and so on."

p. 121 et seq Connectionist virtues:
The neural network approach has strong suport - Aristotle among them in that hi appoach is that you learn virtues through experience. Several others support this approach, too. but note the standard difficulty of neural networks - what exactly is going on inside the network?

Also, discusses Particuarism in this context. "Particularism holds that moral reasons and categories are richly context-sensitive - so much so, in fact, that principles provide people with only very rough guides to appropriate action."

p. 139 Robots in social context:
"Engineers have come to recognise that emotional intelligence, sociability, and a dynamic relationship with the envirnment are essential for (ro)bots to function competently in social contexts."

p.147 Emotions:
q.v. Lazarus, 1991. Decided there are 15 emotions ("core relational themes"):

anger
anxiety
compassion
disgust
envy
fright
guilt
happiness
hope
jealousy
love
pride
relief
sadness
shame

Argument is that these come into play before decision-making, and serve to prune the decision alternatived to be considered (hence prejudices). No mention of their evolutionary role in pre-speech times.

p.156 CogAff architecture
see Sloman 2005

There follows a lot of stuff on what is required for robots to perform in society, but not a lot fo depth. main pont is that there's a lot of research into this area, but still not much advanced. They clearly expect robots to be fully autonomous individually when dealing with humans.

p.172 et seq. Franklin {\&} LIDA:
They seem ethused by this. Learning Intelligent Decision Agent. This is an architecture which they are trying to embody to demonstrate they understnd the issues - gooe engineering principle. Looks like a version of Albus and the Situation Awareness diagram, but with emotions put in by valences.

p. 189 HEADLINES!!:

1 Robots march on Washington to demand their civil rights

2 Terrorist avatar bombs holiday destination

3 Nobel Prize for literature awarded to IBM computer

4 Genocide charges levelled at FARL (Fuerzas Armadas Roboticas de Liberacion)

5 Nanobots repair perforated heart

6 Financial robot amasses personal fortune in currency market

7 UN debates prohibition of self-replicating AI

8 Serial stalker targets robotic sex workers.

X House robots, reprogrammed through contact with military robots, kill owner

p.195 Problems of robots that can learn:
"... the more pressing concern is that very small incremental changes made to structures through which an [autonomous moral agent] acquires and processes the information that it is assimilating will lead to subtle, disturbing and potentially destructive behaviour. For instance, a robot that is learning about social factors related to trust might overgeneralise irrelevant features, for example eye, hair, or skin colour, and develop unwanted prejudices as a result. ... Of particular concern is the possibility that a learning system could discver a way to over-ride control mechaqnisms that function as built-in restraints."

p. 198 Responsibility issues:
"Intelligent machines will pose many new challenges to existing law. We predict that companies producing and utilising intelligent machines will stress the difficulties in determining liability and will encourage no-fault insurance policies. It may also be in their interests to promote a kind of legal status as agents for these machines (similar to that given to corporations) as a means of limiting the financial and legal obligations of those who create and use them. In other words, a kind of de facto moral agency will be atrributed to the systems long before they are capable of acting as fully intelligent autonomous systems."

p. 199 Moral agency:
"... many ethicists have agreed that an action should only be considered moral if it stems from a certain state of the agent's mind - from a certain quality of intention, purpose, motive or disposition."
In other words, you need to know the why, as well as the what, to say an action is moral. But note that the why is often non-observable, or, even more complex, different aspects are observable depending on your own knowledge/ standpoint. Then there's the issue of interactivity; how could this diffuse rsponsibiity?
Need to read Floridi {\&} Sanders for more on this.

p. 215 Epilogue comment:
"Professional ethicists already know that their theories cannot provide real-time decision procedures. Instead, many of them see the project of ethics as aimed at justifying ethical decisions within a single, comprehensive framework. Perfect consequentialism or Kantianism represent ideals against which actions are to be measured. But for such ethicists, the impossible rigour demanded by such perfection looms large. Fr example, can a determined consequentialist also have genuine commitments to friends and family, come what may, or must he subjugate personal relationships to the maximisation of utility?

In our view, this is a monkish pursuit not much removed from the question of whether it is possible to live according to a vow of perfect poverty or perfect silence. People don't want [autonomous moral agents] to replicate the abstractions of moral philosophers any more than they want their neighbours to do so. People want their neighbours to have the capacity to respond flexibly and sensitively in real and virtual environments. They want to have confidence that their neighbours' behaviour will satisfy appropriate norms, and that they can trust their nieghbours' actions."},
author = {Wallach, W and Allen, C},
isbn = {978-0-19-537404-9},
keywords = {AI,Governance,UAV,agents,automation,complexity,consequences,control,culture,dashboard/battlespace,decision-making,design process,empowerment,engineering,errors,ethics,federated control,holons,knowledge configuration,knowledge nets,mental models,network-centric warfare,philosophy,prototyping,reliability,robots,safety,security,situation awareness,socio-tech,software,teams,theory},
publisher = {Oxford University Press},
title = {{Moral machines:  teaching robots right from wrong}},
year = {2009}
}
@book{Saunders,
abstract = {Subject: This book is an introduction to research methods for students planning or undertaking a dissertation or extensive research project in business and management. Preface -- Contributors -- Publisher's acknowledgements -- Business and management research, reflective diaries and the purpose of this book -- Formulating and clarifying the research topic -- Critically reviewing the literature -- Understanding research philosophies and approaches to reasoning -- Formulating the research design -- Negotiating access and research ethics -- Selecting samples -- Using secondary data -- Collecting primary data through observation -- Collecting primary data using semi-structured, in-depth and group interviews -- Collecting primary data using questionnaires -- Analysing quantitative data -- Analysing qualitative data -- Writing and presenting your project report -- Bibliography -- Appendices -- Glossary -- Index.},
address = {Harlow, UK},
annote = {Own Kindle Copy.},
author = {Saunders, M. N. K. and Lewis, Philip and Thornhill, Adrian},
edition = {Seventh},
isbn = {9781292016627},
pages = {741},
publisher = {Pearson Education Ltd},
title = {{Research methods for business students}},
year = {2016}
}
@article{Lessig2000,
annote = {Document in Autonomy!!

See also Sinclair {\#}690 and {\#}3072, making similar points.

His point is that code is not regulated, except by those who write it and those who commission it. And they can build into it what they want, and also the assumptions, explicit or hidden, which then affect the way the code works and the results it produces. Example is:

“RealJukebox is a technology for copying music from a CD to a computer, as well as for downloading music from the Net to store on a computer's hard drive. In October it was revealed that the system was a bit nosy--that it snooped the hard disk of the user and reported back to the company what it found. It did this secretly, of course; RealNetworks didn't tell anyone its product was collecting and reporting personal data. It just did. When this snooping was discovered, the company at first defended the practice (saying no data about individuals were actually stored). But it quickly came to its senses, and promised not to collect such data.”

Also certification: The certifying algorithm (e.g. that you are an adult, living at X, ID number Y) and are OK to access site Z could do this in 2 ways; giving you a password for access, or adding to this some or all the certifying data. It is the people doing the coding who fix this, depending on the instructions given to them plus their assuptions about what these instructions mean.

“Thus whether the certification architecture that emerges protects privacy depends upon the choices of those who code. Their choices depend upon the incentives they face. If protecting privacy is not an incentive--if the market has not sufficiently demanded it and if law has not, either--then this code will not provide it.”“For here's the obvious point: when government steps aside, it's not as if nothing takes its place. It's not as if private interests have no interests; as if private interests don't have ends that they will then pursue. To push the antigovernment button is not to teleport us to Eden. When the interests of government are gone, other interests take their place. Do we know what those interests are? And are we so certain they are anything better?

Our first response should be hesitation. It is proper to let the market develop first. But as the Constitution checks and limits what Congress does, so too should constitutional values check and limit what a market does. We should test both the laws of Congress and the product of a market against these values. We should interrogate the architecture of cyberspace as we interrogate the code of Congress.”[Note US perspective here; GRDP have probably superceded this]},
author = {Lessig, L},
journal = {Harvard Magazine},
keywords = {Consent (informed):,Cyber-physical systems,IoT,affordances,allocation of functions,architecture,certification:,competitive challenges,consequences,design process,ethics,federated control,harvard business school,interface design,legal,legislation,models,requirements,safety,security,socio-tech,software},
title = {{Code is law}},
url = {https://www.harvardmagazine.com/2000/01/code-is-law-html http://code-is-law.org},
year = {2000}
}
@article{Dombroskie2011,
abstract = {Despite being ecologically, economically, and scientifically important as well as a relatively well known group of insects, the order Lepidoptera can be difficult for non-experts to identify reliably. The matrix-based key presented here provides an easy and reliable way to identify the more difficult groups of adult Lepidoptera using a standard dissecting microscope. The key allows identification to the level of subfamily or tribe for most Canadian Lepidoptera, includes 222 taxa, and uses 73 characters with 266 character states. Taxon pages covering the diversity, diagnosis, and taxonomic references of each taxon accompany the identification key.},
author = {Dombroskie, Jason},
doi = {10.3752/cjai.2011.17},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dombroskie - 2011 - A Matrix Key to Families, Subfamilies and Tribes of Lepidoptera of Canada.pdf:pdf},
issn = {1911-2173},
journal = {Canadian Journal of Arthropod Identification},
number = {17},
title = {{A Matrix Key to Families, Subfamilies and Tribes of Lepidoptera of Canada}},
url = {https://cjai.biologicalsurvey.ca/d{\_}17/d{\_}17{\_}download.html},
year = {2011}
}
@inproceedings{Candes2017,
author = {Candes, Emmanuel},
booktitle = {Le¸ cons Jacques-Louis Lions},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Candes - 2017 - About the Replicability of Scientific Research in the Big Data Era What Statistics Can Offer.pdf:pdf},
title = {{About the Replicability of Scientific Research in the Big Data Era: What Statistics Can Offer?}},
url = {https://www.ljll.math.upmc.fr/IMG/pdf/ljll170317candes.e-14.5mo.pdf},
year = {2017}
}
@inproceedings{Lenarduzzi2016,
abstract = {Context: One of the most important steps of the Lean Startup methodology is the definition of Minimum Viable Product (MVP), needed to start the learning process by integrating the early adopters' feedbacks as soon as possible. Objective: This study aims at identifying the common definitions of MVP proposed and the key factors identified to help entrepreneurs efficiently define their MVP, reducing errors due to unconsidered unknown factors. Method: We identified the MVP definitions and key factors by means of a systematic mapping study, defining the research questions and the protocol to be used. We selected the bibliographic sources, the keywords, and the selection criteria for searching the relevant papers. Results: We found 97 articles and, through inclusion and exclusion criteria, removed 75 articles, which reduced the total to 22 at the end of the process. The results are a classification schema for characterizing the definition of Minimum Viable Product in Lean Startups and a set of common key factors identified in the MVP definitions. Conclusion: The identified key factors are related to technical characteristics of the product as well as market and customer aspects. We found a positive improvement of the state of the art of MVP and the definition of Minimum.},
author = {Lenarduzzi, Valentina and Taibi, Davide},
booktitle = {Proceedings - 42nd Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2016},
doi = {10.1109/SEAA.2016.56},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lenarduzzi, Taibi - 2016 - MVP Explained A Systematic Mapping Study on the Definitions of Minimal Viable Product.pdf:pdf},
isbn = {9781509028191},
issn = {2376-9505},
keywords = {Entrepreneurship,Lean Startup,Minimum Viable Product,Startup},
month = {aug},
pages = {112--119},
publisher = {IEEE},
title = {{MVP Explained: A Systematic Mapping Study on the Definitions of Minimal Viable Product}},
url = {http://ieeexplore.ieee.org/document/7592786/},
year = {2016}
}
@article{Merckx2014,
abstract = {* Light traps are used to make inferences about local macro-moth communities, but very little is known about the efficiency with which they attract moths from varying distances, and how this may differ among families. * We released 731 marked individuals, from three of the most common and species-rich macro-moth families, at several distances from low-wattage actinic light traps in open and woodland habitat. * Logistic regression showed family-specific sampling areas: erebids were attracted from up to 27 m, geometrids from up to 23 m, and noctuids from up to 10 m from the light source, with these distances corresponding to a 5{\%} recapture rate. Sampling size was also family-specific: a maximum of 55{\%} of erebids, 15{\%} of geometrids, and 10{\%} of noctuids were predicted to be trapped when flying near (0–1 m) light traps. * Our study demonstrates that weak light traps: (i) have remarkably local sampling ranges, resulting in samples that are highly representative of the local habitat, and (ii) attract small, and family-specific proportions of individuals within these ranges. * We suggest that the local sampling ranges of weak light traps make them excellent tools to monitor nocturnal macro-moth communities. As trap efficiency differs among macro-moth families, care must be taken in relating the abundance of the sample to absolute local abundance. Frequent sampling can provide adequate data on relative temporal change in the local macro-moth fauna, however.},
author = {Merckx, Thomas and Slade, Eleanor M.},
doi = {10.1111/icad.12068},
editor = {Basset, Yves and Christie, Fiona},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Merckx, Slade - 2014 - Macro-moth families differ in their attraction to light Implications for light-trap monitoring programmes.pdf:pdf},
isbn = {1752-4598},
issn = {17524598},
journal = {Insect Conservation and Diversity},
keywords = {Attraction range,Biodiversity monitoring,Erebidae,Geometridae,Lepidoptera,Mark-release-recapture,Moth sampling,Noctuidae,Species diversity,Standardised sampling},
month = {sep},
number = {5},
pages = {453--461},
title = {{Macro-moth families differ in their attraction to light: Implications for light-trap monitoring programmes}},
url = {http://doi.wiley.com/10.1111/icad.12068},
volume = {7},
year = {2014}
}
@article{Licence2017,
abstract = {This guidance explains how the Open Government Licence (OGL) works for users. The OGL is a simple set of terms and conditions under which information providers in the public sector can license the use and re-use of their information. Provided that you comply with the terms you have permission to use information anywhere in the world. The licence is also non-exclusive which means that you will not be the only person able to make use of it. The Open Government Licence enables you to use information for both commercial and non-commercial purposes. Using information under the OGL There is no need to register or formally apply for the OGL and no charges or fees are involved for using information under the licence. Users simply need to ensure that their use of information complies with the terms. What information can I use under the OGL? If you come across an information resource that says it is made available under an OGL, you are authorised by the information provider to use it consistent with those licence terms. Much of this information will be accessible on public sector web sites, online portals or printed publications. For more detailed information about the scope of Crown copyright and database rights offered under the OGL please see the UK Government Licensing Framework. Is there information I cannot use under the OGL? You cannot use information which has not been offered for use expressly under the OGL. You will need to contact the relevant rights owner of the information if no licence or re-use details are given. The OGL has been developed to allow the use of as much information as possible, however, there are certain types information it does not cover. These are:  information which includes personal data, for example, the names and addresses of individuals  information which has neither been published nor disclosed under information access legislation (including the Freedom of Information Acts for the UK and Scotland) by or with the consent of the information provider Last updated: March 2017  logos which identify a government departmental or a public sector organisation, Coats of Arms or Crests, and the Royal Arms , unless they form an integral part of a dataset or document (and are shown accurately in their context in that dataset or document  military insignia  third party rights the information provider is not authorised to license  information subject to other intellectual property rights, including patents, trademarks, and design rights  identity documents such as the British Passport. How can I use information licensed under the OGL?},
author = {Licence, Open Government},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Licence - 2017 - Open Government Licence Guidance for users.pdf:pdf},
journal = {The National Archives},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {1--3},
title = {{Open Government Licence Guidance for users}},
url = {http://www.nationalarchives.gov.uk/documents/information-management/ogl-information-provider-guidance.pdf},
year = {2017}
}
@inproceedings{Buckland2005,
abstract = {The need to monitor trends in biodiversity raises many technical issues. What are the features of a good biodiversity index? How should trends in abundance of individual species be estimated? How should composite indices, possibly spanning very diverse taxa, be formed? At what spatial scale should composite indices be applied? How might change-points - points at which the underlying trend changes - be identified? We address some of the technical issues underlying composite indices, including survey design, weighting of the constituent indices, identification of change-points and estimation of spatially varying time trends. We suggest some criteria that biodiversity measures for use in monitoring surveys should satisfy, and we discuss the problems of implementing rigorous methods. We illustrate the properties of different composite indices using UK farmland bird data. We conclude that no single index can capture all aspects of biodiversity change, but that a modified Shannon index and the geometric mean of relative abundance have useful properties. {\textcopyright} 2005 The Royal Society.},
author = {Buckland, S. T. and Magurran, A. E. and Green, R. E. and Fewster, R. M.},
booktitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
doi = {10.1098/rstb.2004.1589},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Buckland et al. - 2005 - Monitoring change in biodiversity through composite indices.pdf:pdf},
issn = {09628436},
keywords = {Biodiversity monitoring,Indices of trend,Shannon index,Simpson's index},
month = {feb},
number = {1454},
pages = {243--254},
publisher = {Royal Society},
title = {{Monitoring change in biodiversity through composite indices}},
volume = {360},
year = {2005}
}
@article{Bartolucci2018,
abstract = {We discuss how latent variable models are useful to deal with the complexities of big data from different perspectives: simplification of data structure; flexible representation of dependence between variables; reduction of selection bias. Problems involved in parameter estimation are also discussed.},
author = {Bartolucci, Francesco and Bacci, Silvia and Mira, Antonietta},
doi = {10.1016/j.spl.2018.02.023},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bartolucci, Bacci, Mira - 2018 - On the role of latent variable models in the era of big data.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Bayesian inference,Complex data,Maximum likelihood estimation,Parallel computing,Selection bias},
month = {may},
pages = {165--169},
publisher = {North-Holland},
title = {{On the role of latent variable models in the era of big data}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300683?via{\%}3Dihub},
volume = {136},
year = {2018}
}
@inproceedings{palmer2005amarket,
author = {Palmer, P J and Sarvar, F and Conway, P P and Jaggernauth, W A and Olseger, M},
booktitle = {Proceedings of the Japanese Society of Automotive Engineers Conference, JSAE 2005},
month = {may},
organization = {Tokyo, Japan},
pages = {25--27},
title = {{A Roadmap for an Efficient Urban Delivery System - A case study of a technical project in a rapidly developing market}},
year = {2005}
}
@misc{Microsoft2018,
abstract = {This article describes how you can digitally sign a macro project by using a certificate. If you don't already have a digital certificate, you must obtain one. To test macro projects on your own computer, you can create your own self-signing certificate by using the Selfcert.exe tool.},
author = {Microsoft},
booktitle = {Microsoft Official Support},
title = {{Digitally sign your macro project - Office Support}},
url = {https://support.office.com/en-us/article/Digitally-sign-your-macro-project-956E9CC8-BBF6-4365-8BFA-98505ECD1C01{\#}ID0EAABAAA=Newer{\_}Versions},
urldate = {2018-09-03},
year = {2018}
}
@article{Fish2016,
abstract = {A study of the cultural ecosystem services (CES) arising from peoples' interactions with the rural environment is conducted within the context of a landscape scale, ‘nature improvement' initiative in the United Kingdom. Taking a mixed methodological approach, the research applies, and demonstrates empirically, a framework for CES developed under the UK National Ecosystem Assessment (Fish et al., 2016). Applications of the framework involve the study of the ‘environmental spaces' and ‘cultural practices' that contribute to the realisation of benefits to well-being. In this paper empirical work is undertaken to inform the CES evidence base informing management priorities of the Northern Devon Nature Improvement Area (NDNIA) in south west England. Findings from a questionnaire survey, qualitative mapping, group discussion and a participatory arts-based research process are presented to document the many and diverse ways this study area matters to local communities. The paper analyses the qualities that research participants attribute to the environmental space of the NDNIA, the cultural practices conducted and enabled within it, and their associated benefits. The implications of the study for applying this framework through mixed methodological research are discussed, alongside an account of the impact of this approach within the NDNIA itself.},
author = {Fish, Robert and Church, Andrew and Willis, Cheryl and Winter, Michael and Tratalos, Jamie A. and Haines-Young, Roy and Potschin, Marion},
doi = {10.1016/j.ecoser.2016.09.017},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fish et al. - 2016 - Making space for cultural ecosystem services Insights from a study of the UK nature improvement initiative.pdf:pdf},
issn = {22120416},
journal = {Ecosystem Services},
keywords = {Cultural ecosystem services,Ecosystem assessment,Mixed-methodologies,Shared values},
month = {oct},
pages = {329--343},
publisher = {Elsevier B.V.},
title = {{Making space for cultural ecosystem services: Insights from a study of the UK nature improvement initiative}},
volume = {21},
year = {2016}
}
@misc{Annon,
author = {Annon},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Annon - Unknown - Mach3 USB Motion Controller Manual.pdf:pdf},
publisher = {NOVUSUN},
title = {{Mach3 USB Motion Controller Manual}},
url = {http://wwwnvcnc.net}
}
@misc{Quadient2018,
abstract = {Online datacleaner},
author = {Quadient},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{Quadient DataCleaner https://datacleaner.org/}},
url = {https://datacleaner.org/},
urldate = {2018-03-12},
year = {2018}
}
@article{Jahangirian2011,
abstract = {While literature reviews with a large-scale scope are nowadays becoming a staple element of modern research practice, there are many challenges in taking on such an endeavour, yet little evidence of previous studies addressing these challenges exists. This paper introduces a practical and efficient review framework for extremely large corpora of literature, refined by five parallel implementations within a multi-disciplinary project aiming to map out the research and practice landscape of modelling, simulation, and management methods, spanning a variety of sectors of application where such methods have made a significant impact. Centred on searching and screening techniques along with the use of some emerging IT-assisted analytic and visualisation tools, the proposed framework consists of four key methodological elements to deal with the scale of the reviews, namely: (a) an incremental and iterative review structure, (b) a 3-stage screening phase including filtering, sampling and sifting, (c) use of visualisation tools, and (d) reference chasing (both forward and backward). Five parallel implementations of systematically conducted literature search and screening yielded a total initial search result of 146 087 papers, ultimately narrowed down to a final set of 1383 papers which was manageable within the limited time and other constraints of this research work. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Jahangirian, Mohsen and Eldabi, Tillal and Garg, Lalit and Jun, Gyuchan T. and Naseer, Aisha and Patel, Brijesh and Stergioulas, Lampros and Young, Terry},
doi = {10.1016/j.ijinfomgt.2010.07.004},
isbn = {0268-4012},
issn = {02684012},
journal = {International Journal of Information Management},
keywords = {Management,Modelling,Simulation,Systematic literature review,Visualisation},
month = {jun},
number = {3},
pages = {234--243},
title = {{A rapid review method for extremely large corpora of literature: Applications to the domains of modelling, simulation, and management}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0268401210000976},
volume = {31},
year = {2011}
}
@article{Williams2005a,
abstract = {There has been much prescriptive work in project management, exemplified in various "Bodies of Knowledge". However, experience shows some projects overspending considerably. Recently, systemic modeling research into the behavior of large projects explains project oversponds by "systemic" effects and the (sometimes counterintuitive) effect of management actions. However, while this work is becoming more widely known, embedding the lessons in project-management practice is not straightforward. The current prescriptive dominant discourse of project management contains implicit underlying assumptions with which the systemic modeling work clashes, indeed showing how conventional methods can exacerbate rather than alleviate project problems. Exploration of this modeling suggests that for projects that are complex, uncertain, and time-limited, conventional methods might be inappropriate, and aspects of newer methodologies in which the project "emerges" rather than being fully preplanned might be more appropriate. Some of the current literature on project-classification schemes also suggests similar parameters, without the rationale that the systemic modeling provides, thus providing useful backup to this analysis. The eventual aim of this line of work is to enable project managers to choose effective ways to manage projects based on understanding and model-based theory.},
author = {Williams, Terry},
doi = {10.1109/TEM.2005.856572},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Williams - 2005 - Assessing and moving on from the dominant project management discourse in the light of project overruns.pdf:pdf},
issn = {00189391},
journal = {IEEE Transactions on Engineering Management},
keywords = {Project failures,Project management theory,Systemic modeling},
month = {nov},
number = {4},
pages = {497--508},
title = {{Assessing and moving on from the dominant project management discourse in the light of project overruns}},
url = {http://ieeexplore.ieee.org/document/1522702/},
volume = {52},
year = {2005}
}
@article{Morton1911,
author = {Morton, Kenneth and Stenton, Rupert and Miyake, T. and Nurse, C. and Lucas, W. and South, Richard and Newman, L. and Distant, W. and Wileman, A.},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 1911 - The Entomologist An Illustrated Journal of General Entomology No. 574.pdf:pdf},
journal = {The Entomologist},
keywords = {Entomology,Life Sciences},
number = {574},
pages = {81},
title = {{The Entomologist: An Illustrated Journal of General Entomology No. 574}},
url = {https://digitalcommons.usu.edu/bee{\_}lab{\_}co/290},
volume = {44},
year = {1911}
}
@inproceedings{Valiati2006,
abstract = {The design of multidimensional visualization techniques is based on the assumption that a graphical representation of a large dataset can give more insight to a user, by providing him/her a more intuitive support in the process of exploiting data. When developing a visualization technique, the analytic and exploratory tasks that a user might need or want to perform on the data should guide the choice of the visual and interaction metaphors implemented by the technique. Usability testing of visualization techniques also needs the definition of users' tasks. The identification and understanding of the nature of the users' tasks in the process of acquiring knowledge from visual representations of data is a recent branch in information visualization research. Some works have proposed taxonomies to organize tasks that a visualization technique should support. This paper proposes a taxonomy of visualization tasks, based on existing taxonomies as well as on the observation of users performing exploratory tasks in a multidimensional data set using two different visualization techniques, Parallel Coordinates and RadViz. Different scenarios involving low-level tasks were estimated for the completion of some high-level tasks, and they were compared to the scenarios observed during the users' experiments.},
author = {Valiati, Eliane R. A. and Pimenta, Marcelo S. and Freitas, Carla M. D. S.},
booktitle = {Proceedings of the 2006 AVI workshop on BEyond time and errors novel evaluation methods for information visualization - BELIV '06},
doi = {10.1145/1168149.1168169},
isbn = {1595935622},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {1},
title = {{A taxonomy of tasks for guiding the evaluation of multidimensional visualizations}},
url = {https://dl.acm.org/citation.cfm?id=1168169 http://portal.acm.org/citation.cfm?doid=1168149.1168169},
year = {2006}
}
@article{Wasserstein2016,
abstract = {Additional reading: http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
doi = {10.1080/00031305.2016.1154108},
eprint = {1011.1669},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wasserstein, Lazar - 2016 - The ASA's Statement on p-Values Context, Process, and Purpose.pdf:pdf},
isbn = {0003-1305 1537-2731},
issn = {0003-1305},
journal = {The American Statistician},
month = {apr},
number = {2},
pages = {129--133},
pmid = {25246403},
publisher = {Taylor {\&} Francis},
title = {{The ASA's Statement on p-Values: Context, Process, and Purpose}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108},
volume = {70},
year = {2016}
}
@article{williams1997technologyassembly,
author = {Williams, D J and Edwards, T C and Palmer, P J},
journal = {Journal of Electronics Manufacturing},
pages = {69--77},
title = {{Technology Trends in Electronics and Photonics and their Effect on Manufacturing and Assembly}},
volume = {7},
year = {1997}
}
@techreport{Sauerwein1996,
abstract = {Which products and services can be used to obtain a high level of customer satisfaction? Which product features have a more than proportional influence on satisfaction, and which attributes are an absolute must in the eyes of the customer? So far customer satisfaction was mostly seen as a one-dimensional construction-the higher the perceived product quality, the higher the customer's satisfaction and vice versa. But fulfilling the individual product requirements to a great extent does not necessarily imply a high level of customer satisfaction. It is also the type of requirement which defines the perceived product quality and thus customer satisfaction. Departing from Kano's model of customer satisfaction, a methodology is introduced which determines which influence the components of products and services have on customer satisfaction. The authors also demonstrate how the results of a customer survey can be interpreted and how conclusions can be drawn and used for the management of customer satisfaction is demonstrated. Kano's model of customer satisfaction In his model, Kano (Kano, 1984) distinguishes between three types of product requirements which influence customer satisfaction in different ways when met: Must-be requirements: If these requirements are not fulfilled, the customer will be extremely dissatisfied. On the other hand, as the customer takes these requirements for granted, their fulfillment will not increase his satisfaction. The must-be requirements are basic criteria of a product. Fulfilling the must-be requirements will only lead to a state of "not dissatisfied". The customer regards the must-be requirements as prerequisites, he takes them for granted and therefore does not explicitly},
author = {Sauerwein, Elmar and Bailom, Franz and Matzler, Kurt and Hinterhuber, Hans H},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sauerwein et al. - 1996 - Preprints Volume I of the IX. International Working Seminar on Production Economics.pdf:pdf},
pages = {313--327},
title = {{Preprints Volume I of the IX. International Working Seminar on Production Economics}},
url = {https://faculty.kfupm.edu.sa/CEM/bushait/CEM{\_}515-082/kano/kano-model2.pdf},
year = {1996}
}
@misc{Fiandrino2012,
abstract = {This package provides a way to easily draw diagrams in documents and presentations from a list of items thanks to TikZ. The idea cames out from this question on TeX.StackExchange.},
address = {Hei­del­berg, Germany},
author = {Fiandrino, Claudio},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fiandrino - 2012 - The smartdiagram package.pdf:pdf},
pages = {1--27},
publisher = {The Comprehensive TEX Archive Network},
title = {{The smartdiagram package}},
url = {http://mirror.las.iastate.edu/tex-archive/graphics/pgf/contrib/smartdiagram/smartdiagram.pdf},
year = {2012}
}
@misc{DepartmentforBusinessEnergyandIndustrialStrategy2018,
abstract = {The Free Company Data Product is a downloadable data snapshot containing basic company data of live companies on the register. This snapshot is provided as ZIP files containing data in CSV format and is split into multiple files for ease of downloading.},
author = {BEIS},
booktitle = {HM GOV website},
title = {{Companies House}},
url = {http://download.companieshouse.gov.uk/en{\_}output.html},
urldate = {2018-06-19},
year = {2018}
}
@techreport{EARTO-142014,
address = {Brussels},
annote = {Document inRoad2CPS},
author = {EARTO-14},
institution = {European Association of Research and Technology Organisations},
title = {{The TRL scale as a research {\&} innovation policy tool, EARTO recommendations}},
url = {http://www.earto.eu/fileadmin/content/03{\_}Publications/The{\_}TRL{\_}Scale{\_}as{\_}a{\_}R{\_}I{\_}Policy{\_}Tool{\_}-{\_}EARTO{\_}Recommendations{\_}-{\_}Final.pdf},
year = {2014}
}
@incollection{Chamberlain2013,
abstract = {The Journal Impact Factor (JIF; owned and published by Thomson Reuters) is a summation of the impact of all articles in a journal based on citations. Publishers have used the JIF to gain recognition, authors are evaluated by their peers based on the JIF of the journals they have published, and authors often choose where to publish based on the JIF.},
author = {Chamberlain, Scott},
booktitle = {Information Standards Quarterly (ISQ) Summer},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Chamberlain - 2013 - Consuming Article-Level Metrics Observations and Lessons from Comparing Aggregator Provider Data.pdf:pdf},
number = {2},
pages = {5--13},
title = {{Consuming Article-Level Metrics: Observations and Lessons from Comparing Aggregator Provider Data}},
url = {http://www.niso.org/sites/default/files/stories/2017-08/FE{\_}Chamberlain{\_}Consuming{\_}ALMs{\_}isq{\_}v25no2.pdf},
volume = {25},
year = {2013}
}
@techreport{Engelen2010a,
abstract = {Describes how to include an SVG image easily in LaTeX using Inkscape},
author = {Engelen, Jbc},
booktitle = {Tex User Group},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Engelen - 2010 - How to include an SVG image in LATEX(3).pdf:pdf},
institution = {Tex User Group},
title = {{How to include an SVG image in LATEX}},
url = {https://bugs. http://ftp2.ru.freebsd.org/.0/CTAN/info/svg-inkscape/InkscapePDFLaTeX.pdf http:/http://tug.ctan.org/info/svg-inkscape/InkscapePDFLaTeX.pdf},
year = {2010}
}
@misc{Jamshidi,
annote = {• Following chapters on SoS approaches and techniques, there are chapters authored by leading experts in the applications of net centricity, management, defense, e-business, infrastructures, wireless sensor networks, services, space exploration, electrical power systems, transportation, sustainability, robotic swarms, healthcare, and Global Earth Observation (GEO).},
author = {Jamshidi, M},
isbn = {978-0-470-19590-1},
keywords = {reference architectures,software,systems},
publisher = {J. Wiley {\&} Sons},
title = {{System of systems engineering - innovations for the 21st century}},
year = {2009}
}
@article{AsstProfessor2017,
abstract = {ata mining is the process of analyzing data from different views and summarizing it into useful data. " Data mining, also popularly referred to as knowledge discovery from data (KDD), is the automated or convenient extraction of patterns representing knowledge implicitly stored or captured in large databases, data warehouses, the Web, other massive information repositories or data streams. " . This paper provides a survey on various data mining techniques such as classification, clustering, regression, summarization and so on. This paper also discusses some of the data mining applications Keywords: knowledge discovery in data, data mining application, descriptive model, predictive model. I. INTRODUCTION Data mining, discovering of hidden predictive information from large data sets and it is a powerful new technology with great potential to help companies focus on the most important information in their data warehouses. Data mining (sometimes called data or knowledge discovery) is the process of analyzing data from different perspectives and summarizing it into useful information -information that can be used to increase revenue, cuts costs, or both. Data mining software is one of a number of analytical tools for analyzing data. It allows users to analyze data from many different dimensions or angles, categorize it, and summarize the relationships identified. Technically, data mining is the process of finding correlations or patterns among dozens of fields in large relational databases.},
author = {Deepashri, K.S. and Kamath, Ashwini},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Deepashri, Kamath - 2017 - Survey on Techniques of Data Mining and its Applications.pdf:pdf},
journal = {International Journal of Emerging Research in Management {\&}Technology},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {62},
pages = {2278--9359},
title = {{Survey on Techniques of Data Mining and its Applications}},
url = {https://pdfs.semanticscholar.org/b738/3df4705133a132f58104b514b80555fe78cb.pdf},
volume = {ISSN},
year = {2017}
}
@article{Han2014,
abstract = {Authors service information about how to choose which publication to write for and submission guidelines are available for all. Please visit www.emeraldinsight.com/authors for more information. About Emerald www.emeraldinsight.com Emerald is a global publisher linking research and practice to the benefit of society. The company manages a portfolio of more than 290 journals and over 2,350 books and book series volumes, as well as providing an extensive range of online products and additional customer resources and services. Abstract Purpose – The purpose of this paper is to address the challenge of opinion mining in text documents to perform further analysis such as community detection and consistency control. More specifically, we aim to identify and extract opinions from natural language documents and to represent them in a structured manner to identify communities of opinion holders based on their common opinions. Another goal is to rapidly identify similar or contradictory opinions on a target issued by different holders. Design/methodology/approach – For the opinion extraction problem we opted for a supervised approach focusing on the feature selection problem to improve our classification results. On the community detection problem, we rely on the Infomap community detection algorithm and the multi-scale community detection framework used on a graph representation based on the available opinions and social data. Findings – The classification performance in terms of precision and recall was significantly improved by adding a set of " meta-features " based on grouping rules of certain part of speech (POS) instead of the actual words. Concerning the evaluation of the community detection feature, we have used two quality metrics: the network modularity and the normalized mutual information (NMI). We evaluated seven one-target similarity functions and ten multi-target aggregation functions and concluded that linear functions perform poorly for data sets with multiple targets, while functions that calculate the average similarity have greater resilience to noise. Originality/value – Although our solution relies on existing approaches, we managed to adapt and integrate them in an efficient manner. Based on the initial experimental results obtained, we managed to integrate original enhancements to improve the performance of the obtained results. 1. Problem statement Since the availability of large amounts of data, both structured and unstructured, the meaningful analysis of data remains a challenge. We address in this paper the challenge of opinion mining in text documents to perform further analysis such as community detection and consistency control. More specifically, we aim to identify and extract opinions from natural language documents and to represent them in a structured manner to identify communities of opinion holders based on their common opinions. Another goal is to rapidly identify similar or contradictory opinions on an entity issued by different holders. In our approach, we consider documents written in English. Our proposed solution combines strategies from natural language processing (NLP) and machine learning to identify opinions expressed across documents. Our solution is tracking opinion shift over time, detecting individual change of opinion or deviations from the collective opinion. We considered an additional feature based on the extracted opinions, namely, opinion-driven},
author = {Han, Hao and Nakawatase, Hidekazu and Oyama, Keizo and Yamaguchi, Yutaro and Yamamoto, Shuhei and Satoh, Tetsuji and Dinsoreanu, Mihaela and Potolea, Rodica},
doi = {10.1108/IJWIS-10-2017-0067},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Han et al. - 2014 - Identification of Substructures in Complex Networks using Formal Concept Analysis.pdf:pdf},
journal = {International Journal of Web Information Systems},
keywords = {Classification,Community detection,Consistency control,Opinion mining,Taxonomy Paper type Research paper,Text mining},
number = {17},
pages = {324--342},
title = {{Identification of Substructures in Complex Networks using Formal Concept Analysis}},
url = {https://doi.org/10.1108/IJWIS-10-2017-0067 http://dx.doi.org/10.1108/IJWIS-04-2014-0016{\%}5Cnhttp://{\%}5Cnhttp://dx.doi.org/10.1108/IJWIS-04-2014-0014{\%}5Cnhttp://dx.doi.org/10.1108/IJWIS-04-2014-0015},
volume = {1011},
year = {2014}
}
@article{Gupta2018,
abstract = {The digital era has opened up new possibilities for data-driven research. This paper discusses big data challenges in environmental monitoring and reflects on the use of statistical methods in tackling these challenges for improving the quality of life in cities.},
author = {Gupta, Shivam and Mateu, Jorge and Degbelo, Auriol and Pebesma, Edzer},
doi = {10.1016/j.spl.2018.02.030},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gupta et al. - 2018 - Quality of life, big data and the power of statistics.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Air quality,Big data,Land use regression,Optimal location,Smart city},
month = {may},
pages = {101--104},
publisher = {North-Holland},
title = {{Quality of life, big data and the power of statistics}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300750?via{\%}3Dihub},
volume = {136},
year = {2018}
}
@misc{Org2015,
annote = {id: 1},
author = {Org, Open Source},
number = {9/22/2015},
title = {{The Open Source Definition | Open Source Initiative }},
url = {http://opensource.org/definition},
volume = {2015},
year = {2015}
}
@article{Vogel2017,
abstract = {Entomologists call it the windshield phenomenon. “If you talk to people, they have a gut feeling. They re- member how insects used to smash on your windscreen,” says Wolfgang W{\"{a}}gele, director of the Leibniz In- stitute for Animal Biodiversity in Bonn, Germany. Today, drivers spend less time scraping and scrubbing.},
author = {Vogel, Gretchen},
doi = {10.1126/science.356.6338.576},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vogel - 2017 - Where have all the insects gone.pdf:pdf},
journal = {Science},
number = {6338},
pages = {576--579},
title = {{Where have all the insects gone?}},
volume = {356},
year = {2017}
}
@article{Coyle1994,
abstract = {Futures research includes the problem of generating reasonably exhaustive and plausible scenarios for a given topic, a problem for which there are no truly satisfactory solutions. This article reviews and evaluates a method, field anomaly relaxation, first put forward some 20 years ago. The evaluation is in the context of an illustrative study of political developments in Europe. The research reconstructs and further develops the method and concludes that it has something to offer for scenario generation. Its weaknesses are identified and suggestions for further research are made. {\textcopyright} 1994.},
author = {Coyle, R. G. and Crawshay, R. and Sutton, L.},
doi = {10.1016/0016-3287(94)90088-4},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Coyle, Crawshay, Sutton - 1994 - Futures assessment by field anomaly relaxation. A review and appraisal.pdf:pdf},
isbn = {0016-3287},
issn = {00163287},
journal = {Futures},
month = {jan},
number = {1},
pages = {25--43},
publisher = {Pergamon},
title = {{Futures assessment by field anomaly relaxation. A review and appraisal}},
url = {https://www.sciencedirect.com/science/article/pii/0016328794900884},
volume = {26},
year = {1994}
}
@article{stokes2004frameworkstation,
annote = {The technology watch station describe in this paper has been the core of the web presence for the PRIME Faraday partnership and now the Electronics Enabled Products KTN. It is in part responsible for the success of these activities and the continued high level of funding it has attracted since it was launched in 1999.},
author = {Stokes, C A and Palmer, P J},
issn = {0950-4222},
journal = {Industry {\&} Higher Education},
month = {dec},
number = {(6)},
pages = {391--396},
title = {{Framework for a Technology-watch Relay Station}},
volume = {18},
year = {2004}
}
@article{Laaksonen2006,
abstract = {To get reliable estimates ofbiodiversity or relative population sizes, it is impor- tant to develop and properly test new survey tools in comparison with previous methods. Here, we introduce a new, effective bait-trap model, viz. the “Oulu” model, for Lepidoptera surveys and monitoring schemes. An extensive field ex- periment showed that the new bait-trap model captures more individuals and more species than the widely-used “Jalas” model, while the species richness and species composition of the total catches did not differ between the trap models. The differences between the trap models were consistent over time and habitats. We suggest that the “Oulu” model yields high catches because few individuals can escape from the trap. It is thus an effective tool to be used in Lepidoptera sur- veys and studies.},
author = {Laaksonen, Jesse and Laaksonen, Toni and It{\"{a}}mies, Juhani and Rytk{\"{o}}nen, Seppo and V{\"{a}}lim{\"{a}}ki, Panu},
doi = {10.33338/ef.84301},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Laaksonen et al. - 2006 - View of A new efficient bait-trap model for Lepidoptera surveys – the “Oulu” model.pdf:pdf},
journal = {Laaksonen, J., Laaksonen, T., It{\"{a}}mies, J., Rytk{\"{o}}nen, S. {\&} V{\"{a}}lim{\"{a}}ki, P. 2006: A new efficient bait-trap model for Lepidoptera surveys – the “Oulu” model. — Entomol. Fennica},
number = {2},
pages = {153--160},
title = {{View of A new efficient bait-trap model for Lepidoptera surveys – the “Oulu” model}},
url = {https://journal.fi/entomolfennica/article/view/84301/43383},
volume = {17},
year = {2006}
}
@article{Conrad2006,
abstract = {A fundamental problem in estimating biodiversity loss is that very little quantitative data are available for insects, which comprise more than two-thirds of terrestrial species. We present national population trends for a species-rich and ecologically diverse insect group: widespread and common macro-moths in Britain. Two-thirds of the 337 species studied have declined over the 35 yr study and 21{\%} (71) of the species declined {\textgreater}30{\%} 10 yr-1. If IUCN (World Conservation Union) criteria are applied at the national scale, these 71 species would be regarded as threatened. The declines are at least as great as those recently reported for British butterflies and exceed those of British birds and vascular plants. These results have important and worrying implications for species such as insectivorous birds and bats, and suggests as-yet undetected declines may be widespread among temperate-zone insects. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Conrad, Kelvin F. and Warren, Martin S. and Fox, Richard and Parsons, Mark S. and Woiwod, Ian P.},
doi = {10.1016/j.biocon.2006.04.020},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Conrad et al. - 2006 - Rapid declines of common, widespread British moths provide evidence of an insect biodiversity crisis.pdf:pdf},
isbn = {0006-3207},
issn = {00063207},
journal = {Biological Conservation},
keywords = {Abundance,Biodiversity,Lepidoptera,Occupancy,Population dynamics,Population trends},
month = {oct},
number = {3},
pages = {279--291},
pmid = {294},
publisher = {Elsevier},
title = {{Rapid declines of common, widespread British moths provide evidence of an insect biodiversity crisis}},
url = {https://www.sciencedirect.com/science/article/pii/S0006320706001777},
volume = {132},
year = {2006}
}
@article{Pearl1995,
abstract = {The primary aim of this paper is to show how graphical models can be used as a mathematical language for integrating statistical and subject-matter information. In particular , the paper develops a principled, nonparametric framework for causal inference, in which diagrams are queried to determine if the assumptions available are sufficient for identifying causal effects from nonexperimental data. If so the diagrams can be queried to produce mathematical expressions for causal effects in terms of observed distributions; otherwise, the diagrams can be queried to suggest additional observations or auxiliary experiments from which the desired inferences can be obtained.},
author = {Pearl, Judea},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 1995 - Causal diagrams for empirical research.pdf:pdf},
journal = {Biometrika},
keywords = {Graph model,Some key words: Causal inference,Structural equations,Treatment effect},
number = {4},
pages = {669--710},
title = {{Causal diagrams for empirical research}},
url = {https://academic.oup.com/biomet/article-abstract/82/4/669/251647},
volume = {82},
year = {1995}
}
@article{Alawamleh2011,
abstract = {Speedier network decision making together with shorter time to bring items to market together with lower network operating costs all result from enhanced knowledge sharing. In addition reuse of enterprise and network knowledge resulting from improved capture means that any risk of repeating earlier project work is limited, repetition of past mistakes is reduced. Decisions are made with greater awareness of any risks involved and therefore there is likely to be a reduction in costs arising from faulty decisions and failed collaborations. While there are many advantages attached to the use of virtual organisations (VOs) there are also challenges, including risks that have become apparent through undertaking a review of the literature. In total 13 sources of risk were found stemming from the network related risks in a VO, where the emphasis of the study was placed. This paper contains a thorough study that will identify these threats as well as gaining a sound understanding of them by examining them one by one as they have been identified by the literature and previous studies. Subsequently, their relative importance will be analysed through the use of interpretive structural modelling (ISM) using information gathered in a questionnaire. {\textcopyright} 2011 Taylor {\&} Francis.},
author = {Alawamleh, Mohammad and Popplewell, Keith},
doi = {10.1080/00207543.2010.519735},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Alawamleh, Popplewell - 2011 - Interpretive structural modelling of risk sources in a virtual organisation.pdf:pdf},
issn = {0020-7543},
journal = {International Journal of Production Research},
keywords = {ISM,SME,VO,interpretive structural modelling,risk,virtual organisation},
month = {oct},
number = {20},
pages = {6041--6063},
publisher = { Taylor {\&} Francis Group },
title = {{Interpretive structural modelling of risk sources in a virtual organisation}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00207543.2010.519735},
volume = {49},
year = {2011}
}
@article{Omar1999,
abstract = {The role of market information is critical to the success or failure of new product introduction. The challenge of bringing market information to design engineers throughout the design process is addressed, using an information modelling approach. This allows valuable quality function deployment (QFD) information to be captured alongside other product design information, within an object-oriented database. An environment that facilitates the collection, modification and evaluation of information for QFD analysis is provided. This integrated software environment allows valuable market information to be shared between multi-discipline project team members. Use of the decision support system is demonstrated through a case study.},
author = {Omar, A. R. and Harding, J. A. and Popplewell, K.},
doi = {10.1108/09576069910280440},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Omar, Harding, Popplewell - 1999 - Design for customer satisfaction an information modelling approach.pdf:pdf},
issn = {09576061},
journal = {Integrated Manufacturing Systems},
keywords = {Customer requirements,Design,Information modelling,Quality,Quality function deployment},
number = {3-4},
pages = {199--209},
publisher = {MCB Univ Press Ltd},
title = {{Design for customer satisfaction: an information modelling approach}},
volume = {10},
year = {1999}
}
@misc{Burn-Callander2015,
abstract = {Errors in company spreadsheets could be putting billions of pounds at risk, research has found. This is despite high-profile corporate catastrophes, such as the collapse of US energy giant Enron, ringing alarm bells about inaccurate accounts and financial spreadsheets more than a decade ago.},
address = {London, UK},
author = {Burn-Callander, Rebecca},
booktitle = {The Telegraph},
month = {apr},
pages = {1},
title = {{Stupid errors in spreadsheets could lead to Britain's next corporate disaster - Telegraph}},
url = {http://www.telegraph.co.uk/finance/newsbysector/banksandfinance/11518242/Stupid-errors-in-spreadsheets-could-lead-to-Britains-next-corporate-disaster.html},
year = {2015}
}
@article{Paakkonen2015,
abstract = {Many business cases exploiting big data have been realised in recent years; Twitter, LinkedIn, and Facebook are examples of companies in the social networking domain. Other big data use cases have focused on capturing of value from streaming of movies (Netflix), monitoring of network traffic, or improvement of processes in the manufacturing industry. Also, implementation architectures of the use cases have been published. However, conceptual work integrating the approaches into one coherent reference architecture has been limited. The contribution of this paper is technology independent reference architecture for big data systems, which is based on analysis of published implementation architectures of big data use cases. An additional contribution is classification of related implementation technologies and products/services, which is based on analysis of the published use cases and survey of related work. The reference architecture and associated classification are aimed for facilitating architecture design and selection of technologies or commercial solutions, when constructing big data systems.},
archivePrefix = {arXiv},
arxivId = {1312.7123},
author = {P{\"{a}}{\"{a}}kk{\"{o}}nen, Pekka and Pakkala, Daniel},
doi = {10.1016/j.bdr.2015.01.001},
eprint = {1312.7123},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/P{\"{a}}{\"{a}}kk{\"{o}}nen, Pakkala - 2015 - Reference Architecture and Classification of Technologies, Products and Services for Big Data Systems.pdf:pdf},
isbn = {978-1-4799-6023-1},
issn = {22145796},
journal = {Big Data Research},
keywords = {Big data,Classification,Literature survey,Reference architecture},
month = {dec},
number = {4},
pages = {166--186},
pmid = {76559233},
publisher = {Elsevier},
title = {{Reference Architecture and Classification of Technologies, Products and Services for Big Data Systems}},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000027?via{\%}3Dihub},
volume = {2},
year = {2015}
}
@inproceedings{Vassiliadis2002,
abstract = {Extraction-Transformation-Loading (ETL) tools are pieces of software responsible for the extraction of data from several sources, their cleansing, customization and insertion into a data warehouse. In this paper, we focus on the problem of the definition of ETL activities and provide formal foundations for their conceptual representation. The proposed conceptual model is (a) customized for the tracing of inter-attribute relationships and the respective ETL activities in the early stages of a data warehouse project; (b) enriched with a 'palette' of a set of frequently used ETL activities, like the assignment of surrogate keys, the check for null values, etc; and (c) constructed in a customizable and extensible manner, so that the designer can enrich it with his own re-occurring patterns for ETL activities.},
address = {New York, New York, USA},
author = {Vassiliadis, Panos and Simitsis, Alkis and Skiadopoulos, Spiros},
booktitle = {Proceedings of the 5th ACM international workshop on Data Warehousing and OLAP - DOLAP '02},
doi = {10.1145/583890.583893},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vassiliadis, Simitsis, Skiadopoulos - 2002 - Conceptual modeling for ETL processes.pdf:pdf},
isbn = {1581135904},
issn = {1035-6851},
keywords = {ETL,conceptual modeling,data warehousing},
pages = {14--21},
pmid = {11903432},
publisher = {ACM Press},
title = {{Conceptual modeling for ETL processes}},
url = {http://portal.acm.org/citation.cfm?doid=583890.583893},
year = {2002}
}
@article{webb2004electromagneticharness,
annote = {Results enabled project major industrial partner In2connect to offer EMC protection as part of product offering. No data in academic literature previously available. (In2connect, Long Eaton contact: J. Astill).},
author = {Webb, D P and Jaggernauth, W A and Cottrill, M C W and West, A A and Conway, P P and Palmer, P J},
doi = {10.1243/0954407041580102},
issn = {0954-4070},
journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
month = {jul},
number = {(7)},
pages = {667--673},
title = {{Electromagnetic Compatibility Performance of Large Area Flexible Printed Circuit Automotive Harness}},
url = {http://dx.doi.org/10.1243/0954407041580102},
volume = {218},
year = {2004}
}
@article{Gentleman2005,
abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has
remained relatively stagnant. Publication is largely done in the same manner today as it was fifty
years ago. Many journals have adopted electronic formats, however, their orientation and style is
little different from a printed document. The documents tend to be static and take little advantage
of computational resources that might be available. Recent work, Gentleman and Temple Lang
(2003), suggests a methodology and basic infrastructure that can be used to publish documents in
a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang (2003) propose a
paradigm where documents are mixtures of code and text. Such documents may be self-contained
or they may be a component of a compendium which provides the infrastructure needed to provide
access to data and supporting software. These documents, or compendiums, can be processed in a
number of different ways. One transformation will be to replace the code with its output – thereby
providing the familiar, but limited, static document. 
In this paper we apply these concepts
to a seminal paper in bioinformatics, namely The Molecular Classification of Cancer, Golub et al
(1999). The authors of that paper have generously provided data and other information that have
allowed us to largely reproduce their results. Rather than reproduce this paper exactly we demonstrate that such a reproduction is possible and instead concentrate on demonstrating the usefulness
of the compendium concept itself.},
author = {Gentleman, Robert},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Gentleman - 2005 - Reproducible Research A Bioinformatics Case Study.pdf:pdf},
journal = {Statistical Applications in Genetics and Molecular Biology},
number = {1},
pages = {25},
title = {{Reproducible Research: A Bioinformatics Case Study}},
url = {http://www.bepress.com/sagmb},
volume = {4},
year = {2005}
}
@article{Ritchey1991,
abstract = {───────────────────────────────── Abstract -This article deals with the foundations of analysis and synthesis as scientific methods, and especially with the requirements for the successful application of these methods. Although analysis and synthesis always go hand in hand – they complement one another – there are important situations in which one method can be regarded as more suitable than the other. This concerns the question of which method is most appropriate as the primary point of departure for the study of a given system or object of scientific inquiry.},
author = {Ritchey, Tom},
doi = {10.1002/sres.3850080402},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ritchey - 1991 - Analysis and synthesis On scientific method. Based on a study by Bernhard Riemann.pdf:pdf},
isbn = {1099-1735},
issn = {10991735},
journal = {Systems Research},
keywords = {applicability of methods,choice of method,methodology},
number = {4},
pages = {21--41},
pmid = {731},
publisher = {Thesis Publishers},
title = {{Analysis and synthesis: On scientific method. Based on a study by Bernhard Riemann}},
url = {http://www.swemorph.com/pdf/anaeng-r.pdf},
volume = {8},
year = {1991}
}
@article{Rittel1973,
abstract = {The search for scientific bases for confronting problems of social policy is bound to fail, because of the nature of these problems. They are "wicked" problems, whereas science has developed to deal with "tame" problems. Policy problems cannot be definitively described. Moreover, in a pluralistic society there is nothing like the undisputable public good; there is no objective definition of equity; policies that respond to social problems cannot be meaningfully correct or false; and it makes no sense to talk about "optinaal solutions" to social probIems unless severe qualifications are imposed first. Even worse, there are no "solutions" in the sense of definitive and objective answers.},
annote = {Paper in Autonomy!!

Horst Rittel and Melvin M. Webber formally described the concept of wicked problems in a 1973 treatise, contrasting "wicked" problems with relatively "tame," soluble problems in mathematics, chess, or puzzle solving ie the origin of the term ‘wicked problem

Starts by utlining the problems of categorising society and its multifarious needs. Note also: The term wicked problem was introduced by Horst W J Rittel in a 1967 lecture, and subsequently elaborated more fully in collaboration with M W Webber in their Dilemmas in a General Theory of Planning, Policy Sciences, Volume 4, 1973, pages 155-169.

The Ten Rules of Wicked Problems
-There is no definitive formulation of a wicked problem
-Wicked problems have no stopping rule
-Solutions to wicked problems are not true-or-false, but better-or-worse
-There is no immediate and no ultimate test of a solution to a wicked problem (because of complex feedback loops)
-Every solution to a wicked problem is a "one-shot operation"; because there is no
opportunity to learn by trial-and-error, every attempt counts significantly
-Wicked problems do not have an enumerable (or an exhaustively describable) set of
potential solutions, nor is there a well-described set of permissible operations that may be
incorporated into the plan
-Every wicked problem is essentially unique
-Every wicked problem can be considered to be a symptom of another problem
-The existence of a discrepancy in representing a wicked problem can be explained in
numerous ways. The choice of explanation determines the nature of the problem's
resolution
-The planner (designer) has no right to be wrong (because the attempted solutions are in the real world, with consequences)},
author = {Rittel, H W J and Webber, M M},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Rittel, Webber - 1973 - Dilemmas in a general theory of planning.pdf:pdf},
journal = {Policy Sciences},
keywords = {Governance,HRM,STS,accounting practices,agility,blackboards,business process re-engineering,competitive challenges,complexity,consequences,dashboard/battlespace,decision-making,definitions,errors,ethics,federated control,harvard business school,holons,knowledge,maintenance,measurement,mental models,methods,metrics/metrication,network-centric warfare,organisational design,philosophy,production control,projects,prototyping,quotes,resilience,robots,safety,security,simulation,situation awareness,socio-tech,strategic planning,strategy,supply chain,theory,trust,wisdom},
pages = {155--169},
title = {{Dilemmas in a general theory of planning}},
volume = {4},
year = {1973}
}
@book{Bowker2013,
abstract = {Episodes in the history of data, from early modern math problems to today's inescapable "dataveillance," that demonstrate the dependence of data on culture. We live in the era of Big Data, with storage and transmission capacity measured not just in terabytes but in petabytes (where peta- denotes a quadrillion, or a thousand trillion). Data collection is constant and even insidious, with every click and every "like" stored somewhere for something. This book reminds us that data is anything but "raw," that we shouldn't think of data as a natural resource but as a cultural one that needs to be generated, protected, and interpreted. The book's essays describe eight episodes in the history of data from the predigital to the digital. Together they address such issues as the ways that different kinds of data and different domains of inquiry are mutually defining; how data are variously "cooked" in the processes of their collection and use; and conflicts over what can-or can't-be "reduced" to data. Contributors discuss the intellectual history of data as a concept; describe early financial modeling and some unusual sources for astronomical data; discover the prehistory of the database in newspaper clippings and index cards; and consider contemporary "dataveillance" of our online habits as well as the complexity of scientific data curation.},
address = {London, UK},
author = {Bowker, Geoffrey C. and Brine, Kevin R. and Garvey, Ellen Gruber and Gitelman, Lisa and Jackson, Steven J. and Virginia, Jackson and Krajewski, Markus and Poovey, Mary and Raley, Rita and Ribes, David and Rosenberg, Daniel and Stanley, Matthew and Williams, Travis D.},
isbn = {978-0262518284},
pages = {208},
publisher = {MIT Press},
title = {{‘Raw data' is an oxymoron(Infrastructures)}},
year = {2013}
}
@article{Yi2007,
author = {Yi, Ji Soo and ah Kang, Youn and Stasko, John},
doi = {10.1109/TVCG.2007.70515},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Yi, Kang, Stasko - 2007 - Toward a Deeper Understanding of the Role of Interaction in Information Visualization.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {nov},
number = {6},
pages = {1224--1231},
title = {{Toward a Deeper Understanding of the Role of Interaction in Information Visualization}},
url = {http://ieeexplore.ieee.org/document/4376144/},
volume = {13},
year = {2007}
}
@article{Slater2016,
abstract = {In recent years, a wide array of tools have emerged for the purposes of conducting educational data mining (EDM) and/or learning analytics (LA) research. In this article, we hope to highlight some of the most widely used, most accessible, and most powerful tools available for the researcher interested in conducting EDM/LA research. We will highlight the utility that these tools have with respect to common data preprocessing and analysis steps in a typical research project as well as more descriptive information such as price point and user-friendliness. We will also highlight niche tools in the field, such as those used for Bayesian knowledge tracing (BKT), data visualization, text analysis, and social network analysis. Finally, we will discuss the importance of familiarizing oneself with multiple tools--a data analysis toolbox--for the practice of EDM/LA research.},
annote = {educational data mining (EDM).
Data mining goes back to 1977.
Reviews 40 tools - a non exhaustive list.},
author = {Slater, Stefan and Joksimovic, S. and Kovanovic, Vitomir and Baker, Ryan S. and Gasevic, Dragan},
doi = {10.3102/1076998616666808},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Slater et al. - 2016 - Tools for Educational Data Mining A Review.pdf:pdf},
isbn = {1076-9986},
issn = {1076-9986},
journal = {Journal of Educational and Behavioral Statistics},
keywords = {big data,data analysis,data cleaning,data management,modeling,software,text mining},
month = {feb},
number = {1},
pages = {85--106},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Tools for Educational Data Mining: A Review}},
url = {http://journals.sagepub.com/doi/10.3102/1076998616666808 http://jeb.sagepub.com/cgi/doi/10.3102/1076998616666808},
volume = {42},
year = {2016}
}
@article{Legagneux2018,
abstract = {Scientists, policy makers, and journalists are three key, interconnected players involved in prioritizing and implementing solutions to mitigate the consequences of anthropogenic pressures on the environment. The way in which information is framed and expertise is communicated by the media is crucial for political decisions and for the integrated management of environmental issues. Here we present a comparative study of scientific literature and press articles addressing climate change and biodiversity. We extensively scrutinized the scientific literature, research funding, and press articles from the USA, Canada, and United Kingdom addressing climate change and biodiversity issues between 1991 and 2016. We found that media coverage of climate change was up to eight times higher compared to biodiversity. This discrepancy could not be explained by different scientific output between the two issues. Moreover, climate change media coverage was often related to specific events whereas no such indication of a connection was found in the case of biodiversity. An international communication strategy is urgently required to raise public awareness on biodiversity issues.We discussed several initiatives that scientists could undertake to better communicatemajor discoveries to the public and policy makers.},
author = {Casajus, Nicolas and Chevrinais, Marion and Ropars, Pascale and Gu{\'{e}}ry, Lorelei and Naud, Marie-Jos{\'{e}} and B{\^{e}}ty, Joel and Chevallier, Cl{\'{e}}ment and Cazelles, Kevin and Berteaux, Dominique and Gravel, Dominique and Jacquet, Claire and Jaffr{\'{e}}, Mikael and Noisette, Fanny and Legagneux, Pierre and Vissault, Steve and Archambault, Philippe},
doi = {10.3389/fevo.2017.00175},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Casajus et al. - 2018 - Our House Is Burning Discrepancy in Climate Change vs. Biodiversity Coverage in the Media as Compared to Scienti.pdf:pdf},
isbn = {2296-701X},
issn = {2296-701X},
journal = {Frontiers in Ecology and Evolution},
keywords = {Biodiversity,Climate Change,Media coverage,Public Opinion,Science Communication,biodiversity loss,research financing},
month = {jan},
pages = {175},
publisher = {Frontiers},
title = {{Our House Is Burning: Discrepancy in Climate Change vs. Biodiversity Coverage in the Media as Compared to Scientific Literature}},
url = {http://journal.frontiersin.org/article/10.3389/fevo.2017.00175/full},
volume = {5},
year = {2018}
}
@article{Golumbic2017,
abstract = {Increased interest in public engagement with science worldwide has resulted in the growth of funding opportunities for scientists in the rapidly expanding field of citizen science. This paper describes a case study based on interviews and observations, including a six-month field diary, of ten scientists who engaged in a citizen science project to receive funding for their scientific research. It examines how these scientists perceived their commitment to the public, and it explores relationships between the ways that citizen science is defined and presented in the literature and the ideas that scientists in this project have about citizen science. The findings indicate that these scientists were motivated mostly by their interest in promoting scientific research and obtaining prestigious funding. Many of the scientists also found it difficult to accept the idea that the public can make actual contributions to science. Although the scientists acknowledged the advantages and benefits of citizen participation for the public, they had no desire to actively engage with the public and would rather conduct a traditional study without the public's involvement. Exposing scientists to public engagement and citizen science concepts, especially at early stages of their scientific carrier, could help overcome barriers and encourage scientists to further engage the public in such initiatives.},
author = {Golumbic, Yaela N. and Orr, Daniela and Baram-Tsabari, Ayelet and Fishbain, Barak},
doi = {10.5334/cstp.53},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Golumbic et al. - 2017 - Between Vision and Reality A Study of Scientists' Views on Citizen Science.pdf:pdf},
issn = {2057-4991},
journal = {Citizen Science: Theory and Practice},
keywords = {Citizen science,public engagement with science,public understanding of science,qualitative research,science communication},
month = {oct},
number = {1},
pages = {6},
publisher = {Ubiquity Press, Ltd.},
title = {{Between Vision and Reality: A Study of Scientists' Views on Citizen Science}},
url = {https://theoryandpractice.citizenscienceassociation.org/article/10.5334/cstp.53/},
volume = {2},
year = {2017}
}
@inproceedings{Abedjan2017,
abstract = {If we just have a bunch of data sets in a repository, it is unlikely anyone will ever be able to find, let alone reuse, any of this data. With adequate metadata, there is some hope, but even so, challenges will remain..},
address = {Chicago},
author = {Abedjan, Z and Golab, L and of Data, F Naumann - Conference on Management and 2017, Undefined},
booktitle = {SIGMOD 2017},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Abedjan et al. - 2017 - Data profiling A tutorial.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {122},
title = {{Data profiling: A tutorial}},
url = {https://dl.acm.org/citation.cfm?id=3054772},
year = {2017}
}
@techreport{LRWTvolunteer2019,
abstract = {The Trust acknowledges and greatly appreciates the dedication and hard work of all its volunteers who contribute to the work of the Trust with a wealth of different skills and experience. People volunteer for many different reasons but whatever the motivation, both the volunteer and the Trust should benefit. The partnership between the Trust and its volunteers has enabled us to advance nature conservation and awareness in the counties and we are committed to continuing this vital work, which is essential in achieving our aims for the future.},
address = {Leicester},
author = {{Leicestershire and Rutland Wildlife Trust}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Leicestershire and Rutland Wildlife Trust - 2019 - LRWT Volunteer Guidelines.pdf:pdf},
institution = {LRWT},
pages = {7},
title = {{LRWT Volunteer Guidelines}},
year = {2019}
}
@article{Jawaheri2018,
abstract = {With the rapid increase of threats on the Internet, people are contin-uously seeking privacy and anonymity. Services such as Bitcoin and Tor were introduced to provide anonymity for online transactions and Web browsing. Due to its pseudonymity model, Bitcoin lacks retroactive operational security, which means historical pieces of information could be used to identify a certain user. We investigate the feasibility of deanonymizing users of Tor hidden services who rely on Bitcoin as a payment method by exploiting public informa-tion leaked from online social networks, the Blockchain, and onion websites. This, for example, allows an adversary to link a user with @alice Twitter address to a Tor hidden service with private.onion address by finding at least one past transaction in the Blockchain that involves their publicly declared Bitcoin addresses. To demonstrate the feasibility of this deanonymization attack, we carried out a real-world experiment simulating a passive, lim-ited adversary. We crawled 1.5K hidden services and collected 88 unique Bitcoin addresses. We then crawled 5B tweets and 1M Bit-coinTalk forum pages and collected 4.2K and 41K unique Bitcoin addresses, respectively. Each user address was associated with an online identity along with its public profile information. By ana-lyzing the transactions in the Blockchain, we were able to link 125 unique users to 20 Tor hidden services, including sensitive ones, such as The Pirate Bay and Silk Road. We also analyzed two case studies in detail to demonstrate the implications of the resulting information leakage on user anonymity. In particular, we confirm that Bitcoin addresses should always be considered exploitable, as they can be used to deanonymize users retroactively. This is espe-cially important for Tor hidden service users who actively seek and expect privacy and anonymity.},
archivePrefix = {arXiv},
arxivId = {1801.07501},
author = {Jawaheri, Husam Al and Sabah, Mashael Al and Boshmaf, Yazan and Erbad, Aimen},
eprint = {1801.07501},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jawaheri et al. - 2018 - When A Small Leak Sinks A Great Ship Deanonymizing Tor Hidden Service Users Through Bitcoin Transactions Analys.pdf:pdf},
keywords = {Attack,Bitcoin,Deanonymization,Privacy,Tor Hidden Services},
month = {jan},
title = {{When A Small Leak Sinks A Great Ship: Deanonymizing Tor Hidden Service Users Through Bitcoin Transactions Analysis}},
url = {http://arxiv.org/abs/1801.07501 https://arxiv.org/pdf/1801.07501.pdf},
year = {2018}
}
@inproceedings{Embury2014,
abstract = {Current linked open data standards have encouraged the publication of a large number of data sets on the public Web. While some data providers put a lot of energy and resources into maintaining high quality data, others do not, meaning that the quality of the data in many LOD sources is variable and unpredictable. This makes the construction of novel applications on top of the data more difficult and expensive than it otherwise would be. However, these same data standards also open up possibilities for new ways of managing information quality (IQ). In this paper, we propose one such approach, the IQ-bot, and present the results of our study of its feasibility. An IQ-bot is a 3rd party component that crawls the Web of data, looking for changes that have been made to data sets, and inferring from them where a correction to a data defect has been made. These corrections can then potentially be made available for application to other databases showing evidence of the presence of the same data defect. In this way, the benefits of the curation effort put into a small number of data sets can be propagated throughout the Web of data. Copyright is held by the author/owner(s).},
author = {Embury, Suzanne M and Jin, Binling and Sampaio, Sandra and Eleftheriou, Iliada},
booktitle = {CEUR Workshop Proceedings},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Embury et al. - 2014 - On the feasibility of crawling linked data sets for reusable defect corrections.pdf:pdf},
issn = {16130073},
pages = {8},
title = {{On the feasibility of crawling linked data sets for reusable defect corrections}},
url = {www.trilliumsoftware.com/home/products/TSDiscovery},
volume = {1215},
year = {2014}
}
@inproceedings{Shneiderman2008,
abstract = {Database searches are usually performed with query languages and form fill in templates, with results displayed in tabular lists. However, excitement is building around dynamic queries sliders and other graphical selectors for query specification, with results displayed by information visualization techniques. These filtering techniques have proven to be effective for many tasks in which visual presentations enable discovery of relationships, clusters, outliers, gaps, and other patterns. Scaling visual presentations from millions to billions of records will require collaborative research efforts in information visualization and database management to enable rapid aggregation, meaningful coordinated windows, and effective summary graphics. This paper describes current and proposed solutions (atomic, aggregated, and density plots) that facilitate sense-making for interactive visual exploration of billion record data sets.},
author = {Shneiderman, Ben},
booktitle = {Proceedings of the 2008 ACM SIGMOD international conference on Management of data - SIGMOD '08},
doi = {10.1145/1376616.1376618},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shneiderman - 2008 - Extreme visualization.pdf:pdf},
isbn = {9781605581026},
issn = {07308078},
keywords = {ACM Classification Keywords H5 Information interfa,HCI) H2 DATABASE MANAGEMENT General Terms: Human F,aggregation,coordinated windows,database search,density plots,dynamic queries,user interface},
pages = {3--12},
pmid = {17080791},
title = {{Extreme visualization}},
url = {https://www.cs.umd.edu/{~}ben/papers/Shneiderman2008Extreme.pdf http://portal.acm.org/citation.cfm?doid=1376616.1376618},
year = {2008}
}
@inproceedings{Chung2013,
abstract = {the idea of this paper is to use LINQ, a language, to write domain specific HW accelerator, which can run in FPGA in cores. We present LINQits, a exible hardware template that can be mapped onto programmable logic or ASICs in a heteroge- neous system-on-chip for a mobile device or server. Unlike xed-function accelerators, LINQits accelerates a domain- specic query language called LINQ. LINQits does not pro- vide coverage for all possible applications|however, existing applications (re-)written with LINQ in mind benet exten- sively from hardware acceleration. Furthermore, the LIN- Qits framework o ers a graceful and transparent migration path from software to hardware. LINQits is prototyped on a 2W heterogeneous SoC called the ZYNQ processor, which combines dual ARM A9 pro- cessors with an FPGA on a single die in 28nm silicon tech- nology. Our physical measurements show that LINQits im- proves energy e ciency by 8.9 to 30.6 times and performance by 10.7 to 38.1 times compared to optimized, multithreaded C programs running on conventional ARM A9 processors. Due to stringent energy demands coupled with the recent failure of Den- nard Scaling [10, 18, 14, 42, 9], today's client devices are highly constrained in their capabilities|with many emerg- ing, disruptive applications out of immediate reach, such as augmented reality, continous speech recognition, interactive personal agents (e.g., $\backslash$Watson" on a phone), and novel user experiences. We envision a deployment scenario where application-specic accelerators for LINQ queries are generated automatically in a centrally-managed repository (e.g., app store). The generated accelerators can either be mined automatically and fabricated as a general ASIC that supports many applications or targeted on a per- application basis to an FPGA.},
annote = {Presents the idea of custom FPGA Hardware for Dat queries.},
author = {Chung, Eric S and Davis, John D and Lee, Jaewon},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture - ISCA '13},
doi = {10.1145/2485922.2485945},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Chung, Davis, Lee - 2013 - LINQits Big Data on Little Clients.pdf:pdf},
isbn = {9781450320795},
issn = {01635964},
keywords = {ASIC,C13 [Processor Architectures]: Adaptable architec-,FPGA,Performance Keywords Co-processor accelerator,big data,data-flow architectures,database,high-level language architec-tures General Terms D,mobile,query language},
pages = {261--272},
title = {{LINQits: Big Data on Little Clients}},
url = {http://delivery.acm.org/10.1145/2490000/2485945/p261-chung.pdf?ip=158.125.51.236{\&}id=2485945{\&}acc=ACTIVE SERVICE{\&}key=BF07A2EE685417C5.F25E547D993D41C5.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1531171063{\_}33f87bae69fbe0a1be219bab416e2382 http://dl.acm.org/ci},
year = {2013}
}
@article{Powney2015,
abstract = {Biological records are one of the most important sources of data for a large number of research areas. For example, their application has made valuable contributions to climate change ecology, where they are used to monitor species range shifts; to conservation ecology, where they are used to assess species' Red List status; and to biogeography, where they are used to highlight hotspots of biodiversity. A major benefit of biological records is the large spatial extent of the coverage combined with the fine spatial precision of the data: this combination is essential for any ecologist hoping to address large-scale questions about biodiversity and environmental change. Because most biological records are collected by a vast pool of volunteer recorders, studies utilizing biological records have the advantage of large-scale long-term data that it would otherwise be unfeasibly expensive to collect. We review the application of biological records by focussing on four key areas of biodiversity research: biogeography, trend assessments, climate change ecology, and conservation biology. We showcase the diversity of insights that biological records have delivered, which in turn illustrates the contribution of the voluntary recording community to our understanding of biodiversity science.},
author = {Powney, Gary D. and Isaac, Nick J.B.},
doi = {10.1111/bij.12517},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Powney, Isaac - 2015 - Beyond maps A review of the applications of biological records.pdf:pdf},
isbn = {1095-8312},
issn = {10958312},
journal = {Biological Journal of the Linnean Society},
keywords = {Biodiversity,Citizen science,Climate change,Distribution change,Range shift,Richness,Species trends},
month = {jul},
number = {3},
pages = {532--542},
publisher = {Oxford University Press},
title = {{Beyond maps: A review of the applications of biological records}},
url = {https://academic.oup.com/biolinnean/article-lookup/doi/10.1111/bij.12517},
volume = {115},
year = {2015}
}
@misc{Broman2015a,
abstract = {R packages are the best way to distribute R code and documentation, and, despite the impression that the official manual (Writing R Extensions) might give, they really are quite simple to create. You should make an R package even for code that you don't plan to distribute. You'll find it is easier to keep track of your own personal R functions if they are in a package. And it's good to write documentation, even if it's just for your future self.},
author = {Broman, Karl},
title = {{R package primer: a minimal tutorial}},
url = {http://kbroman.org/pkg{\_}primer/},
year = {2015}
}
@article{Goddard2010,
abstract = {As urbanisation increases globally and the natural environment becomes increasingly fragmented, the importance of urban green spaces for biodiversity conservation grows. In many countries, private gardens are a major component of urban green space and can provide considerable biodiversity benefits. Gardens and adjacent habitats form interconnected networks and a landscape ecology framework is necessary to understand the relationship between the spatial configuration of garden patches and their constituent biodiversity. A scale-dependent tension is apparent in garden management, whereby the individual garden is much smaller than the unit of management needed to retain viable populations. To overcome this, here we suggest mechanisms for encouraging 'wildlife-friendly' management of collections of gardens across scales from the neighbourhood to the city. },
address = {Institute for Integrative and Comparative Biology, University of Leeds, Leeds, UK, LS2 9JT. bsmag@leeds.ac.uk},
annote = {id: 7; LR: 20110503; CI: Copyright 2009; JID: 8805125; CIN: Trends Ecol Evol. 2010 Apr;25(4):201-2; author reply 202-3. PMID: 20083324; RF: 107; 2009/01/07 [received]; 2009/07/10 [revised]; 2009/07/15 [accepted]; 2009/09/14 [aheadofprint]; ppublish},
author = {Goddard, M A and Dougill, A J and Benton, T G},
doi = {10.1016/j.tree.2009.07.016 [doi]},
issn = {0169-5347; 0169-5347},
journal = {Trends in ecology {\&} evolution},
keywords = {Animals,Biodiversity,City Planning,Conservation of Natural Resources/methods,Plants/classification/genetics,Urbanization/trends},
month = {feb},
number = {2},
pages = {90--98},
publisher = {Elsevier Ltd},
title = {{Scaling up from gardens: biodiversity conservation in urban environments }},
volume = {25},
year = {2010}
}
@article{Editorial2018,
abstract = {I n the space of seven weeks, Cambridge Analytica went from being the poster child for smart, data-driven electioneering to a pariah. In early May, the firm that once promised to “find your voters and move them to action” announced it was starting bankruptcy proceedings, as the “siege of media coverage has driven away virtually all of the Company's customers and suppliers” (bit.ly/2r1wwj1). The},
author = {Editorial},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Editorial - 2018 - What can we learn from the Facebook–Cambridge Analytica scandal.pdf:pdf},
journal = {Significance, The Royal Statistical Society},
pages = {4},
title = {{What can we learn from the Facebook–Cambridge Analytica scandal?}},
url = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2018.01139.x},
year = {2018}
}
@inproceedings{Rajkumar2010,
author = {Rajkumar, Ragunathan Raj and Lee, Insup and Sha, Lui and Stankovic, John},
booktitle = {Proceedings of the 47th Design Automation Conference},
pages = {731--736},
publisher = {ACM},
title = {{Cyber-physical systems: the next computing revolution}},
year = {2010}
}
@article{Leeper2014,
author = {{Leeper, Thomas}, J.},
doi = {10.32614/RJ-2014-015},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Leeper, Thomas - 2014 - Archiving Reproducible Research with R and Dataverse.pdf:pdf},
issn = {2073-4859},
journal = {The R Journal},
number = {1},
pages = {151},
title = {{Archiving Reproducible Research with R and Dataverse}},
url = {https://journal.r-project.org/archive/2014/RJ-2014-015/index.html},
volume = {6},
year = {2014}
}
@article{Abramo2011a,
author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea and {Di Costa}, Flavia},
doi = {10.1007/s11192-011-0459-x},
issn = {0138-9130},
journal = {Scientometrics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {dec},
number = {3},
pages = {929--941},
title = {{National research assessment exercises: a comparison of peer review and bibliometrics rankings}},
url = {http://link.springer.com/10.1007/s11192-011-0459-x},
volume = {89},
year = {2011}
}
@book{Stodden2014,
abstract = {In computational science, reproducibility requires that researchers make code and data available to others so that the data can be analyzed in a similar manner as in the original publication. Code must be available to be distributed, data must be accessible in a readable format, and a platform must be available for widely distributing the data and code. In addition, both data and code need to be licensed permissively enough so that others can reproduce the work without a substantial legal burden. Implementing Reproducible Research covers many of the elements necessary for conducting and distrib. Part 1. Tools -- part 2. Practices and guidelines -- part 3. Platforms.},
author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
isbn = {1466561599},
pages = {448},
publisher = {CRC Press/Taylor and Francis},
title = {{Implementing reproducible research}},
year = {2014}
}
@article{Keim2001,
abstract = {Computer systems today store vast amounts of data. Researchers, including those working on the “How Much In the rising tide of business Information?” project at the University of California, Berkeley, recently esti- transaction data, these tools help mated, about 1 exabyte (1 million ter- distinguish which are strategic abytes) of data is generated annually assets and which are not worth worldwide, including 99.997{\%} avail- collecting in the first place. able only in digital form. This world- wide data deluge means that in the next three years, more data will be generated Daniel A. Keim than during all previous human history.},
author = {Keim, Daniel A.},
doi = {10.1145/381641.381656},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Keim - 2001 - Visual exploration of large data sets.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {8},
pages = {38--44},
title = {{Visual exploration of large data sets}},
url = {https://dl.acm.org/citation.cfm?id=381656 http://portal.acm.org/citation.cfm?doid=381641.381656},
volume = {44},
year = {2001}
}
@techreport{MolnarP.&Gill2018,
abstract = {This report focuses on the impacts of automated decision-making in Canada's immigration and refugee system from a human rights perspective. It highlights how the use of algorithmic and automated technologies to replace or augment administrative decision-making in this context threatens to create a laboratory for high-risk experiments within an already highly discretionary system. Vulnerable and under-resourced communities such as non-citizens often have access to less robust human rights protections and fewer resources with which to defend those rights. Adopting these technologies in an irresponsible manner may only serve to exacerbate these disparities. In fact, the federal government already has been in the process of developing a system of “predictive analytics” to automate certain activities currently conducted by immigration officials and to support the evaluation of some immigrant and visitor applications. The government has also quietly sought input from the private sector related to a 2018 pilot project for an “Artificial Intelligence Solution” in immigration decision-making and assessments, including in Humanitarian and Compassionate applications and Pre-Removal Risk Assessments. The ramifications of using automated decision-making in the immigration and refugee space are far-reaching. Hundreds of thousands of people enter Canada every year through a variety of applications for temporary and permanent status. The nuanced and complex nature of many refugee and immigration claims may be lost on these technologies, leading to serious breaches of internationally and domestically protected human rights, in the form of bias, discrimination, privacy breaches, due process and procedural fairness issues, among others. This report surveys some of the current and proposed uses of automated decision-making in Canada's immigration and refugee system. Next, it provides an overview of the various levels of decision-making across the full lifecycle of the immigration and refugee process to illustrate how these decisions may be affected by new technologies. The report then develops a human rights analysis of the use of automated decision systems from a domestic and international perspective. These technologies in the immigration and refugee system also raise crucial constitutional and administrative law issues, including matters of procedural fairness and standard of review. Finally, the report documents a number of other systemic policy challenges related to the adoption of these technologies—including those concerning access to justice, public confidence in the legal system, private sector accountability, technical capacity within government, and other global impacts.},
author = {{Molnar, P. {\&} Gill}, L.},
booktitle = {International Human Rights Program {\&} Citizen Lab},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Molnar, P. {\&} Gill - 2018 - Bots at the Gate a human rights analysis of automated decision-making in Canada's immigration and refugee sys.pdf:pdf},
pages = {88},
title = {{Bots at the Gate: a human rights analysis of automated decision-making in Canada's immigration and refugee system}},
url = {https://www.academia.edu/37489499/Bots{\_}at{\_}the{\_}Gate{\_}A{\_}Human{\_}Rights{\_}Analysis{\_}of{\_}Automated{\_}Decision-Making{\_}in{\_}Canadas{\_}Immigration{\_}and{\_}Refugee{\_}System?email{\_}work{\_}card=title},
year = {2018}
}
@inproceedings{Budgen2006,
abstract = {Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance.},
archivePrefix = {arXiv},
arxivId = {1304.1186},
author = {Budgen, David and Brereton, Pearl},
booktitle = {Int. Conf. Soft. Engin.},
doi = {10.1145/1134285.1134500},
eprint = {1304.1186},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Budgen, Brereton - 2006 - Performing systematic literature reviews in software engineering.pdf:pdf},
isbn = {1595933751},
issn = {1595933751},
keywords = {Evidence,Systematic literature review},
pages = {1051},
pmid = {10853839},
title = {{Performing systematic literature reviews in software engineering}},
url = {http://delivery.acm.org/10.1145/1140000/1134500/p1051-budgen.pdf?ip=158.125.51.236{\&}id=1134500{\&}acc=ACTIVE SERVICE{\&}key=BF07A2EE685417C5.F25E547D993D41C5.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1527591823{\_}bac88f025c31075293fc84f61df24e5c http://portal.acm.},
year = {2006}
}
@article{Sherar2011,
abstract = {Background: Over the past decade, accelerometers have increased in popularity as an objective measure of physical activity in free-living individuals. Evidence suggests that objective measures, rather than subjective tools such as questionnaires, are more likely to detect associations between physical activity and health in children. To date, a number of studies of children and adolescents across diverse cultures around the globe have collected accelerometer measures of physical activity accompanied by a broad range of predictor variables and associated health outcomes. The International Children's Accelerometry Database (ICAD) project pooled and reduced raw accelerometer data using standardized methods to create comparable outcome variables across studies. Such data pooling has the potential to improve our knowledge regarding the strength of relationships between physical activity and health. This manuscript describes the contributing studies, outlines the standardized methods used to process the accelerometer data and provides the initial questions which will be addressed using this novel data repository. Methods. Between September 2008 and May 2010 46,131 raw Actigraph data files and accompanying anthropometric, demographic and health data collected on children (aged 3-18 years) were obtained from 20 studies worldwide and data was reduced using standardized analytical methods. Results: When using 8, 10 and 12 hrs of wear per day as a criterion, 96{\%}, 93.5{\%} and 86.2{\%} of the males, respectively, and 96.3{\%}, 93.7{\%} and 86{\%} of the females, respectively, had at least one valid day of data. Conclusions: Pooling raw accelerometer data and accompanying phenotypic data from a number of studies has the potential to: a) increase statistical power due to a large sample size, b) create a more heterogeneous and potentially more representative sample, c) standardize and optimize the analytical methods used in the generation of outcome variables, and d) provide a means to study the causes of inter-study variability in physical activity. Methodological challenges include inflated variability in accelerometry measurements and the wide variation in tools and methods used to collect non-accelerometer data. {\textcopyright} 2011 Sherar et al; licensee BioMed Central Ltd.},
author = {Sherar, Lauren B and Griew, Pippa and Esliger, Dale W and Cooper, Ashley R and Ekelund, Ulf and Judge, Ken and Riddoch, Chris},
doi = {10.1186/1471-2458-11-485},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sherar et al. - 2011 - International children's accelerometry database (ICAD) Design and methods.pdf:pdf},
issn = {14712458},
journal = {BMC Public Health},
month = {dec},
number = {1},
pages = {485},
pmid = {21693008},
title = {{International children's accelerometry database (ICAD): Design and methods}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21693008 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3146860 http://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-11-485},
volume = {11},
year = {2011}
}
@article{Kitchin2015a,
abstract = {Since the mid-1990s a plethora of indicator projects have been developed and adopted by cities seeking to measure and monitor various aspects of urban systems. These have been accompanied by city benchmarking endeavours that seek to compare intraand inter-urban performance. More recently, the data underpinning such projects have started to become more open to citizens, more real-time in nature generated through sensors and locative/social media, and displayed via interactive visualisations and dashboards that can be accessed via the internet. In this paper, we examine such initiatives arguing that they advance a narrowly conceived but powerful realist epistemology – the city as visualised facts – that is reshaping how managers and citizens come to know and govern cities. We set out how and to what ends indicator, benchmarking and dashboard initiatives are being employed by cities. We argue that whilst these initiatives often seek to make urban processes and performance more transparent and to improve decision making, they are also underpinned by a naive instrumental rationality, are open to manipulation by vested interests, and suffer from often unacknowledged methodological and technical issues. Drawing on our own experience of working on indicator and dashboard projects, we argue for a conceptual re-imaging of such projects as data assemblages – complex, politically-infused, sociotechnical systems that, rather than reflecting cities, actively frame and produce them.},
author = {Kitchin, Rob and Lauriault, Tracey P. and McArdle, Gavin},
doi = {10.1080/21681376.2014.983149},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kitchin, Lauriault, McArdle - 2015 - Knowing and governing cities through urban indicators, city benchmarking and real-time dashboards.pdf:pdf},
issn = {2168-1376},
journal = {Regional Studies, Regional Science},
keywords = {Benchmarking,Cities,Dashboards,Data assemblage,Epistemology,Governance,Indicators,Real-time},
month = {jan},
number = {1},
pages = {6--28},
publisher = {Routledge},
title = {{Knowing and governing cities through urban indicators, city benchmarking and real-time dashboards}},
url = {https://www.tandfonline.com/doi/full/10.1080/21681376.2014.983149},
volume = {2},
year = {2015}
}
@misc{LIGOScientificCollaboration2016,
abstract = {This page has been prepared by the LIGO Scientific Collaboration (LSC) and the Virgo Collaboration to inform the broader community about a confirmed astrophysical event observed by the gravitational-wave detectors, and to make the data around that time available for others to analyze. There is also a technical details page about the data linked below, and feel free to contact us.},
author = {{LIGO Scientific Collaboration}},
booktitle = {LIGO Open Science Center},
doi = {10.7935/K5MW2F23},
title = {{LIGO Open Science Center release of GW150914}},
url = {https://losc.ligo.org/events/GW150914/},
urldate = {2018-09-03},
year = {2016}
}
@article{DAngelo2011,
abstract = {National exercises for the evaluation of research activ- ity by universities are becoming regular practice in ever more countries. These exercises have mainly been con- ducted through the application of peer-review methods. Bibliometricshasnotbeenable to offer a valid large-scale alternative because of almost overwhelming difficulties in identifying the true author of each publication.We will address this problem by presenting a heuristic approach to author name disambiguation in bibliometric datasets for large-scale research assessments. The application proposed concerns the Italian university system, com- prising 80 universities and a research staff of over 60,000 scientists. The key advantage of the proposed approach is the ease of implementation. The algorithms are of practical application and have considerably better scala- bility and expandability properties than state-of-the-art unsupervised approaches. Moreover, the performance in terms of precision and recall, which can be further improved, seems thoroughly adequate for the typical needs of large-scale bibliometric research assessments.},
author = {D'Angelo, Ciriaco Andrea and Giuffrida, Cristiano and Abramo, Giovanni},
doi = {10.1002/asi.21460},
isbn = {1532-2882},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {feb},
number = {2},
pages = {257--269},
pmid = {502955140},
title = {{A heuristic approach to author name disambiguation in bibliometrics databases for large-scale research assessments}},
url = {http://doi.wiley.com/10.1002/asi.21460},
volume = {62},
year = {2011}
}
@article{Scott2018,
abstract = {What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness' of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.},
annote = {According to some, ‘‘6 types of analyses — Descriptive, Exploratory, Inferential, Predic-
tive, Causal and Mechanistic''},
author = {Scott, E. Marian},
doi = {10.1016/J.SPL.2018.02.050},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Scott - 2018 - The role of Statistics in the era of big data Crucial, critical and under-valued.pdf:pdf},
issn = {0167-7152},
journal = {Statistics {\&} Probability Letters},
month = {may},
pages = {20--24},
publisher = {North-Holland},
title = {{The role of Statistics in the era of big data: Crucial, critical and under-valued}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300956?via{\%}3Dihub},
volume = {136},
year = {2018}
}
@article{Somers-Yeates2013,
abstract = {With moth declines reported across Europe, and parallel changes in the amount and spectra of street lighting, it is important to understand exactly how artificial lights affect moth populations. We therefore compared the relative attractiveness of shorter wavelength (SW) and longer wavelength (LW) lighting to macromoths. SW light attracted significantly more individuals and species of moth, either when used alone or in competition with LW lighting. We also found striking differences in the relative attractiveness of different wavelengths to different moth groups. SW lighting attracted significantly more Noctuidae than LW, whereas both wavelengths were equally attractive to Geometridae. Understanding the extent to which different groups of moth are attracted to different wavelengths of light will be useful in determining the impact of artificial light on moth populations.},
address = {Centre for Ecology and Conservation, Biosciences, University of Exeter, Penryn, UK. rhs206@exeter.ac.uk},
annote = {id: 2; LR: 20150426; JID: 101247722; OID: NLM: PMC3730649; OTO: NOTNLM; 2013 [ppublish]; epublish},
author = {Somers-Yeates, R and Hodgson, D and McGregor, P K and Spalding, A and Ffrench-Constant, R H},
doi = {10.1098/rsbl.2013.0376 [doi]},
issn = {1744-957X; 1744-9561},
journal = {Biology letters},
keywords = {Animals,Conservation of Natural Resources,England,Lepidoptera,Light,Moths/physiology,Seasons,Species Specificity,Visual Perception,artificial light pollution,ecological impact,metal halide street lights,moth population declines},
month = {may},
number = {4},
pages = {20130376},
title = {{Shedding light on moths: shorter wavelengths attract noctuids more than geometrids}},
volume = {9},
year = {2013}
}
@article{Munson2010,
abstract = {This document describes the eBird reference data set and the processing steps taken during creation. We hope this data will be a useful resource for studying avian dynamics and for developing new ecological modeling techniques.},
author = {Munson, M Arthur and Webb, Kevin and Sheldon, Daniel and Fink, Daniel and Hochachka, Wesley M and Iliff, Marshall and Riedewald, Mirek and Sorokina, Daria and Sullivan, Brian and Wood, Christopher and Kelling, Steve},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Munson et al. - 2010 - The eBird Reference Dataset, Version 2. 0.pdf:pdf},
journal = {Network},
pages = {1--11},
title = {{The eBird Reference Dataset, Version 2. 0}},
url = {http://www.birds.cornell.edu http://www.avianknowledge.net/content/features/archive/eBird{\_}Ref},
year = {2010}
}
@article{Harding2001,
abstract = {Changes occur very quickly within competitive business environments, and successful companies need to respond quickly by producing and delivering improved products and services, or by changing their business strategies and operational systems. Such changes may require redesign of resources, processes, strategies or organizational structures within the enterprise. Redesign is an expensive and risky process. This paper reports on how information models, databases and support tools, can be used to reduce uncertainty, by modelling the desired enterprise, and predicting its performance, before costly physical implementations are undertaken. Thus, management can gain valuable insight into the potential efficiency and performance capabilities of the redesigned enterprise, and minimize the risks associated with change. An overview is provided of an information-centred, multi-view design system to facilitate and accelerate the design or redesign of manufacturing enterprises. The design system includes both an information model, to store details of the proposed enterprise, and multiple design tools to support both the building and evaluation of the model. The design tools enable the model to be viewed in different ways, thus emphasizing and clarifying particular aspects of the design, and enabling the potential performance of the designed enterprise to be predicted.},
author = {Harding, J. A. and Popplewell, K.},
doi = {10.1080/09511920110040557},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Harding, Popplewell - 2001 - Enterprise design information The key to improved competitive advantage.pdf:pdf},
issn = {0951192X},
journal = {International Journal of Computer Integrated Manufacturing},
month = {nov},
number = {6},
pages = {514--521},
publisher = {Taylor {\&} Francis Group},
title = {{Enterprise design information: The key to improved competitive advantage}},
volume = {14},
year = {2001}
}
@article{August2015,
author = {August, Tom and Harvey, Martin and Lightfoot, Paula and Kilbey, David and Papadopoulos, Timos and Jepson, Paul},
doi = {10.1111/bij.12534},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/August et al. - 2015 - Emerging technologies for biological recording.pdf:pdf},
issn = {00244066},
journal = {Biological Journal of the Linnean Society},
month = {jul},
number = {3},
pages = {731--749},
publisher = {Oxford University Press},
title = {{Emerging technologies for biological recording}},
url = {https://academic.oup.com/biolinnean/article-lookup/doi/10.1111/bij.12534},
volume = {115},
year = {2015}
}
@article{EnglandRonaldSterneWilkinson1966,
abstract = {Those who find enjoyment in the books of P. B.M. Allan have had at least an introduction to the history of our familiar method of using artificial bait to attract nocturnal Lepidoptera (Allan, 1937, 1943, 1947). While the present paper was in manuscript, D. E. Allen's welcome contribution on the origin of the method came to hand (Allen, 1965); several historians of science have since added their comments (Allan, 1965; Wilkinson, 1965). The discovery of additional material in the publications of the early nineteenth century has made desirable a sum- mary of what we now know about the development of " sugaring. "},
author = {Wilkinson, Ronald Sterne},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wilkinson - 1966 - The Invention of Sugaring for Moths in Ninetheeth-Century.pdf:pdf},
journal = {The Michigan Entomologist},
number = {1},
pages = {10},
title = {{The Invention of "Sugaring" for Moths in Ninetheeth-Century}},
url = {https://scholar.valpo.edu/tgle/vol1/iss1/1},
volume = {1},
year = {1966}
}
@article{Faraway2018,
abstract = {Small data is sometimes preferable to big data. A high quality small sample can produce superior inferences to a low quality large sample. Data has acquisition, computation and privacy costs which require costs to be balanced against benefits. Statistical inference works well on small data but not so well on large data. Sometimes aggregation into small datasets is better than large individual-level data. Small data is a better starting point for teaching of Statistics.},
author = {Faraway, Julian J. and Augustin, Nicole H.},
doi = {10.1016/j.spl.2018.02.031},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Faraway, Augustin - 2018 - When small data beats big data.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Big data,Small data},
month = {may},
pages = {142--145},
publisher = {North-Holland},
title = {{When small data beats big data}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300762?via{\%}3Dihub},
volume = {136},
year = {2018}
}
@article{Kim2003,
author = {Kim, Won and Choi, Byoung-Ju and Hong, Eui-Kyeong and Kim, Soo-Kyung and Lee, Doheon},
doi = {10.1023/A:1021564703268},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2003 - A Taxonomy of Dirty Data.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {1},
pages = {81--99},
publisher = {Kluwer Academic Publishers},
title = {{A Taxonomy of Dirty Data}},
url = {http://link.springer.com/10.1023/A:1021564703268},
volume = {7},
year = {2003}
}
@article{JinXing2014,
abstract = {With increasing amounts of spatial, spectral and temporal remote sensing data and heterogeneity of platforms, we have entered an era of big data in remote sensing research. Imagery now routinely exceeds the memory size of personal computers so splitting/distributing big remote sensing data becomes a necessary pre-processing step. Standard rectangle based splitting methods can distort existing geometric and topological information and lose features as images are split into tiles. To address these challenges, we propose a sampling based image splitting method, which models the dataset as a streaming service and splits the dataset with a Voronoi diagram. The streaming data is systematically sampled to initially select the seeds of a Voronoi diagram. Voronoi regions are then generated according to spatial and spectral distances using Fortune's sweepline algorithm [1]. We test the splitting method with AVIRIS imagery of North America in 2013 (courtesy of NASA/JPL-Caltech) to evaluate the ability to detect objects of our splitting method. For evaluation we employ the object-based classification method of Hay and Castilla [2]. In contrast to rectangle based splitting approaches, most polygon borders generated by our method are found to converge with object borders (e.g., trees, building, and roads). When deployed with MapReduce, our sampling based splitting method also helps balance the computation intensity between each computing node.},
author = {Xing, Jin and Sieber, Renee},
doi = {10.1109/IGARSS.2014.6946699},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Xing, Sieber - 2014 - Sampling based image splitting in large scale distributed computing of earth observation data.pdf:pdf},
isbn = {978-1-4799-5775-0},
journal = {2014 IEEE Geoscience and Remote Sensing Symposium},
keywords = {Big Data,Splitting,Sreaming,Voronoi},
month = {jul},
pages = {1409--1412},
publisher = {IEEE},
title = {{Sampling based image splitting in large scale distributed computing of earth observation data}},
url = {http://ieeexplore.ieee.org/document/6946699/ http://www.scopus.com/inward/record.url?eid=2-s2.0-84911366834{\&}partnerID=tZOtx3y1},
year = {2014}
}
@article{Elgendy2016,
abstract = {Information is a key success factor influencing the performance of decision makers, specifically the quality of their decisions. Nowadays, sheer amounts of data are available for organizations to analyze. Data is considered the raw material of the 21st century, and abundance is assumed with today's 15 billion devices [aka Things!] already connected to the Internet. Accordingly, solutions need to be studied and provided in order to handle and extract value and knowledge from these datasets. Furthermore, decision makers need to be able to gain valuable insights from such rapidly changing data of high volume, velocity, variety, veracity, and value by using big data analytics. This paper aims to research how big data analytics can be integrated into the decision making process. Accordingly, using a design science methodology, the “Big – Data, Analytics, and Decisions” (B-DAD) framework was developed in order to map big data tools, architectures, and analytics to the different decision making phases. The ultimate objective and contribution of the framework is using big data analytics to enhance and support decision making in organizations, by integrating big data analytics into the decision making process. Consequently, an experiment in the retail industry was administered to test the framework. Accordingly, results showed added value when integrating big data analytics into the decision making process.},
author = {Elgendy, Nada and Elragal, Ahmed},
doi = {10.1016/J.PROCS.2016.09.251},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Elgendy, Elragal - 2016 - Big Data Analytics in Support of the Decision Making Process.pdf:pdf},
issn = {1877-0509},
journal = {Procedia Computer Science},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jan},
pages = {1071--1084},
publisher = {Elsevier},
title = {{Big Data Analytics in Support of the Decision Making Process}},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324206?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {100},
year = {2016}
}
@misc{Schatz2014,
abstract = {In the ‘classical' engineering process of embedded systems, models are most commonly used to support the early deﬁnition of the system (or parts thereof) or the environment. However, with the move from (complex) embedded systems to cyber-physical systems (CPS), we are experiencing a paradigm-shift. In contrast to traditional embedded systems, CPS are driven by three additional dimensions of complexity: The ‘cross'-dimension, with issues like cross-application domains, cross-technologies, cross-organizations, etc; the ‘self'-dimension, with issues like self-monitoring, self-adapting, self-optimizing, etc.; and the ‘live'-dimension, with issues like live-conﬁguration, live-update, live-enhancement, etc. Due to the necessary shift from the ‘classical' development process in dedicated application domains with a clear distinction between Design/Implementation and Maintenance/Operation to the continuous development cycle merging these phases as well as roles like developer/operator/user, also the role of models in CPS requires a rethinking. In this contribution, we sketch how the model-based approach with construction, analysis and synthesis of models has to be adapted to support the engineering of cyber-physical systems, using the domain of smart energy systems for illustration.},
address = {Berlin},
annote = {Paper in Road2CPS. 

Overview paper; useful. Author worked on CyPHERS project. 

Paper actually considers CPS co-workers as control agents, adjusting the CPS to their own local needs. First document within Road2CPS to do so. Recall Rasmussen here.

Comment from Conclusions section:
"In this contribution we argued that cyber-physical systems are not only large-scale networked em-bedded systems, but consistute a paradigm shift in systems engineering. The key principles of cyber-physical systems and main complexity drivers are the ‘cross'-, ‘live'-, and ‘self'-domain.

Due to these principles we expect that:
• the explicit use of models will play a dominant role in the engineering of CPS
• the traditional distinction between design/implementation and operation/maintenance of a system will be abandoned in favor of an integrated life cycle.

However, to meet these changes in engineering CPS, we are faced with the challenges:
• to provide a model engineering framework including both a common theory of modeling paradigms as well as methods to arrange (disributed) layers of models
• to turn a CPS into its own engineering and development environment, with built-in mechanisms to construct, analyze, and synthesize models of its environment, platform, and functionality"


Defines classes of models:
"In these aforementioned domains – speciﬁcally automotive, areonautics, and automation – the following main categories of models are generally used:

• Functional models: These models - often using a notation speciﬁc to the domain of application – describe the functionality of the system under development. In embedded systems, generally control-theoretic description techniques like Block Diagrams/Data Flow Diagrams and extensions thereof are used.

• Platform models: These models describe the platform the functionality is deployed to – in general describing both the HW elements including control units and buses as well as the accompanying SW stack like middleware or operating systems.

• Environment models: These models describe the behavior of (the part of) the environment the system under development is embedded into. In general, physical processes – like the rigid-body mechanics – controlled by the system are described, in rare cases these models also describe users of the system.

The use of concise description techniques to explicitly construct these models has become one of the core assets of a model-based development process: By limiting the expressibility – and thus focusing the speciﬁcation on the relevant aspects – they constructively support the assurance of quality. Furthermore, the use of explicit models enables two further mechanisms: To analyze models w.r.t. their validity or correctness, e.g., by simulating executable models, check for absence of non-determinism, or verify their conformance [4]. Furthermore, to synthesize new models or complete partial models, e.g., by obtaining test-cases from executable models, or generate deployments of function models including execution schedules to platforms [12]."
[4] Manfred Broy, Sascha Kirstan, Helmut Krcmar, and Bernhard Sch¨atz. What is the Beneﬁt of a Model-Based Design of Embedded Software Systems in the Car Industry? In Emerging Technologies for the Evolution and Maintenance of Software Models. IGI Global, 2011.
[12] Sebastian Voss and Bernhard Sch¨atz. Deployment and Scheduling Synthesis for Mixed-Criticality Shared Memory Systems. In Engineering of Computer Based Systems. IEEE, 2013.
Definition of difference between embedded systems and CPS:
"[8- E. Geisberger {\&} M. Broy, 2012, agendaCPS - integrierte forschungsagenda Cyber-Physical Systems. SpringerVieweg/acatech] stresses that CPS encompasses “embedded systems (...), but also logistics-, coordination- and management-processes as well as internet services”, and are ”interconnected (...) locally as well as globally”. These aspects – speciﬁcally the combination of physical and organizational processes on a local and a global scale – are essential to differentiate embedded systems like a motor management system and even a complete vehicle from those CPS we consider to be a new class of system. Examples for these later class – as also listed in [8] – are [smart traffic systems, smart health care systems, smart energy systems]"

Main characteristics of CPS:
• ‘Cross'-Dimension: As CPS cover large-scale processes – both physical and organizational - these processes generally go across borders, with respect to application domains, engineering disciplines, used technologies, or involved organizations, to mention a few.
• ‘Live'-Dimension: Furthermore, CPS generally support missing critical processes, making it impossible to turn off the system to make changes and therefore, for instance, requiring (re-)conﬁguration, (re-)deployment, (de- )commissioning, update, or enhancement during runtime.
• ‘Self'-Dimension: Finally, being large-scale and mission critical, CPS must [be] cooperative with system engineers, operators, users, and other systems by actively supporting their processes, requiring autonomous capabilities of documentation, monitoring, optimization, healing, or adapting, among others.

Implications pf these characteristics are:
• a large range models of different domains, disciplines, and technologies are used, and
• the use of models is shifted from the design and implementation phase to the operation and maintenance phase.

The ﬁrst consequence is caused by the ‘cross'-characteristics of CPS. As a result it is necessary to ﬁnd a common theory of modeling, allowing to relate these different kinds of models. Often, they can be organized in layers of models, corresponding to the layers of services or functionalities offered by the CPS architecture. ... a three-tier approach of models is used. Models on the lowest layer address electrical devices of the system platform including the immediate physical processes of the environ-ment they are embedded into ... Models of the middle layer add more platform information – like the location of devices - and medium-scale environment information – like effects of blinds and lights on brightness as well as user behavior – and speciﬁcally address complex functionalities like rules governing the reactions of the system to user actions. The models of the top layer allow to transcend the borders of individual nodes. On the platform and environment side, they aggregate nodes to networks of nodes including the corresponding inter-node processes like energy ﬂow. On the functional side, models addressing aspects of optimization like (shiftable) loads or mar-kets are added. ... Therefore, the models - most speciﬁcally of the system (functionality) and the environment – capture aspects ranging from control on the lower layer, to act on the middle layer to plan and cooperate on the top layer, as commonly found in robotic architectures [11].
[11] Nils Ole Tippenhauer. A Multi-Robot Architecture for Autonomous Cooperative Behaviours. Technical report, University of Waterloo, 2005.

The second consequence – the shift of the use of models from the design/implementation phase to the operation/maintenance phase – is primarily caused by the ‘live'- and ‘self'-dimension. Since a CPS is reconﬁgured, updated, and enhanced during runtime, those models used in Section I at design time must be made available at runtime. Furthermore, since CPS can autonomously reconﬁgure or adapt their behavior, especially models of their platform and functionality must be made explicit at runtime. As a result of this shift, not only the construction of these models, but also corresponding analysis and synthesis methods must be made available during operation and maintenance, turing a CPS into its own engineering and development environment.

This is of even more importance as the clear distinction between a designer/implementer and an operator/user of a system is increasingly blurred in a CPS. Using again the example of the smart energy systems, ofﬁce space users are allowed to adapt the rules controlling the behavior of their ofﬁce – turning them from a user into a designer. Similarly, adding a new device at runtime turns a system implementer into a system operator. As the updating or enhancement of functionality can have problematic side effects and is carried out by domain experts or users rather then system engineers, validation and veriﬁcation mechanisms prior to the activation of the changed behavior are necessary. In case of the smart energy system, the rule component provides a domain-speciﬁc language to deﬁne behavior using concepts like events, values, and actions, as well as mechanism to verify the absence of ﬂaws like overlapping or missing rules [10]. Thus, capabilities like the soundness analysis of the model-based development process mentioned in Section I or compatibility analysis as in [3] can be integrated in a similar fashion into a CPS engineering approach. Likewise, synthesis techniques like the generation of schedules mentioned in Section I can be applied to support the conﬁguration space exploration in the engineering of CPS. In the smart energy system, for instance, degradation plans for the incremental deactivation of devices in case of grid failure are synthesized from the platform and function models. Note that while these analysis and synthesis techniques require the use of models at runtime, these functionalities need not be provided online by the executing components of the CPS. For instance, in the smart energy system, the architecture uses dedicated components for the analysis and synthesis of models, which then are forwarded to the executing rule system. As however the models used are provided by the running system, both parts form a combined system, merging operating and engineering environments.},
author = {Sch{\"{a}}tz, B},
booktitle = {CPS20:  CPS 20 years from now - visions and challenges},
keywords = {AI,BPR,Cyber-physical systems,IoT,affordances,agents,allocation of functions,architecture,business process re-engineering,complexity,control,decision-making,design process,engineering,knowledge configuration,maintenance,models,philosophy,product introduction process,resilience,robots,robustness,sustainability,systems},
title = {{The role of models in engineering of cyber-physical systems – challenges and possibilities}},
url = {http://www.cyphers.eu/cps20},
year = {2014}
}
@article{Embury2001,
abstract = {Integration of data sources opens up possibilities for new and valuable applications of data that cannot be supported by the individual sources alone. Unfortunately, many data integration projects are hindered by the inherent heterogeneities in the sources to be integrated. In particular, differences in the way that real world data is encoded within sources can cause a range of difficulties, not least of which is that the conflicting semantics may not be recognised until the integration project is well under way. Once identified, semantic conflicts of this kind are typically dealt with by configuring a data transformation engine, that can convert incoming data into the form required by the integrated system. However, determination of a complete and consistent set of data transformations for any given integration task is far from trivial. In this paper, we explore the potential application of techniques for integrity enforcement in supporting this process. We describe the design of a data reconciliation tool (LITCHI) based on these techniques that aims to assist taxonomists in the integration of biodiversity data sets. Our experiences have highlighted several limitations of integrity enforcement when applied to this real world problem, and we describe how we have overcome these in the design of our system.},
author = {Embury, Suzanne M. and Brandt, Sue M. and Robinson, John S. and Sutherland, Iain and Bisby, Frank A. and Gray, W. Alex and Jones, Andrew C. and White, Richard J.},
doi = {10.1016/S0306-4379(01)00044-8},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Embury et al. - 2001 - Adapting integrity enforcement techniques for data reconciliation.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Biodiversity information systems,Data integration,Data reconciliation,Integrity constraints,Integrity enforcement},
number = {8},
pages = {657--689},
publisher = {Elsevier Ltd},
title = {{Adapting integrity enforcement techniques for data reconciliation}},
volume = {26},
year = {2001}
}
@article{Liao2017,
abstract = {Complex networks have emerged as a simple yet powerful framework to represent and analyze a wide range of complex systems. The problem of ranking the nodes and the edges in complex networks is critical for a broad range of real-world problems because it affects how we access online information and products, how success and talent are evaluated in human activities, and how scarce resources are allocated by companies and policymakers, among others. This calls for a deep understanding of how existing ranking algorithms perform, and which are their possible biases that may impair their effectiveness. Many popular ranking algorithms (such as Google's PageRank) are static in nature and, as a consequence, they exhibit important shortcomings when applied to real networks that rapidly evolve in time. At the same time, recent advances in the understanding and modeling of evolving networks have enabled the development of a wide and diverse range of ranking algorithms that take the temporal dimension into account. The aim of this review is to survey the existing ranking algorithms, both static and time-aware, and their applications to evolving networks. We emphasize both the impact of network evolution on well-established static algorithms and the benefits from including the temporal dimension for tasks such as prediction of network traffic, prediction of future links, and identification of significant nodes.},
annote = {Complex networks look like a very important area for anlysis of all sorts of data.},
archivePrefix = {arXiv},
arxivId = {1704.08027},
author = {Liao, Hao and Mariani, Manuel Sebastian and Medo, Mat{\'{u}}{\v{s}} and Zhang, Yi Cheng and Zhou, Ming Yang},
doi = {10.1016/j.physrep.2017.05.001},
eprint = {1704.08027},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Liao et al. - 2017 - Ranking in evolving complex networks.pdf:pdf},
issn = {03701573},
journal = {Physics Reports},
keywords = {Centrality metrics,Complex networks,Network science,Ranking,Recommendation,Temporal networks},
month = {may},
pages = {1--54},
publisher = {North-Holland},
title = {{Ranking in evolving complex networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0370157317300935?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email{\#}fig1},
volume = {689},
year = {2017}
}
@inproceedings{Bhagdev2008,
abstract = {This paper describes hybrid search, a search method supporting both document and knowledge retrieval via the flexible combination of ontology-based search and keyword-based matching. Hybrid search smoothly copes with lack of semantic coverage of document content, which is one of the main limitations of current semantic search methods. In this paper we define hybrid search formally, discuss its compatibility with the current semantic trends and present a reference implementation: K-Search. We then show how the method outperforms both keyword-based search and pure semantic search in terms of precision and recall in a set of experiments performed on a collection of about 18.000 technical documents. Experiments carried out with professional users show that users understand the paradigm and consider it very powerful and reliable. K-Search has been ported to two applications released at Rolls-Royce plc for searching technical documentation about jet engines. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Bhagdev, Ravish and Chapman, Sam and Ciravegna, Fabio and Lanfranchi, Vitaveska and Petrelli, Daniela},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-68234-9_41},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bhagdev et al. - 2008 - Hybrid search Effectively combining keywords and semantic searches.pdf:pdf},
isbn = {3540682333},
issn = {03029743},
keywords = {Semantic Web in use,Semantic search},
pages = {554--568},
title = {{Hybrid search: Effectively combining keywords and semantic searches}},
volume = {5021 LNCS},
year = {2008}
}
@article{Jain2013,
abstract = {In this paper, the concept of data mining was summarized and its significance towards its methodologies was illustrated. The data mining based on Neural Network and Genetic Algorithm is researched in detail and the key technology and ways to achieve the data mining on Neural Network and Genetic Algorithm are also surveyed. This paper also conducts a formal review of the area of rule extraction from ANN and GA.},
author = {Jain, Nikita and Srivastava, Vishal},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Jain, Srivastava - 2013 - Data Mining Techniques a Survey Paper.pdf:pdf},
journal = {IJRET: International Journal of Research in Engineering and Technology},
keywords = {--------------------------------------------------,Big Data,data mining,genetic algorithm,neural network,rule extraction},
mendeley-tags = {Big Data},
number = {11},
pages = {116--119},
title = {{Data Mining Techniques: a Survey Paper}},
url = {http://esatjournals.net/ijret/2013v02/i11/IJRET20130211019.pdf http://esatjournals.net/ijret/2013v02/i11/IJRET20130211019.pdf{\%}0Ahttp://ijret.org/Volumes/V02/I11/IJRET{\_}110211019.pdf},
volume = {2},
year = {2013}
}
@techreport{Edgeintelligenceprojectteam2018,
abstract = {To enable and realize the true value of the internet of things (IoT), edge intelligence pushes processing for data intensive applications away from the core of the cloud to the edge of the network. This radical transformation from the cloud to the edge, edge intelligence, will support trillions of sensors and billions of systems. It will treat data in motion differently from data at rest. This White Paper synthesizes current trends in the areas of cloud computing, mobile networking, IoT and other domains that require low delay in communication and decision. Such domains include smart manufacturing, video analysis for security and safety, automotive, intelligent city furniture or virtual reality. The publication explores market potential and vertical use case requirements, analyzes gaps and produces recommendations for adopting vertical edge intelligence technologies. This White Paper was developed by the IEC Market Strategy Board (MSB) edge intelligence project team with major contributions from Huawei and the Fraunhofer Institute for Open Communications Systems FOKUS},
address = {Geneva, Switzerland},
author = {Edge intelligence project team},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Edge intelligence project team - 2018 - Edge Intelligence White Paper.pdf:pdf},
institution = {International Electrotechnical Commission},
pages = {134},
title = {{Edge Intelligence: White Paper}},
url = {http://www.iec.ch/whitepaper/edgeintelligence/},
year = {2018}
}
@article{DeGlas1986,
author = {{De Glas}, Frank},
doi = {10.1515/libr.1986.36.1.40},
issn = {18658423},
journal = {Libri},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {1},
pages = {40--64},
title = {{Fiction and Bibliometrics: Analyzing a Publishing House's Stocklist}},
url = {https://www.degruyter.com/view/j/libr.1986.36.issue-1/libr.1986.36.1.40/libr.1986.36.1.40.xml},
volume = {36},
year = {1986}
}
@article{Melorose2015,
abstract = {Almost universally, women with higher levels of education have fewer children. Better education is associated with lower mortality, better health, and different migration patterns. Hence, the global population outlook depends greatly on further progress in education, particularly of young women. By 2050, the highest and lowest education scenarios--assuming identical education-specific fertility rates--result in world population sizes of 8.9 and 10.0 billion, respectively. Better education also matters for human development, including health, economic growth, and democracy. Existing methods of multi-state demography can quantitatively integrate education into standard demographic analysis, thus adding the "quality" dimension.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Melorose, J. and Perroy, R. and Careas, S.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Melorose, Perroy, Careas - 2015 - World population prospects.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {United Nations},
keywords = {icle},
number = {6042},
pages = {587--92},
pmid = {25246403},
title = {{World population prospects}},
url = {https://esa.un.org/unpd/wpp/publications/files/key{\_}findings{\_}wpp{\_}2015.pdf http://www.ncbi.nlm.nih.gov/pubmed/21798940},
volume = {1},
year = {2015}
}
@inproceedings{Singh2010,
abstract = {Web Data Mining is an important area of Data Mining which deals with the extraction of interesting knowledge from the World Wide Web, It can be classified into three different types i.e. web content mining, web structure mining and web usages mining. The aim of this paper is to provide past, current evaluation and update in each of the three different types of web mining i.e. web content mining, web structure mining and web usages mining and also outlines key future research directions. This paper also reports the comparisons and summary of various methods of web data mining with applications, which gives the overview of development in research and some important research issues.},
archivePrefix = {arXiv},
arxivId = {cs/0011033v1},
author = {Singh, Brijendra and Singh, Hemant Kumar},
booktitle = {2010 IEEE International Conference on Computational Intelligence and Computing Research},
doi = {10.1109/ICCIC.2010.5705856},
eprint = {0011033v1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Singh - 2010 - Web Data Mining research A survey.pdf:pdf},
isbn = {978-1-4244-5965-0},
issn = {19310145},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {1--10},
primaryClass = {cs},
title = {{Web Data Mining research: A survey}},
url = {http://ieeexplore.ieee.org/abstract/document/5705856/ http://ieeexplore.ieee.org/document/5705856/},
year = {2010}
}
@article{Kotsiantis2006,
abstract = {In this paper, we provide the preliminaries of basic concepts about association rule mining and survey the list of existing association rule mining techniques. Of course, a single article cannot be a complete review of all the algorithms, yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions that have yet to be explored.},
author = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris},
journal = {Science},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {1},
pages = {71--82},
title = {{Association Rules Mining: A Recent Overview}},
url = {http://www.csis.pace.edu/{~}ctappert/dps/d861-13/session2-p1.pdf},
volume = {32},
year = {2006}
}
@article{Ramos2010,
annote = {id: 1; issn: print 1004-3756; issn: electronic 1861-9576; publication{\_}type: full{\_}text},
author = {Ramos, Ana Lu{\'{i}}sa and Ferreira, Jos{\'{e}} Vasconcelos and Barcel{\'{o}}, Jaume},
doi = {10.1007/s11518-010-5144-8},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ramos, Ferreira, Barcel{\'{o}} - 2010 - Revisiting the similar process to engineer the contemporary systems.pdf:pdf},
issn = {1004-3756},
journal = {Journal of Systems Science and Systems Engineering},
keywords = {This paper addresses the present-day context of Sy,academic papers,and facilities. An integrated holistic approach is,and interests. The Systems Engineering approach ai,and post-graduate programmes in the field. This wo,and to update the context of the SIMILAR process m,bringing to the interaction a considerable amount,hardware,information,international conferences,personnel,processes,providing the means to enable their successful rea,responsibilities,revisiting and setting up an updated framework for,skills,software,the number of parties involved (stakeholders and s},
number = {3},
pages = {321 {\textless}last{\_}page{\textgreater} 350},
title = {{Revisiting the similar process to engineer the contemporary systems}},
url = {http://dx.doi.org/10.1007/s11518-010-5144-8},
volume = {19},
year = {2010}
}
@misc{Thomas2017,
abstract = {New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (“crowds”) as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential—and limitations—of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine “technologies” are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.},
author = {Thomas, James and Noel-Storr, Anna and Marshall, Iain and Wallace, Byron and McDonald, Steven and Mavergames, Chris and Glasziou, Paul and Shemilt, Ian and Synnot, Anneliese and Turner, Tari and Elliott, Julian and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and Kolakowsky-Hayner, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch{\"{u}}nemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and Soares-Weiser, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Garc{\'{i}}a, Laura Mart{\'{i}}nez and Synnot, Anneliese and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Thomas, James and Egan, Cathy and McDonald, Steve and Tritton, Roger and Elliott, Julian and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Turner, Tari and Foxlee, Ruth and Morley, Richard and van Valkenhoef, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and Noel-Storr, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and {Yepes Nu{\~{n}}ez}, Juan Jos{\'{e}} and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
booktitle = {Journal of Clinical Epidemiology},
doi = {10.1016/j.jclinepi.2017.08.011},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Thomas et al. - 2017 - Living systematic reviews 2. Combining human and machine effort.pdf:pdf},
issn = {18785921},
keywords = {Automation,Citizen science,Crowdsourcing,Machine learning,Systematic review,Text mining},
month = {nov},
pages = {31--37},
pmid = {28912003},
publisher = {Pergamon},
title = {{Living systematic reviews: 2. Combining human and machine effort}},
url = {http://www.sciencedirect.com/science/article/pii/S0895435617306042?via{\%}3Dihub},
volume = {91},
year = {2017}
}
@article{Singh2016,
abstract = {Cleaning spreadsheet data types is a common problem faced by millions of spreadsheet users. Data types such as date, time, name, and units are ubiquitous in spreadsheets, and cleaning transformations on these data types involve parsing and pretty printing their string representations. This presents many challenges to users because cleaning such data requires some background knowledge about the data itself and moreover this data is typically nonuniform, unstructured, and ambiguous. Spreadsheet systems and Programming Languages provide some UI-based and programmatic solutions for this problem but they are either insufficient for the user's needs or are beyond their expertise. In this paper, we present a programming by example methodology of cleaning data types that learns the desired transformation from a few input-output examples. We propose a domain specific language with probabilistic semantics that is parameterized with declarative data type definitions. The probabilistic semantics is based on three key aspects: (i) approximate predicate matching, (ii) joint learning of data type interpretation, and (iii) weighted branches. This probabilistic semantics enables the language to handle non-uniform, unstructured, and ambiguous data. We then present a synthesis algorithm that learns the desired program in this language from a set of input-output examples.We have implemented our algorithm as an Excel add-in and present its successful evaluation on 55 benchmark problems obtained from online help forums and Excel product team.},
author = {Singh, Rishabh and Gulwani, Sumit},
doi = {10.1145/2837614.2837668},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Gulwani - 2016 - Transforming spreadsheet data types using examples(2).pdf:pdf},
isbn = {9781450335492},
issn = {15232867},
journal = {ACM SIGPLAN Notices},
keywords = {Noisy examples,Probabilistic synthesis,Program synthesis,Programming By examples,Spreadsheet programming},
month = {apr},
number = {1},
pages = {343--356},
publisher = {Association for Computing Machinery},
title = {{Transforming spreadsheet data types using examples}},
volume = {51},
year = {2016}
}
@article{Vantieghem2017,
abstract = {Citizen science projects have become important data sources for ecologists. However, opportunistic data are not only characterized by spatial and temporal biases, but probably also contain species identification errors, especially concerning morphologically similar species. Such misidentifications may result in wrongly estimated distribution ranges and trends, and thus in inadequate conservation measures. We illustrate this issue with three skipper butterflies (Hesperiidae) in Flanders (northern Belgium) using photographs uploaded with observations in data portals. Ochlodes sylvanus and Thymelicus lineola records had relatively low identification error rates (1 and 11 {\%}, respectively), but the majority (59 {\%}) of Thymelicus sylvestris records turned out to be misidentified. Using verified records only allowed us to model their distribution more accurately, especially for T. sylvestris whose actual distribution area had hitherto been strongly overestimated. An additional field study on T. sylvestris confirmed the species distribution model output as the species was almost completely restricted to sites with verified records and was largely absent from sites with unverified records. The preference of T. sylvestris for unimproved grasslands was confirmed by the negative correlation between its model-predicted presence and elevated nitrogen and ammonia levels. Thus, quality control of citizen science data is of major importance to improve the knowledge of species distribution ranges, biotope preferences and other limiting factors. This, in turn, will help to better assess species conservation statuses and to suggest more appropriate management and mitigation measures.},
author = {Vantieghem, Pieter and Maes, Dirk and Kaiser, Aur{\'{e}}lien and Merckx, Thomas},
doi = {10.1007/s10841-016-9924-4},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Vantieghem et al. - 2017 - Quality of citizen science data and its consequences for the conservation of skipper butterflies (Hesperiidae.pdf:pdf},
issn = {15729753},
journal = {Journal of Insect Conservation},
keywords = {Aerial ammonia pollution,Nitrogen-induced environmental change,Ochlodes sylvanus,Species distribution modelling,Thymelicus lineola,Thymelicus sylvestris},
month = {jun},
number = {3},
pages = {451--463},
publisher = {Springer International Publishing},
title = {{Quality of citizen science data and its consequences for the conservation of skipper butterflies (Hesperiidae) in Flanders (northern Belgium)}},
volume = {21},
year = {2017}
}
@article{Smirnova2018,
abstract = {Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.},
annote = {We conclude that the real difficulty is dealing with actual data, defining and clarifying
the associated scientific problems, and providing clean Biostatistical solutions; not the mathematics.},
author = {Smirnova, Ekaterina and Ivanescu, Andrada and Bai, Jiawei and Crainiceanu, Ciprian M.},
doi = {10.1016/j.spl.2018.02.014},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Smirnova et al. - 2018 - A practical guide to big data.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Accelerometer,Big data,Wearable and implantable computing},
month = {may},
pages = {25--29},
publisher = {North-Holland},
title = {{A practical guide to big data}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300592},
volume = {136},
year = {2018}
}
@article{Cuccurullo2013,
abstract = {Our paper analyses 20 years of performance management research published in the english-language journals, included in SSCI database, separating the business domain from public sector one. We used a content analysis for showing the relationships between the subfields of performance management and the time evolution. Through a multiple correspondence analysis based on keywords we provide a framework to track this literature over the 20-year period. We conclude the paper with a discussion on future pathways in the performance management literature.},
author = {Cuccurullo, Corrado and Aria, Massimo and Sarto, Fabrizia},
doi = {10.5465/AMBPP.2013.14270abstract},
issn = {2151-6561},
journal = {Academy of Management Proceedings},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jan},
number = {1},
pages = {14270},
title = {{Twenty years of research on performance management in business and public administration domains}},
url = {http://proceedings.aom.org/cgi/doi/10.5465/AMBPP.2013.14270abstract http://proceedings.aom.org/content/2013/1/14270.abstract},
volume = {2013},
year = {2013}
}
@book{PruittJ.Adlin2006,
abstract = {"The Persona Lifecycle addresses the "how" of creating effective personas and using those personas to design products that people love. It doesn't just describe the value of personas; it offers detailed techniques and tools related to planning, creating, communicating, and using personas to create great product designs. Moreover, it provides rich examples, samples, and illustrations to imitate and model. Perhaps most importantly, it positions personas not as a panacea, but as a method used to complement other user-centered design (UCD) techniques including scenario-based design, cognitive walkthroughs and user testing"--Resource description page. The next frontier for user-centered design : making user representations more usable -- The persona lifecycle : a framework for the persona approach -- Phase 1: Family planning (planning a persona effort) -- Phase 2: Conception {\&} gestation (creating personas) -- Phase 3: Birth {\&} maturation (launching and communicating personas) -- Phase 4: Adulthood (using personas) -- Phase 5: Lifetime achievement and retirement (ROI and reuse of personas) -- Contributed chapters: Users, roles and personas / by Larry Constantine -- Storytelling and narrative / by Whitney Quesenbery -- Reality and design maps / by Tamara Adlin {\&} Holly Jamesen -- Marketing versus design personas / by Bob Barlow-Busch -- Why personas work : the psychological evidence / by Jonathan Grudin -- Appendix A: G4K organizational archetype and sample persona -- Appendix B: Example personas from real projects -- Appendix C: Sample image release form.},
address = {London, UK},
author = {{Pruitt, J., {\&} Adlin}, T.},
isbn = {9780125662512},
pages = {724},
publisher = {Elsevier},
title = {{The persona lifecycle: Keeping people in mind throughout product design. San Francisco: Morgan Kaufmann.}},
year = {2006}
}
@article{Falk2019,
abstract = {Citizen science is an increasingly popular way of engaging volunteers in the collection of scientific data. Despite this, data quality remains a concern and there is little published evidence about the accuracy of records generated by citizen scientists. Here we compare data generated by two British citizen science projects, Blooms for Bees and BeeWatch, to determine the ability of volunteer recorders to identify bumblebee (Bombus) species. We assessed recorders' identification ability in two ways–as recorder accuracy (the proportion of expert-verified records correctly identified by recorders) and recorder success (the proportion of recorder-submitted identifications confirmed correct by verifiers). Recorder identification ability was low ({\textless}50{\%} accuracy; {\textless}60{\%} success), despite access to project specific bumblebee identification materials. Identification ability varied significantly depending on bumblebee species, with recorders most able to correctly identify species with distinct appearances. Blooms for Bees recorders (largely recruited from the gardening community) were markedly less able to identify bumblebees than BeeWatch recorders (largely individuals with a more specific interest in bumblebees). Within both projects, recorders demonstrated an improvement in identification ability over time. Here we demonstrate and quantify the essential role of expert verification within citizen science projects, and highlight where resources could be strengthened to improve recorder ability.},
author = {Falk, Steven and Foster, Gemma and Comont, Richard and Conroy, Judith and Bostock, Helen and Salisbury, Andrew and Kilbey, Dave and Bennett, James and Smith, Barbara},
doi = {10.1371/journal.pone.0218614},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Falk et al. - 2019 - Evaluating the ability of citizen scientists to identify bumblebee (Bombus) species.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {jun},
number = {6},
publisher = {Public Library of Science},
title = {{Evaluating the ability of citizen scientists to identify bumblebee (Bombus) species}},
volume = {14},
year = {2019}
}
@misc{Marsh2017,
abstract = {I do have attendance figures for each lecture at Birdfair but thought summary figures may be more helpful for you: In total 15,885 lecture attendances were made Birdfair - this is the total of every lecture attendance made across all lectures over 3 days - so some people will be counted more than once. The range of numbers attending varied widely over each day and each lecture theatre, and competed with events in the main Events Marquee : {\textperiodcentered} LM 1 had a range from around 30 to 226 with an average audience of 99 across three days {\textperiodcentered} LM2 had a range from 34 to 196 with an average of 98 across the three days {\textperiodcentered} LM3 had arrange from 9 (when a TV ‘name' was on the main stage) to 207 with an average of 90 across the three days {\textperiodcentered} AWBC had a range of 6 (again a TV ‘name' on the main stage) to 119 with an average of 53 across the three days. {\textperiodcentered} The Authors Forum attendance ranged from 31 to 112 with an average across the three days of 60 I hope this if the sort of information you were looking for but please do contact me if you need any other info. With kind regards Sarah Sarah Marsh, Office Assistant Unit 16A, Suite 3, Oakham Enterprise Park Ashwell Road,Oakham, Rutland, LE15 7TU Tel +44 (0) 1572 771079 Email smarsh@birdfair.org.uk},
address = {Oakham},
author = {Marsh, Sarah and Appleton, Tim},
pages = {1},
publisher = {Birdfair},
title = {{Personal Communication}},
year = {2017}
}
@article{Johnson2017,
abstract = {We present a many-body theory that explains and reproduces recent observations of population polarization dynamics, is supported by controlled human experiments, and addresses the controversy surrounding the Internet's impact. It predicts that whether and how a population becomes polarized is dictated by the nature of the underlying competition, rather than the validity of the information that individuals receive or their online bubbles. Building on this framework, we show that next-generation social media algorithms aimed at pulling people together, will instead likely lead to an explosive percolation process that generates new pockets of extremes.},
annote = {Software links supplied.},
archivePrefix = {arXiv},
arxivId = {1712.06009},
author = {Johnson, N. F. and Manrique, P. and Zheng, M. and Cao, Z. and Botero, J. and Huang, S. and Aden, N. and Song, C. and Leady, J. and Velasquez, N. and Restrepo, E. M.},
eprint = {1712.06009},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - Unknown - Population polarization dynamics and next-generation social media algorithms.pdf:pdf},
journal = {arXiv},
month = {dec},
pages = {1--12},
title = {{Population polarization dynamics and next-generation social media algorithms}},
url = {https://arxiv.org/abs/1712.06009 http://arxiv.org/abs/1712.06009 https://arxiv.org/pdf/1712.06009.pdf},
year = {2017}
}
@article{Bratman1975,
author = {Bratman, Harvey},
journal = {Computer},
number = {5},
pages = {28--37},
publisher = {IEEE},
title = {{The software factory}},
year = {1975}
}
@techreport{Duranton2015,
abstract = {EXECUTIVE SUMMARY: It there is one thing that characterizes the development of information technology, it is the unrelenting, always accelerating change. Computing systems keep pervading society deeper and deeper, and have acquired a strong foothold in everyday life. The smartphone and tablet revolution, which was enabled by the wide availability of Internet access, has made life without Information technology urrthinkable. With the Internet of Things on the doorstep, we arc facing the challenge of the exponential growth of the amount of online data. With the advent of Cyber- Physical Systems, computing systems are not just monitoring, but actively controlling parts of the world around us. Finding one's way in the growing amount of data and determining a course of action requires cognitive systems that can make decisions based on available information much faster than humans can.  Yet some of the key technologies that have fuelled the exponential growth of computing systems seem to be heading for a wall dictated by physical limits, creating serious technological challenges for computing power, storage capacity and commu- nication speed: we are nearing the end of the information technology world as we know it. Society has become almost completely dependent on information technology. In reaction to several breaches of trust and integrity, society is increasingly concerned about privacy and security.To address these issues, we need a holistic view of the complete system, both of hardware and software. Designing trustworthy systems will require better security tools. Europe, with its strong presence in software security and with one leading processor architecture is well positioned to play a prominent role here.  Information technology is accelerating the replacement of jobs, particularly middle class ones, by computers and robots. Depen ding on society's a bilityto create new jobs as has ha opened in the past, this may either result in a shift to different jobs or in a growing unemployment rate. However, as compared to the past, the pace of technological development is still accelerating, allowing society less and less time to adapt. This will require considerable flexibility from the educational system, policy makers, legaI frameworks and unions, to name just a few. The human race is increasingly depleting Earth's resources. The production of ICY devices, for example, has an almost insatiable hunger for rare elements.On the other hand, information techno- logy also, increasingly, has the ability to optimize the usage of scarce resources, ranging from real-time tailoring of energy production to consumption, to recycling and the reusing of equipment. Further unleashing the power of information technology on these processes may become possible, again, through the Internet of Things.  Finally, Cyber-Physical Systems and robotic systems, whereby humans, computers and the physical world interact, will enable assisting or replacing human actors In dangerous, tedious, or dirty jobs. These systems still need a considerable amount of research and development, might rely on cloud resources, and will necessitate measures to protect the privacy and security of their users.  CHALLENGES AND ACTIONS DEPENDABILITY, SECURITY Cyber-Physical Systems and the Internet of Things require the highest possible levels of security, safety and reliability for their adoption. In particular,security has to become one of the primary design features of whole systems. Systems have to be dependable by design.  MANAGING SYSTEM COMPLEXITY In information science, we are entering the era of systems of system s,with the a ccom pa nyi n g exponential growth In complexity. We need new system paradigms, such as reactive systems and feedback systems to manage this increase in overall complexity.  POWER AND ENERGY EFFICIENCY We need to overcome energy as the limiting factor for increasing system performance. Instead of over-designing systems and targeting best effort, we need to design methodologies and tools that allow for systems to scale to the desired Quality of Service. We need to investigate methods that allow for information flow and optimization across system layers. Instead of being locked-in silicon-based Von-Neumann a rch itect u res, we need to investigate other architectures based on alternative technologies, such as neuromorphic architectures, Bayes i a n systems, etc.  ENTANGLEMENT BETWEEN THE PHYSICAL AND VIRTUAL WORLD Information systems will sense, monitor, and even control the physical world via Cyber Physical Systems and the Internet of Things{\_} Dealing with large amounts of unstructured data will require new approaches to bridge the gap between the physical world and computer systems. As these systems also exercise control over the physical world, safety is a fundamental concern and becomes a primary design constraint.  MULTIDISCIPLINARY Information systems have moved out of the realm of computer science and moved into the hands of experts in other disciplines. Yet , many systems still need to be adapted to the human-in-the- loop paradigm, letting knowledgeable experts concentrate on the what-to-solve instead of the how-to-solve. This requires a multidisciplinary approach to application development.  HOLISTIC Systems are becoming distributed systems-of-systems of hetero- geneous architectures with a wide variety of applications and communication infrastructure. To make effective and especially efficient use of such complex architectures, we have to develop holistic approaches to system design,allowing for non-functional information flow to enable {\{}dynamic) optimizations.  TECHNOLOGICAL EVOLUTION Several technologies that have fuelled the exponential perfor- mance growth of computer systems in the past decades are facing physical walls. We need to develop new, disruptive techno- logies for efficient data storage, for information processing, and for communication, to sustain this exponential growth. They will, however, need quite some time to become mainstream, so we need to prepare for a period with performance stagnation.  ON POLICY RECOMMENDATIONS RESEARCH PROJECT CONTENT Experience from participants of past and current European projects indicates a concern over the protection of intellectual property rights in projects. Based on those concerns, we recommend careful examination of IP rights in projects, especially in the light of the business models of the project participants. Additionally, IP rights containment is less problematic with a limited number of project participants. Therefore we recommend that some projects can have a limited number of participants, possibly even bilateral concerning IP rights.  Many participants recommend focusing projects on topics that seek to develop basic, non-discriminative technology, or projects that pursue technology development that is too expensive to be carried out by a single company.  PROJECT IMPACT The HiPEAC community is typically involved in development at the bottom of the pyramid, enabling the application of devices and technologies in new end products.  Many factors determine the success of these end products, and it is for that reason often difficult or even impossible to assess the impact at the project's end.  As the instruments and the organization of H2020 deviate considerably from the previous Framework Program, the community is asking for increased guidance to optimize the use of the H2020 instruments and funding.  PROJECT VISIBILITY Compared to physics and astronomy, European projects in the HiPEAC community are virtually invisible to the general public. This, again, is a direct consequence of the position of the very specialized subjects of these projects near the base of the product development pyramid.  To change this situation, we recommend instituting contests, both on a large and on a small scale. These contests should revolve around inspiring, high-visibility subjects, where the large-scale contests typically involve conglomerates of participants, and the small-scale contests would attract smaller groups of people.  In addition, we recommend the use of pre-commercial procure- ment and requirements regulations on consumer products to increase the visibility and use of the results of European projects   }},
address = {Ghent, Belgiom},
annote = {Document in Road2CPS. HiPEAC = High Performance and Embedded Architecture and Compilation
Note the exec summary above. Policy recommendations are uninspiring; create a kind of DARPA, have contests to gain visibility, restrict project participation. But the visibility point is good; Smart City contests would be a good idea.Challenges:DEPENDABILITY BY DESIGNChallenge:
Design systems with dependability in mind starting from the
earliest stages of application development, up to and including
the maintenance stage. We call this challenge "Dependability by
design". It applies from top to bottom to both software and
hardware and to all aspects ot dependability: safety reliability,
security We especially wish to emphasise that security is a high
priority concern that is commonly overlooked during all stages of
system design and implementation.

Solutions:
• Create user awareness regarding security, educate users and developers.
• Make the complete system transparent and auditable: use "open" cores, "open" software.
• Incorporate hardware support for security in all platforms, even in micro-controllers.
• Create hardware and software to support data processing in untrusted environments (e.g. developing homornorphic encryption schemes).
• Add support for expressing security concerns or guarantees in mainstream programming languages{\_}
• Develop tools to automatically protect code and to check tor security leaks{\_}
• Improve low power advanced encryption schemes and (co-)processors.
• Use multiple protection domains, sandboxing„ virtualisation.
• Develop data ananymisers.

Remarks;
• Secure methods are a necessary condition for protecting privacy . A code of conduct about the use of private data should
be rorecognised and enforced.
• Security is an element of guaranteed functional correctness.
• There can be no safety or real-time guarantees without security
• Encryption could be legally enforced for certain services on the Internet and for certain kinds of applications.

MANAGING SYSTEM COMPLEXITY
Challenge:
Manage the complexity of systems that are increasing in size,
distribution, parallelism, compute power, communication,
connectivity and amount of processed data.

Solutions:
• Develop new advanced Models of Computation, Abstractions, and Computing Paradigms to solve the software crisis.
• Explore the path of reactive systems, in particular by applying knowledge from the cybernetics domain concerning the use of feedback loops to stabilise dynamic complex system.
• Develop tools to help developers cope with the complexity of systems, especially with non-functional requirements (timing, power, reliability ...).
• Further develop design space exploration tools that help selecting an optimum in a multi-dimensional solution space.
• Develop tools and approaches to validate complex systems composed of black or grey box components.
• Develop tools to design and test the next generation of hardware platforms, accelerators, 2.5D and 3D integrated
systems, trusted modules ...
• Enable expressing the concurrency in an application (the "what") to improve mapping applications to parallel or distributed systems: compilers and tools are becoming better than humans (at least in specific domains) at these kinds of
transformations (the "how").This could lead to more declarative programming as opposed to imperative programming.

Remarks:
• Developing reactive systems will require a new set of advanced tools, development frameworks, and hardware support.
• Standardisation is an important element of solving complexity. Europe should make more efforts to be part of standardisation efforts in this area.
• When designing advanced Models of Computation, Abstractions and Computing Paradigms, the huge body of legacy systems must also be taken into account.

ENERGY EFFICIENCYChallenge:Energy is currently the limiting factor for performance. If we want
to further increase the performance of systems, we need to find
ways to reduce their energy consumption. E.g., sensors that are
part of the Internet-of-Things may have to operate by scavenging
their power from the environment, significantly limiting their
maximum power consumption.Solutions:
• Move awayfrom over-specifications and best effort approaches:
adapt the hardware to the actually required QoS. Make system
performance scale to demand, develop systems that make
trade-offs between QoS, precision, required amount of data,
energy.
• Develop formalisms, methodologies and tools for "adequate
precision computing", or more generally to deal with "desired
level of Quality of Service": tools that take into account power,
security, time, which use the design-by-contract paradigm,
which can reduce over-specifications, and which can work both
on predictable and on reactive systems.
[Not sure how you can predict thisin advance, given the uncertainites of future uses. Would need to be specified for 'design by contract']
• Develop accelerators for specific tasks, but not limited to number
crunching: for machine learning, for cognitive computing, for
data mining and other data center workloads. These
accelerators must be accompanied by associated formalisms
(languages) and tools so that they can be used easily and
efficiently.
• Ensure interoperability not only across systems, but also enable
cross layer information flows within and between virtual
systems and APIs. The resulting holistic overview will enable
global system and cross-system level optimisation schemes,
especially for managing energy.
• Develop novel, energy-efficient, non-Von-Neumann architec-
tures (neuromorphic architectures, Bayesian, stochastic). Their
efficient implementation may require the application of non-Si
technology.
• Exploit new low power/persistent memory technologies to
build power-efficient memory hierarchies, or to revisit the
memory hierarchy.
• Develop system architectures that limit data movement by
processing the data where it is generated/stored as much as
possible. This is called "Near-Data Computing", whereby com-
puting resources are deployed in the vicinity of the big data.
• Create an ecosystem around 2.5D design with "chiplets" to
stimulate the development of heterogeneous, flexible, energy-
efficient system architectures.

Remarks:
• Optimisation should not only concern computing engines, but
computing and communication. Having local processing will
increase the energy for the compute part, but will reduce the
amount of data to be transmitted.

ENTANGLEMENT BETWEEN THE PHYSICAL AND
VIRTUAL WORLD
Challenge:
The virtual, digital world and the real, physical world become
entangled via the Internet-of-Things and in Cyber-Physical
Systems (CPS). An increasing number of computers directly
observes the physical world, and also controls that world. The
challenge is to correctly observe the physical world, make
decisions and react appropriatelyThis requires an interdisciplinary
approach.

Solutions: [Note the absence of socio-technical thinking]
• Cognitive computing bridges the gap between the natural,
unstructured, non-formalised analog world and the digital
computing realm. New approaches for computing systems will
be required to encompass the requirements of cognitive
systems.
• The design of CPS requires advanced knowledge of mechanical,
chemical, biological, and control processes{\_} This requires
interdisciplinary teams or computing researchers with a
broader training.
• Safety is a very important design criterion for systems that
interact with the physical world (autopilots, pacemakers,
medical scanners ...). While traditional software programs
cause harm when they stop working, crashing CPS systems can
result in disasters. Safety by construction should be mandatory
in such situations.
• Develop tools to help developers cope with the complexity of
systems, especially with non-functional requirements (timing,
power, reliability ...).

Remarks:
• CPS that make autonomous decisions like autopilots and, by
extension, also the cognitive systems they rely on, will require
legal and ethical frameworks in which they can make decisions.
Technical solutions that can enforce these limits will be
required once society lays down the rules.
• Current CPS interact with their environment, but 3D printers
can even create new objects in the physical world that are
conceived in cyberspace.

Secondary challenges
MULTIDISCIPLINARY
Challenge:
Future applications will be so complex (compute intensive,
connected, sensorial, cognitive) that they will require a
multidisciplinary approach. Some components will not even be
IT-components (human body, combustion or electric engine, part
of a factory, robot ...). This will require a multidisciplinary view
incorporating aspects from physics, medicine, biology and
psychology. A challenge is to open up the IT tools to the experts in
these other domains. For example, data analytics is the domain of
statisticians as opposed to computer scientists, pace makers are
programmed by cardiologists, and so on. We need direct, close
collaboration between experts from different domains to design
the appropriate hardware and software.
Solutions:
• Make information science tools easier to use by people that are
not IT-specialists. One approach could be using high-level
languages with large libraries such as Python and tvlatlab
instead of C++.
• Develop approaches that enable domain specialists to express
only what needs to be done, while tools and computers take
care of how to transform this knowledge into an efficient
computing system representation.
• Take the human factor into account at all levels: from interfaces
to reaction time, from what is really required from a human
point of view to what is acceptable in terms of cost, lifetime,
reliability, safety, privacy.
• Make computer science education more multidisciplinary,
exposing students to core knowledge from other science
disciplines in the context of their computer science education:
biology, physics, basic engineering and design.

Remarks:
• One possible explanation for the success of vertical companies
(e.g. Apple, Samsung, Google) is that their employees have
many different backgrounds (not just computer science) due to
the large scope of activities of those companies, and they can
leverage this broad knowledge base while working towards
common goal.
• The success of "interpreted" languages such as Python, Mathlab
is certainly due to their immediate result, without the edit-
compile-execute-debug cycle and their adaptation to specific
domains by easy addition of libraries.

HOLISTIC
Challenge:
Future applications will be distributed, requiring different and
multiple, interconnected computing engines, using different
software and hardware stacks. As a result, their optimisation will
require a holistic approach. Every component will only be a small
part of the solution, and even if each component is optimised
individually their combination will be not operate optimally
because local optimisations may impose penalties on other
components. Additionally, the extra compatibility layers and
translations required to make all components interoperable will
also have their cost. Therefore, we need close collaboration to
design the appropriate hardware and software, and a holistic
view of the complete system in order to make it efficient. Due to
the combinatorial explosion of possible combinations of different
structures and devices, only dynamic real-time adaptation and
optimisation will be have a chance at succeeding.

Solutions:
• Develop approaches (= architectures, tools, abstractions) that
take into account non-functional information (such as
temperature, energy consumption and management, wear,
ageing, errors) at all levels of the applications, allowing for
making decisions throughout all levels of the stack rather than
only at the lower levels. This should be performed ensuring a
high level of interoperability (thereby developing — de-facto —
standards) and security (keeping in mind potential misuse of
this information). In many cases, a dynamic run-time approach
may be the only viable approach{\_}
• Further develop design space exploration tools that allow
mapping the holistic complexity of a system onto simple
decisions.
• Develop tools and approaches to validate complex systems
composed of black or grey-box components (hardware and
software).

Remarks:
• Creating a European ecosystem in which the participants can
complement each other's technologies and solutions will
require a strong willingness to break out of the existing "ivory tower" environments.

TECHNOLOGICAL EVOLUTION
Challenge:
Information systems technology hardware is tightly coupled to
exponentially growing aspects, such as integration density
(Moore's law), communication bandwidth (Nielsen's law), storage
(Kryder's Law), and also to a decreasing device cost (transistors).
These "laws" are running into physical limits that slow down the
expected performance increases{\_} The challenge is to break
through these walls. Fortunately, new technologies can help us
overcome the upcoming roadblocks that hamper existing
technologies. They will, however, need significant development
before they can become mainstream, which may result in a
period of performance stagnation.

Solutions:
• For new storage technologies: new non-volatile memories.
• For new compute technologies: carbon nanotubes, graph ene.
For some applications: neural networks based on new devices
("synapstors")
• For communication technologies: photonics and 5G.
• To cope with the increasing cost of developing ASICS with the
latest technology node: assembling chiplets using various
technologies on an interposer (active or passive, using silicon or
organic materials). This may allow for design diversity in order
to reduce costs, by reserving the most advanced nodes for the
high performance parts of the systems (compute chiplets).
• Another approach to keep design diversity are new (coarse
grain) reconfigurable fabrics that take advantage of new
technologies.

Remarks:
• New technologies will have a drastic impact on system
architecture. For example,the availability of small and fast non-
volatile memories could change the memory hierarchy. New
reconfigurable devices using mixed technologies (silicon and
non-volatile) could reach higher performance and thereby
become suitable for wider markets. Even if the technologies are
immature, computer scientists and architects should already
analyse the impact they may have on existing system
architecture.

Sustainability aspects:
The number of mobile phones is expected
to exceed the world population this year PTC13]. Then, the
total power consumption of all the mobile phones in the
world, assuming there are 7 billion people on Earth, is 3{\_}5 GW.
The total amount of power consumed globally by end user
devices (PCs, 1-Vs, mobile devices) is estimateo to lie between
so and 14.0 GW.The total amount of power used by the whole
ICT ecosystem, including data centers, communication
infrastructure, and ICT device production, is estimated
between 1250 and 3200 GW. (The uncertainty comes from the
many assumptions made, and the ongoing shift of IV into the
digital ecosystem.) Compared to these numbers., mobile
devices account for a mere 0.3{\%} to al{\%} of the total digital
ecosystem's power consumption{\_} (See [http://www.tech-pundit.com/wp-content/uploads/2013/07/Cloud{\_}Begins{\_}With{\_}Coal.pdf?c761ac])

Urban mining techniques can increase the availability of raw
materials. Such techniques extract minerals from electronic
waste instead of from ore [UrbianMiningi. One metric ton of
electronic waste can contain up to 300-400 gram of gold, which
is up to 50x richer than ore. Urban mining is one of the key
technologies to make the electronics industry more sustainable.

Homomorpic computing and security/privacy:[Note: Microsoft stopped supporting security patches for Windows XP when more than 25{\%} of computers were still running this OS. http://www.dailytech.com/Report+Windows+XP+Still+Running+on+Over+25+Percent+of+PCs+/article34627.htm]
New disruptive technologies, like non-volatile storage, can easily
change the landscape. What if we can have several Exabytes of
storage for a cheap price in 10 cm3? Perhaps users and companies
would prefer to store their own data in a device they own and of
which they know the location. In that case, only more advanced
functionality will be distributed and will only have access to the
data (or metadata) it needs{\_} Such individual data stores that are
globally coupled are called federated, or distributed, clouds. [c.f. Named Data Networks - NDN. These require NDN Forwarding Daemons - NFDs; see Broker Architecture gap.]
While remote applications will need data from these private data
stores in order to perform the requested tasks, they should not
get access to all raw data. After all, in that case we would again be
stuck with the problem of data confidentiality as it exists today
with unified clouds. Instead, they should only be provided with
the information required to perform the task. This information
could moreover be anonymised, or be limited to statistical or
other metadata, thereby abstracting the real information from
the user. Therefore, reliable anonymisation and anonymising
statistical abstraction of information is a necessary feature for
the concept of federated clouds to take off.

Another emerging approach is to send encrypted data to the
remote application. The application then performs its operations
without ever decrypting the data. As a result, the application
never knows the actual data nor the meaning of the results it
computes. This process is called homomorphic encryption{\_} It is
the ultimate solution for keeping data private, but it runs
completely counter to the current business model of companies
like Facebook and Google. After all, they are built on gathering
and reselling as much information about their users as possible.Big Data:
In 2010 the world generated over 1.2 Zetta bytes [1.2 x 10**21 bytes] of
new data, so{\%} more than everything produced in human
history beforethatyear To put this in perspective,120 Terabytes
of new data was generated in the time it took to read the
previous sentence. For example, Microsoft Update and
Windows Update push out more than a Petabyte of updates
monthly (it should be noted that th is is essentially a broadcast:
the updates are new data, but all of the individual copies
aren't). A social network like Facebook produces more than
10TB of data per day, with Twitter not far behind (7 TB/day);
each of the 4.6B mobile phones and 30B RFID tags produces
several events per second that need to be stored, processed
and analysed. Likewise, the 2B Internet users also generate a
variety of events that can have important value in areas like
statistics, demographics or marketing{\_} And the soB connected
devices expected by the year 2020 will cause all of the
previously mentioned figures to balloon even further. Domains
like gaming and other virtual worlds or augmented reality are
also turning into massive data management problems.
In the scientific community, discovery has become a data-
driven process, which represents a relatively new fourth
paradigm in science [Hey09], next to the empirical, theoretical
and computational models. The problem is that with current
increases in computation, it may take over a decade to gain
some understanding of what has already been archived from
the most important scientific experiments.

'Dark data'
According EMU/IDC the digital universe will grow from 10 000
Ettabytes (EB) in 2014 to 40 000 EB in 2020 lAilamaki14].We have reached
the point whore sensors can produce much more data than
can ever be processed, and we have to ask the question: how
much data do we really need to answer a particular question?
Do we need to know the key weather parameters of every
square kilometre each minute to make a good weather
forecast, or is it sufficient to have just 0.l{\%} of this information,
but well-chosen in space and in time. If Moores law is really
about to end, the future challenge (before new technology
will re-enable the growth in performance for less power and
area) will no longer be how to do more with more computing
power, but how to do more with the same computing power.
The big data com},
author = {Duranton, M and de Bosschere, K and Cohen, A and Maebe, J and Munk, H},
keywords = {AI,BPR,CSCW,Cyber-physical systems,Governance,IoT,OKP,agility,aging,allocation of functions,architecture,assembly,benchmarking,business process re-engineering,circular economy,communications,competitive challenges,consequences,control,core competences,decision-making,definitions,design process,empowerment,engineering,expertise,federated control,harvard business school,holons,information systems,interface design,job design,knowledge,knowledge configuration,legal,legislation,metrics/metrication,multi-niche marketing,networks,org. trust,philosophy,product introduction process,production control,quality,reference architectures,reliability,resilience,review,robots,safety,security,smart city,standards},
pages = {72},
publisher = {HiPEAC Network of Excellence},
title = {{HiPEAC vision 2015}},
url = {http://hdl.handle.net/1854/LU-6868628},
year = {2015}
}
@article{Bastian2009,
abstract = {Gephi is an open source software for graph and network analysis. It uses a 3D render engine to display large networks in real-time and to speed up the exploration. A flexible and multi-task architecture brings new possibilities to work with complex data sets and produce valuable visual results.¬† We present several key features of Gephi in the context of interactive exploration and interpretation of networks. It provides easy and broad access to network data and allows for spatializing, filtering, navigating, manipulating and clustering. Finally, by presenting dynamic features of Gephi, we highlight key aspects of dynamic network visualization.},
author = {Bastian, Mathieu and Heymann, Sebastien and Jacomy, Mathieu},
doi = {10.1136/qshc.2004.010033},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bastian, Heymann, Jacomy - 2009 - Gephi An Open Source Software for Exploring and Manipulating Networks.pdf:pdf},
isbn = {978-1-57735-421-5},
issn = {14753898},
journal = {Third International AAAI Conference on Weblogs and Social Media},
keywords = {graph explor,network,network science,visualization},
pages = {361--362},
pmid = {1000044483},
title = {{Gephi: An Open Source Software for Exploring and Manipulating Networks}},
url = {http://www.aaai.org/ocs/index.php/ICWSM/09/paper/download/154/1009 http://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154{\%}5Cnpapers2://publication/uuid/CCEBC82E-0D18-4FFC-91EC-6E4A7F1A1972},
year = {2009}
}
@misc{Sinclair2017,
abstract = {This spreadsheet contains the core data used to generate the Gephi diagrams. The table is large and is best read in conjunction with diagrams. The table was generated by asking project members to classify the projects and identify the gaps and relationships. The output was hand checked for consistency before processing. The process is described in the paper cited below.},
author = {Sinclair, Murray and Palmer, Paul and Siemieniuch, Carys and Henshaw, Michael},
doi = {10.17028/RD.LBORO.5082082},
month = {jan},
title = {{Spreadsheet of Gaps and Impacts}},
year = {2017}
}
@article{Shirk2012,
abstract = {Members of the public participate in scientific research in many different contexts, stemming from traditions as varied as participatory action research and citizen science. Particularly in conservation and natural resource management contexts, where research often addresses complex social-ecological questions, the emphasis on and nature of this participation can significantly affect both the way that projects are designed and the outcomes that projects achieve. We review and integrate recent work in these and other fields, which has converged such that we propose the term public participation in scientific research (PPSR) to discuss initiatives from diverse fields and traditions. We describe three predominant models of PPSR and call upon case studies suggesting that-regardless of the research context-project outcomes are influenced by (1) the degree of public participation in the research process and (2) the quality of public participation as negotiated during project design. To illustrate relationships between the quality of participation and outcomes, we offer a framework that considers how scientific and public interests are negotiated for project design toward multiple, integrated goals. We suggest that this framework and models, used in tandem, can support deliberate design of PPSR efforts that will enhance their outcomes for scientific research, individual participants, and social-ecological systems. {\textcopyright} 2012 by the author(s).},
author = {Shirk, Jennifer L. and Ballard, Heidi L. and Wilderman, Candie C. and Phillips, Tina and Wiggins, Andrea and Jordan, Rebecca and McCallie, Ellen and Minarchek, Matthew and Lewenstein, Bruce V. and Krasny, Marianne E. and Bonney, Rick},
doi = {10.5751/ES-04705-170229},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Shirk et al. - 2012 - Public participation in scientific research A framework for deliberate design.pdf:pdf},
issn = {17083087},
journal = {Ecology and Society},
keywords = {Citizen science,Community-based monitoring,Conservation,Outcomes,Participation,Public,Volunteer monitoring},
number = {2},
pages = {20},
publisher = {Resilience Alliance},
title = {{Public participation in scientific research: A framework for deliberate design}},
volume = {17},
year = {2012}
}
@inproceedings{Oliveira2009,
abstract = {This paper presents the SmartClean tool. The purpose of this tool is to detect and correct the Data Quality Problems (DQPs). Compared with existing tools, SmartClean has the following main advantage: the user does not need to specify the execution sequence of the data cleaning operations. For that, an execution sequence was developed. The problems are manipulated (i.e., detected and corrected) following that sequence. The sequence also supports the incremental execution of the operations. In this paper, the underlying architecture of the tool is presented and its components are described in detail. The tool's validity and, consequently, of the architecture is demonstrated through the presentation of a case study. Although SmartClean has cleaning capabilities in all other levels, in this paper are only described those related with the},
author = {Oliveira, Paulo and Rodrigues, F{\'{a}}tima and Henriques, Pedro},
booktitle = {2009 Ninth International Conference on Quality Software},
doi = {10.1109/QSIC.2009.67},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Oliveira, Rodrigues, Henriques - 2009 - SmartClean An Incremental Data Cleaning Tool.pdf:pdf},
isbn = {978-1-4244-5912-4},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {aug},
pages = {452--457},
publisher = {IEEE},
title = {{SmartClean: An Incremental Data Cleaning Tool}},
url = {http://ieeexplore.ieee.org/document/5381543/},
year = {2009}
}
@article{Zappia2018,
abstract = {Clustering techniques are widely used in the analysis of large datasets to group together samples with similar properties. For example, clustering is often used in the field of single-cell RNA-sequencing in order to identify different cell types present in a tissue sample. There are many algorithms for performing clustering, and the results can vary substantially. In particular, the number of groups present in a dataset is often unknown, and the number of clusters identified by an algorithm can change based on the parameters used. To explore and examine the impact of varying clustering resolution, we present clustering trees. This visualization shows the relationships between clusters at multiple resolutions, allowing researchers to see how samples move as the number of clusters increases. In addition, meta-information can be overlaid on the tree to inform the choice of resolution and guide in identification of clusters. We illustrate the features of clustering trees using a series of simulations as well as two real examples, the classical iris dataset and a complex single-cell RNA-sequencing dataset. Clustering trees can be produced using the clustree R package, available from CRAN and developed on GitHub.},
author = {Zappia, Luke and Oshlack, Alicia},
doi = {10.1093/gigascience/giy083},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zappia, Oshlack - 2018 - Clustering trees a visualization for evaluating clusterings at multiple resolutions.pdf:pdf},
issn = {2047217X},
journal = {GigaScience},
month = {jul},
number = {7},
pmid = {30010766},
title = {{Clustering trees: a visualization for evaluating clusterings at multiple resolutions}},
volume = {7},
year = {2018}
}
@inproceedings{10.1007/978-3-540-39964-3_39,
abstract = {Large-scale, dynamic and open environments such as the Grid and Web Services build upon existing computing infrastructures to supply dependable and consistent large-scale computational systems. This kind of architecture has been adopted by the business and scientific communities allowing them to exploit extensive and diverse computing resources to perform complex data processing tasks. In such systems, results are often derived by composing multiple, geographically distributed, heterogeneous services as specified by intricate workflow management. This leads to the undesirable situation where the results are known, but the means by which they were achieved is not. With both scientific experiments and business transactions, the notion of lineage and dataset derivation is of paramount importance since without it, information is potentially worthless. We address the issue of data provenance, the description of the origin of a piece of data, in these environments showing the requirements, uses and implementation difficulties. We propose an infrastructure level support for a provenance recording capability for service-oriented architectures such as the Grid and Web Services. We also offer services to view and retrieve provenance and we provide a mechanism by which provenance is used to determine whether previous computed results are still up to date.},
address = {Berlin, Heidelberg},
author = {Szomszor, Martin and Moreau, Luc},
booktitle = {On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE},
editor = {Meersman, Robert and Tari, Zahir and Schmidt, Douglas C},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Szomszor, Moreau - 2003 - Recording and Reasoning over Data Provenance in Web and Grid Services.pdf:pdf},
isbn = {978-3-540-39964-3},
pages = {603--620},
publisher = {Springer Berlin Heidelberg},
title = {{Recording and Reasoning over Data Provenance in Web and Grid Services}},
year = {2003}
}
@techreport{Lavery2013,
abstract = {Manufacturing generates directly 10{\%} of the UK's GDP and employs 2.5 million people (9{\%} of the employed labour force).  Labour productivity improvements in the sector have reduced labour costs since 2001 at a rate of 3{\%} p.a. to {\pounds}75bn in 2011, a reduction of 1,000,000 jobs.   Conversely, despite spending {\pounds}340bn annually on goods, materials and services (i.e. non-labour resources), these costs have been rising for UK manufacturing sector by 0.4{\%} p.a. since 2004, adjusted for inflation and production volumes.    While UK manufacturers have made good progress in some non-labour resource productivity areas, such as recycling and waste to landfill, significant inefficiencies remain. For example:  •  Remanufacturing is below 2{\%} for most non-perishable/non-consumable products •  27{\%} of freight truck journeys are running empty •  Despite the logical benefits of optimising along the supply chain, few UK manufacturers have been engaging in collaborative discussions with their suppliers •  Many companies have achieved 10 to 15{\%} efficiency gains over the last decade, however leading companies have achieved over 50{\%} improvements in the same timeframe. This study presents opportunities to improve non-labour resource productivity which could enable a revolution in manufacturing and are estimated, conservatively, to be worth for the UK:  •  {\pounds}10 billion p.a. in additional profits for manufacturers – a 12{\%} increase in average annual profits. •  314,000 new manufacturing jobs - a 12{\%} increase in manufacturing employment. •  27 million tonnes of CO2 equivalent p.a. greenhouse gas emissions reduction – 4.5{\%} of the UK's total greenhouse gas emissions in 2010.  Additional benefits to the community include indirect employment, improved national resource security (including energy, food and raw materials), reduced pollution, reduced need for landfills, less traffic congestion, reduced load on energy and transport infrastructure (reducing investment and maintenance spend), improved prosperity in the UK agricultural sector, and economic development in developing nations who supply UK manufacturers.    These benefits come from four types of improvement (Incremental Improvement, Process {\&} System Improvement, Structural Change and Core Redesign) within the resource productivity fields of circular resource use, energy efficiency, process waste reduction, packaging optimisation, transport efficiency and supply chain collaboration. While some of the identified opportunities are well established, others such as supply chain collaboration are new or are the result of recent technology development or business model innovation.   Potentially, all manufacturing companies can benefit from the identified opportunities; the benchmark database developed for this study revealed significant variation in non-labour resource performance between companies within each manufacturing sub-sector. Even pioneering companies leading in one or several areas were found to perform inconsistently across the topics examined.   Eight major barriers to non-resource productivity were found. Four of these apply to most opportunities: senior executive leadership, information, skills and resources. The other four barriers relate to specific types of opportunities and also warrant attention: design, infrastructure, legal constraints and collaboration.   A programme to address these barriers has been developed in consultation with a range of multinational manufacturers, relevant government departments, NGOs and experts around the world. It comprises three streams:  1  Establishment of an NMR Community, for broad engagement and education. This will provide in-depth information, research, tools and interactive information exchange forums. This is designed to build skills and awareness, while inspiring senior executive action. The NMR Community will be openly accessible to all, using the Next Manufacturing Revolution website and the 2degrees platform currently serving over 31,000 members. 2  Tailored support, providing assistance for individual organisations. The Next Manufacturing Revolution founding members will work with established manufacturers to identify opportunities for resource productivity improvements, help to construct the investment case for these, and engage senior executives. This will help develop opportunity awareness and provide access to the necessary skills. 3  Barriers resolution and rollout. While the above streams begin to address the key barriers, more concerted action is required to eliminate them. This will require collaboration amongst the various stakeholders who can together overcome the barriers to improving non-labour resource productivity. A series of workshops, consulting widely to understand all of the issues and then focussing within small group of senior experts from business, government, NGOs, and academia, will begin in the second half of 2013. The outcome from each will be a plan of action with agreed milestones and commitment to address the barriers.  Implementation of this programme is now underway; collaboration with government, member organisations, other NGOs and established manufacturers will assist to accelerate this programme. The Next Manufacturing Revolution welcomes such organisations seeking to participate.    This study is based on input from global experts, multinational corporations, an extensive literature review and a limited sample survey of manufacturers. It has also been peer reviewed by over 40 reviewers.  },
address = {Cambridge, UK},
annote = {Document in T-AREA-SoS
Paper in T-AREA-SoS (I think). Note the quote by ACM Sir Sidney Dalton; "People are the means by which technology is transformed into capability". Apart from criticising CEOs, this is what is missing from this report.

Lots of examples to quote.

p.36 Barriers:
Senior executive leadership.
The effort to pursue all attractive opportunities and the behaviour change necessary to achieve a company's full potential requires senior executive support for resource (including energy) efficiency. Without it, ‘business as usual' continues because fully addressing energy efficiency requires: 
• Action by multiple functions and all divisions within an organisation (and therefore requires the reach of senior executives)
• Changed behaviours and cultures (best led by the most senior executives)
• Investment in skills and equipment (often requiring approval from senior executives)
• Commitment backed up by performance measures, including KPIs, cascaded through the organisation (which can only be initiated by senior executives)

The primary importance of senior executive leadership was explicitly noted in the peer review process for this study and in the literature, because without it, all of the other barriers are seen to be much more difficult to address. 
{\ldots}
While middle management is technically equipped to address resource conservation, in many cases it can be resistant to change because of a range of structural factors: 
a) Decades of downsizing means that middle managers have limited bandwidth for non-core initiatives, as well as keeping up with the latest technologies and approaches
b) They have tightly defined, bonus-linked key performance indicators which mostly drive short-term behaviour and a heightened sensitivity to risk
c) They perceive, often correctly, that the systems they manage are optimised; they do not have decision rights over multiple systems or core product redesign to be able to capture greater savings
d) Savings that require co-operation between often distant middle managers, such as facilities managers and production managers, are always challenging
e) Leading a culture change initiative (required for incremental energy efficiency savings) can be perceived as a threat to the authority of senior executives

Resource efficiency therefore needs to be addressed, or at the very least initiated by, the CEO/COO/CFO. Historically, it has been fragmented into its constituent elements (energy, waste, etc.) resulting in smaller benefits, which are less worthy of CEO attention and seen as technical issues which fall within the remit of middle management. Further, the CEO requires conviction and external evidence (information) of the opportunities available, because the technical advice from internal management will be biased against change for the above reasons. 
...
Resources
Common energy efficiency funding issues include: 
• Companies having cash limits and not being prepared to consider alternative financing, despite attractive returns
• Unrealistically high hurdle rates being applied, despite the low and controlled risk involved in energy efficiency projects
• All benefits (including, for example, the value of greenhouse gas emissions savings) not being included in business cases
• Efficiency projects are deprioritised in favour of other investment options (such as plant expansion or new product development).
These barriers must all be addressed if the potential of energy efficiency is to be achieved. All are interrelated and mutually reinforcing; to address only some is not sufficient (see Figure 19). 

A range of other issues have been suggested as barriers, but are considered to be symptoms of the above-discussed four barriers, rather than barriers in their own right. These include: 
• Lack of time (a function of lack of senior executive leadership and resources)
• Lack of interest/prioritisation (a function of lack of senior executive leadership)
• Product availability (a function of lack of information)
• Lack of skilled personnel (a function of inappropriate skill mix, lack of information and/or inadequate resources to afford the necessary expertise)

p.40 Waste reduction:
Total UK manufacturing waste was 22.7 million tonnes in 2008. 

From 2002 to 2009 in England, total waste halved and waste sent to landfill dropped by two thirds, driven in large part by legislation and significant increases in landfill taxes which have made landfilling the most expensive disposal option for non-hazardous waste. 

However, substantial profitable waste reduction opportunities worth {\pounds}800 million p.a. remain: 
• Four manufacturing sub-sectors (food, beverage {\&} tobacco; wood and wood products; non-metallic mineral products, and; furniture and other manufacturing) reduced their total waste by less than 20{\%} from 2002 to 2009. Bringing these to best practice is estimated to save {\pounds}480 million p.a. in avoided material costs.
• Zero waste to landfill has now been achieved by leading companies in most manufacturing sub-sectors; taking companies to this level would save the UK manufacturing sector {\pounds}250 million every year in avoided landfill costs (which is in addition to the avoided materials costs above).
• Best practice companies are achieving healthy revenues from the sale of their waste. Achieving these across the board would result in additional company profits of {\pounds}70 million per annum.

Adopting good practice in process waste reduction and management would reduce greenhouse gas emissions by 2.6 MtCO2e per year through avoided embodied emissions in wasted product and avoided landfill methane. 

Five inter-related barriers exist to substantially reducing process waste: senior executive leadership, information, product design, skills and infrastructure. All of these must be addressed simultaneously to unlock the benefits available. 
{\ldots}
p. 46 While the performance of many of these businesses is laudable, a range of companies have achieved zero waste to landfill, at least at a number of sites. Locally, these include Tarkett, Nampak Plastics, British Gypsum, Pepsico UK and Ireland at 11 sites, Toyota Motor Europe at 8 sites, Caterpillar at 2 UK sites, and FMC Technologies at their Dunfermline plant in Fife63. Internationally, General Motors has achieved zero waste to landfill at 102 sites globally and claims that it now generates {\$}1 billion a year from the reuse and recycling of production by-products64. {\ldots} Toyota Motor Europe reduced its total waste per vehicle by 70{\%} in ten years65

p.72 Re-use, recycling [see attached figure for Ricoh]:
includes product reuse, remanufacturing, cascaded use, recycling and recovery138.
• Reuse – redeploying a product without the need for refurbishment – e.g. second hand motor vehicles.
• Remanufacturing – returning a product to the Original Equipment Manufacturer (OEM) performance specification and giving a warranty close to that of a newly manufactured equivalent – e.g. Caterpillar has a successful engine remanufacturing business.139
• Cascaded use - using a product for a lower value purpose – e.g. turning used clothes into pillow stuffing. This can occur within the operations of a customer, for example where computers are redeployed within a company for less demanding applications.
• Recycling – extracting a product's raw materials and using them for new products – e.g. aluminium and steel are widely recycled.
• Recovery – using a product's materials for a basic, low value purpose such as road base or combustion to produce heat. 

While recycling rates are high in the UK, there is minimal activity in higher value circular resource flows – e.g. remanufacturing accounts for just 1{\%} of UK manufacturing sector turnover. 
This is despite global pioneers in remanufacturing capturing 95{\%} of accessible products and using them to generate substantial additional profits. They succeed by retaining control of their products, managing them while in use, resolving reverse logistics challenges, building remanufacturing capabilities and designing for longevity and circular resource use. 

Manufacturing sub-sectors that offer the greatest opportunities to capture significant value from circular resource use are: 
• Electrical, electronic and optical products
• Machinery and equipment
• Transport equipment

For just these three sub-sectors, remanufacturing can create {\pounds}5.6bn to 8bn p.a. of value for manufacturers, support over 310,000 new jobs that are skilled and rewarding and reduce UK greenhouse gas emissions. 

The key barriers to achieving these remanufacturing benefits are: senior executive leadership, information, skills, design, infrastructure, legal constraints and collaboration. 

Use of recycled/renewable153 material by manufacturers varies substantially between sectors and companies within sectors (see Figure 47)154. For paper, recycled content varies from 30{\%} to 100{\%}, aluminium from 8{\%} to 67{\%}, and steel from 35{\%} to 52{\%}. 
• Cisco (electrical, electronic and optical sub-sector) reused 45{\%} of returned equipment in 2008, creating additional profit of {\$}100million158
• In 2009, Caterpillar remanufactured over {\pounds}130million worth of material159
• Renault's remanufacturing business generates €200million in revenue per annum160

Control of a product beyond its life with the first customer is important to enable the manufacturer to secure the return of valuable resources/parts/equipment at low cost162. To do this a range of business models have been developed, including offering services instead of products (e.g. power instead of aircraft engines), leasing (e.g. of photocopiers), incentivised return and reuse (e.g. deposit schemes, discounts on replacements), ownership of waste streams (e.g. refuse contracts) and collection schemes (e.g. for specific products such as paper/cardboard). 

Good product design can substantially reduce the cost of product circularity. Aspects include developing ‘timeless' designs, choosing recyclable materials, reducing the number of materials, ensuring interchange-ability of parts between models, designing new products to re-use old components, designing for disassembly, optimising product designs for multiple life-spans, ensuring products can easily be cleaned, incorporating self-diagnosis, improving maintenance access and designing in flexibility for future upgrades and retrofits. Most companies currently design for fashion and obsolescence which suits a linear product model where maximising one-time revenues is the aim. 
• Ricoh, who note that their remanufactured photocopiers have a manufacturing greenhouse gas emissions footprint 90{\%} lower than the equivalent new copier169
• The EEF has noted that remanufactured products such as diesel engines can have 85 per cent lower energy costs and 60 per cent lower material costs than new counterparts171.
• The Advanced Remanufacturing and Technology Centre in Singapore notes that, compared with new components, remanufactured components save {\textgreater}60{\%} energy, {\textgreater}70{\%} materials, {\textgreater}90{\%} water, and {\textgreater}80{\%} pollutant emissions, while retaining {\textgreater}80{\%} of the product's value172.
• The US Motor {\&} Equipment Manufacturers Association testified to the International Trade Commission on Remanufactured Goods that: “Remanufacturing saves 80{\%} of the energy and material used to manufacture equivalent new parts”173.
• GKN reuses 80{\%} of the steel in recovered driveshafts through remanufacturing175.
• Edwards remanufactures over 90{\%} by weight of a pump's components – with a corresponding saving in raw materials and their embodied carbon dioxide176.

p.90 Barriers to recycling
• Senior executive leadership. Changing the perceptions of an entire organisation and its customers is a difficult task requiring the skills, experience and influence of senior executives. Significant resources may be required to establish new capabilities and/or infrastructure (e.g. reverse logistics). Only senior executives can drive a change of culture and/or business model, should this be necessary (e.g. to a service or leasing model). Changing a business model challenges existing business practices. For example, amongst all of the departments in a company who must retune their approach for remanufacturing, one of the most important is the sales team – the face of the company and the key to the success of a product. Salespeople prefer to sell what they know using tried and proven techniques to hit their short term sales targets. New offerings and new customer segments present risks to salespeople and therefore require sales training and incentives to shift attention to remanufactured products and address an often hard-wired belief that remanufactured products are a threat to their new sales commissions. Senior executive leadership is vital to achieving this shift. 
• Awareness
• Inconsistent use of the term 'remanufacturing'
• Skills. Very few people have developed circular resource use companies from start-up; even fewer have transformed a large company from a pre-existing linear model because very few companies have undergone this transition. The skills required include strategic thinking, engineering, marketing, logistics, process design, and change management.
• Design. Product design plays a crucial role in the success of manufacturing because it directly impacts on the ability of a company to monitor, disassemble, inspect, and reassemble remanufactured products. All of these impact labour cost and hence heavily affect the economics of remanufacturing (and recycling). Difficult disassembly, irreversible closures, tight access, specialised tool requirements, multiple materials (especially plastics), surface coatings, glues and labels, and insufficient materials information all need to be avoided through good design.
• Recovery infrastructure. [Processes, material diversity, availability of channels and flows through them]
• Regulatory impediments. [e.g. banning remanufactured parts in new goods]
• Access to product information
• Changing the throw-away culture200
• improving the image/perceptions of ‘remanufactured'.
p.95 Transport efficiency:
Freight transport, worth {\pounds}29.5 billion in 2011, has seen no significant improvement in efficiency over the last decade. The manufacturing sector can impact a significant proportion of this transport spend through influencing suppliers' transport choices, making decisions themselves for trans-shipping and haulage to customers and affecting downstream transportation with the weight and form of their products. 

60{\%} to 70{\%} of goods transport in the UK is by road - the most expensive (both economically and environmentally) surface transport mode; road freight transport was therefore the focus of this discussion. 

Good practice manufacturers around the world have achieved a 36{\%} improvement in their transport greenhouse gas emissions202 which, if adopted in the UK, would save {\pounds}650 million per annum203 for manufacturers, avoid 3 MtCO2e p.a. of greenhouse gas emissions, reduce NOx, SOx and particulates by 36{\%}, and improve national productivity through reduced traffic delays. The greatest savings are expected in the road transport-intensive Food, Machinery, Transport Equipment and Manufactured Articles sub-sectors. 

The key barriers to achieving these transport efficiency benefits - senior management leadership, information, resources and appropriate skills - are common with Energy Efficiency. Additional barriers include legislation and the need for greater collaboration; improved infrastructure is also required to be able to quickly and efficiently switch modes. 

p.110 Supply chain collaboration:
For example, Tesco plc recently announced a major change in the way they do business to incorporate a new core value: ‘Using our scale for Good.' Central to this is how they work in partnership with suppliers to deliver the innovation needed to meet customer needs and make a difference to society. Tesco have set up an online supplier community to support collaboration, called the Tesco Knowledge Hub254 that has as its goal a 30{\%} carbon reduction in products sold by Tesco255. The majority of Tesco's top 1,000 suppliers are active members of this community. 

Cross-boundary optimisation: A study of the upstream energy usage for Walkers Crisps (owned by PepsiCo) identified that substantial energy was being used by potato farmers humidifying their produce in order to maximise their revenues because they were paid by weight. Walkers then spent 10{\%} of their energy in the potato frying step removing this additional water. Changing the contract eliminated this misalignment, saving the supply chain {\pounds}1.2 million p.a.259 

Best practice sharing: Cider maker Bulmers has invested in an orcharding team which provides expertise to all contract apple growers. For example, they provide timely information about growing conditions and pest management each season. This enables farmers to minimise spraying against fungi and insects – reducing costs and environmental impact265. 

British Sugar, working with the National Farmers Union, has increased the yield of sugar beet crops in the UK by 75{\%} since 1980284. Since 1982, pesticide application has reduced by 60{\%}, nitrogen by 40{\%} and phosphate application by 70{\%}285.

In 2011, UK manufacturers spent {\pounds}340 billion on goods, services and materials257. These non-labour inputs were dominated by flows from other domestic manufacturers (23{\%}), offshore manufacturers (22{\%}), and wholesale and retail (17{\%}) which comprise mostly manufactured goods. Secondary resource inputs were from mining {\&} quarrying (7{\%}), agriculture (4{\%}), utilities (4{\%}) and transport (4{\%}) (see Figure 66). These inputs offer opportunities for improvement through supply chain collaboration – a total of 81{\%} of spend on goods, services and materials amounting to {\pounds}278 billion in 2011. 

p.130 Revenue growth:
Revenue growth occurs (through price uplift or increased volume) when a manufacturer creates value for customers. This can occur through non-labour resource productivity at various stages in a product's life cycle, including: 
• Product manufacture (making more resource efficient products that have higher customer preference)
• Product deployment (efficient delivery models such as Product Service Systems/'servicising')
• Improving product-in-use efficiency (by reducing either cost/impact per usage, or overall usage and hence increasing customer preference and saving operating costs)
• Re-use of products (including through new business models such as collaborative consumption)
• Remanufacturing (as discussed in the Circular Resource Use chapter)

* Rolls-Royce ‘Total Care', which charges customers on a {\$} per engine flying hour basis. This fee covers services such as predictive maintenance as well as repair and overhaul activities. It delivers higher reliability and ‘time on wing' thus improving resource productivity per engine while adding value for customers through avoided breakdowns and delays313.
* Many paint companies have moved from supplying paint to automotive Original Equipment Manufacturers (OEMs) to providing automotive finished coatings. Because the paint manufacturers are paid per vehicle, they have developed new application systems (such as heat cured powder coatings) which reduce overall paint use, increase line speed through reduced drying time, and reduce HSE risks from paint spray314.
* Fuji Xerox and Ricoh (and others) have moved from provision of photocopiers to ‘photocopied pages'. Through predictive maintenance and use of closed loops for photocopier machinery, resource utilisation has been improved315.
* Philips offers ‘pay per lux' where they specify, install and replace lightbulbs in return for a regular fee based on the light hitting work surfaces where it is needed316.

There are four key barriers to revenue growth from non-labour resource productivity, namely senior executive leadership to drive what is often substantial change, information availability (case studies, incentive alignment, cost transparency and clear standards), resources (for product development, business model design and testing, and marketing) and a suitable skill set.},
author = {Lavery, G and Penell, N and Brown, S and Evans, S},
keywords = {business process re-engineering,competitive challenges,engineering,networks,organisational design,philosophy,resilience,strategic planning,strategy,supply chain,sustainability,systems},
pages = {164},
publisher = {Institute for Manufacturing, Cambridge University},
title = {{The next manufacturing revolution:  non-labour resource productivity and its potential for UK manufacturing}},
url = {http://www.nextmanufacturingrevolution.org},
year = {2013}
}
@article{Bartsch1912,
author = {Bartsch, Rudolf C.B.},
doi = {10.1155/1912/70954},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Bartsch - 1912 - “Sugaring” in the Autumn.pdf:pdf},
issn = {16877438},
journal = {Psyche (New York)},
number = {6},
pages = {195--199},
title = {{“Sugaring” in the Autumn}},
volume = {19},
year = {1912}
}
@article{LupLow2001,
abstract = {Existing duplicate elimination methods for data cleaning work on the basis of computing the degree of similarity between nearby records in a sorted database. High recall can be achieved by accepting records with low degrees of similarity as duplicates, at the cost of lower precision. High precision can be achieved analogously at the cost of lower recall. This is the recall–precision dilemma. We develop a generic knowledge-based framework for effective data cleaning that can implement any existing data cleaning strategies and more. We propose a new method for computing transitive closure under uncertainty for dealing with the merging of groups of inexact duplicate records and explain why small changes to window sizes has little effect on the results of the sorted neighborhood method. Experimental study with two real-world datasets show that this approach can accurately identify duplicates and anomalies with high recall and precision, thus effectively resolving the recall–precision dilemma.},
author = {{Lup Low}, Wai and {Li Lee}, Mong and {Wang Ling}, Tok},
doi = {10.1016/S0306-4379(01)00041-2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lup Low, Li Lee, Wang Ling - 2001 - A knowledge-based approach for duplicate elimination in data cleaning.pdf:pdf},
issn = {0306-4379},
journal = {Information Systems},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {dec},
number = {8},
pages = {585--606},
publisher = {Pergamon},
title = {{A knowledge-based approach for duplicate elimination in data cleaning}},
url = {https://www.sciencedirect.com/science/article/pii/S0306437901000412},
volume = {26},
year = {2001}
}
@article{Cannataro2003,
abstract = {The Grid is an integrated infrastructure for coordinated resource sharing and problem solving in distributed environments. The effective and efficient use of stored data and its transformation into information and knowledge will be a main driver in Grid evolution. The use of ontologies to describe Grid resources will simplify and structure the systematic building of Grid applications through the composition and reuse of software components and the development of knowledge-based services and tools. The paper presents an ontology for the Data Mining domain that can be used to simplify the development of distributed knowledge discovery applications on the Grid, offering to a domain expert a reference model for the different kind of data mining tasks, methodologies and software available to solve a given problem, helping a user in finding the most appropriate solution. How the DAMON ontology is used to enhance the design of distributed data mining applications on the KNOWLEDGE GRID is also shown.},
author = {Cannataro, Mario and Cannataro, Mario and Comito, Carmela and Comito, Carmela},
journal = {First International Workshop on Semantics in Peer-to-Peer and Grid Computing, in conjunction with WWW2003},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {113--134},
title = {{A Data Mining Ontology for Grid Programming}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.196.5116{\&}rep=rep1{\&}type=pdf{\#}page=119},
year = {2003}
}
@misc{OpenCollaboration2018,
abstract = {A topic-centric list of high-quality open datasets in public domains.},
author = {{Open Collaboration}},
booktitle = {GitHub},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{Public Datasets}},
url = {https://github.com/awesomedata/awesome-public-datasets},
urldate = {2018-07-31},
year = {2018}
}
@inproceedings{Azuan2017,
abstract = {Dataspaces aim to remove the up-front costs of information integration by gathering the needed domain information through targeted interactions with the end-user throughout the life-time of the integration. State-of-the-art tools are used to rapidly construct an initial (incorrect) integration, which is then refined in a payas-you-go manner by asking end-users to supply feedback on the resulting data. The idea is that end-users will choose to put e.ort into providing feedback on the areas of the integration where the quality is important to them, while other less well-used areas will receive a smaller share of user a.ention. This approach is promising but open problems remain. One issue is that the end-user loses control over the process. Their contribution is to specify their query requirements and to provide feedback on the results, as directed by the dataspace. But what feedback should the user supply to get the data they want? We propose a new approach to data integration in which the end-user and the dataspace work as equal partners to meet the integration goal. Both are able to perform data integration tasks directly, and both request and provide feedback on the results. In addition, the dataspace observes the actions of the end-user when carrying out integration, with the aim of automating that part of the work in future integration tasks. In this paper, we explore this idea by examining how a dataspace can observe an end-user at work, correcting errors in query results, to gather feedback needed to refine the mappings used for integration. We propose an algorithm for converting manual corrections to feedback, and present the results of a preliminary evaluation comparing this approach with seeking explicit feedback from end-users.},
author = {Azuan, Nurzety A. and Embury, Suzanne M. and Paton, Norman W.},
booktitle = {Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, HILDA 2017},
doi = {10.1145/3077257.3077272},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Azuan, Embury, Paton - 2017 - Observing the data scientist Using manual corrections as implicit feedback.pdf:pdf},
isbn = {9781450350297},
keywords = {Dataspaces,Implicit feedback,Information integration,Manual data correction,Pay-as-you-go},
month = {may},
publisher = {Association for Computing Machinery, Inc},
title = {{Observing the data scientist: Using manual corrections as implicit feedback}},
year = {2017}
}
@article{Dennis2016,
abstract = {At a time of climate change and major loss of biodiversity, it is important to have efficient tools for monitoring populations. In this context, animal abundance indices play an important r{\^{o}}le. In producing indices for invertebrates, it is important to account for variation in counts within seasons. Two new methods for describing seasonal variation in invertebrate counts have recently been proposed; one is nonparametric, using generalized additive models, and the other is parametric, based on stopover models. We present a novel generalized abundance index which encompasses both parametric and nonparametric approaches. It is extremely efficient to compute this index due to the use of concentrated likelihood techniques. This has particular relevance for the analysis of data from long-term extensive monitoring schemes with records for many species and sites, for which existing modeling techniques can be prohibitively time consuming. Performance of the index is demonstrated by several applications to UK Butterfly Monitoring Scheme data. We demonstrate the potential for new insights into both phenology and spatial variation in seasonal patterns from parametric modeling and the incorporation of covariate dependence, which is relevant for both monitoring and conservation. Associated R code is available on the journal website.},
author = {Dennis, Emily B. and Morgan, Byron J.T. and Freeman, Stephen N. and Brereton, Tom M. and Roy, David B.},
doi = {10.1111/biom.12506},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dennis et al. - 2016 - A generalized abundance index for seasonal invertebrates.pdf:pdf},
issn = {15410420},
journal = {Biometrics},
keywords = {Butterflies,Citizen science,Concentrated likelihood,Normal mixtures,Phenology,UKBMS},
month = {dec},
number = {4},
pages = {1305--1314},
publisher = {Blackwell Publishing Inc.},
title = {{A generalized abundance index for seasonal invertebrates}},
volume = {72},
year = {2016}
}
@article{Abramo2009,
author = {Abramo, Giovanni and D'Angelo, Ciriaco Andrea and Caprasecca, Alessandro},
doi = {10.1016/j.respol.2008.11.001},
issn = {00487333},
journal = {Research Policy},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {feb},
number = {1},
pages = {206--215},
title = {{Allocative efficiency in public research funding: Can bibliometrics help?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0048733308002667},
volume = {38},
year = {2009}
}
@article{Ozkose2015,
abstract = {Owing to the self-improvement desire, the human being always tries to reach to the current information and generate new ones from the data on hand. The practices are realized by processing and transforming the data, whose existence is broadly accepted, into information. Generating information from data is vitally important in terms of regulating the life. Especially firms need to store and transform data quickly and properly into information in order to achieve the objectives such as having a competitive edge, producing new products, moving the firm ahead and stabilizing the internal dynamics. The increase in the amount of data sources also increases the amount of the data acquired. Therefore storing and processing data become difficult and classical approaches remain incapable to do such transactions. By means of Big Data large amount of data with a wide range can be stored, managed and processed. Besides Big Data ensures proper information quickly and offers advantage and convenience to the firms, researchers and consumers by taking the properties of Volume, Value, Variety, Veracity and Velocity into consideration. This study consists of 5 parts. In the Introduction part the features, classification, the process, the areas of usage and the techniques of Big Data are explained. In the second part the appearance process and the advantages of the concept of Big Data are illustrated with examples. A detailed literature review is produced in the third part. The actual studies and the most interested areas of Big Data are told in this part. In the fourth part the future of the Big Data is evaluated. Besides the situation and distribution of the studies on Big Data in Turkey and all over the world is presented. In the Conclusion part, an overall assessment is included and probable troubles are mentioned.},
author = {{\"{O}}zk{\"{o}}se, Hakan and Arı, Emin Serta{\c{c}} and Gencer, Cevriye},
doi = {10.1016/J.SBSPRO.2015.06.147},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/{\"{O}}zk{\"{o}}se, Arı, Gencer - 2015 - Yesterday, Today and Tomorrow of Big Data.pdf:pdf},
issn = {1877-0428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jul},
pages = {1042--1050},
publisher = {Elsevier},
title = {{Yesterday, Today and Tomorrow of Big Data}},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815036265?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {195},
year = {2015}
}
@book{Franklin2010,
abstract = {Maps of species' distributions or habitat suitability are required for many aspects of environmental research, resource management and conservation planning. These include biodiversity assessment, reserve design, habitat management and restoration, species and habitat conservation plans and predicting the effects of environmental change on species and ecosystems. The proliferation of methods and uncertainty regarding their effectiveness can be daunting to researchers, resource managers and conservation planners alike. Franklin summarises the methods used in species distribution modeling (also called niche modeling) and presents a framework for spatial prediction of species distributions based on the attributes (space, time, scale) of the data and questions being asked. The framework links theoretical ecological models of species distributions to spatial data on species and environment, and statistical models used for spatial prediction. Providing practical guidelines to students, researchers and practitioners in a broad range of environmental sciences including ecology, geography, conservation biology, and natural resources management.},
annote = {Not in lboro library},
author = {Franklin, Janet and Miller, Jennifer A.},
isbn = {9780521876353},
pages = {338},
publisher = {Cambridge University Press},
title = {{Mapping species distribution. Spatial inference and prediction.}},
year = {2010}
}
@article{Zwitter2014,
abstract = {The speed of development in Big Data and associated phenomena, such as social media, has surpassed the capacity of the average consumer to understand his or her actions and their knock-on effects. We are moving towards changes in how ethics has to be perceived: away from individual decisions with specific and knowable outcomes, towards actions by many unaware that they may have taken actions with unintended consequences for anyone. Responses will require a rethinking of ethical choices, the lack thereof and how this will guide scientists, governments, and corporate agencies in handling Big Data. This essay elaborates on the ways Big Data impacts on ethical conceptions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zwitter, Andrej},
doi = {10.1177/2053951714559253},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zwitter - 2014 - Big Data ethics.pdf:pdf},
isbn = {ID 2384174},
issn = {2053-9517},
journal = {Big Data {\&} Society},
keywords = {Big Data governance,Ethics,education,moral responsibility,societal impact},
month = {jul},
number = {2},
pages = {1--6},
pmid = {21675331},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Big Data ethics}},
url = {http://journals.sagepub.com/doi/10.1177/2053951714559253},
volume = {1},
year = {2014}
}
@article{Sullivan2009,
abstract = {New technologies are rapidly changing the way we collect, archive, analyze, and share scientific data. For example, over the next several years it is estimated that more than one billion autonomous sensors will be deployed over large spatial and temporal scales, and will gather vast quantities of data. Networks of human observers play a major role in gathering scientific data, and whether in astronomy, meteorology, or observations of nature, they continue to contribute significantly. In this paper we present an innovative use of the Internet and information technologies that better enhances the opportunity for citizens to contribute their observations to science and the conservation of bird populations. eBird is building a web-enabled community of bird watchers who collect, manage, and store their observations in a globally accessible unified database. Through its development as a tool that addresses the needs of the birding community, eBird sustains and grows participation. Birders, scientists, and conservationists are using eBird data worldwide to better understand avian biological patterns and the environmental and anthropogenic factors that influence them. Developing and shaping this network over time, eBird has created a near real-time avian data resource producing millions of observations per year. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Sullivan, Brian L. and Wood, Christopher L. and Iliff, Marshall J. and Bonney, Rick E. and Fink, Daniel and Kelling, Steve},
doi = {10.1016/j.biocon.2009.05.006},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sullivan et al. - 2009 - eBird A citizen-based bird observation network in the biological sciences.pdf:pdf},
isbn = {0006-3207},
issn = {00063207},
journal = {Biological Conservation},
keywords = {Bird observations,Citizen-science,Observation network,Scale-dependent analysis,eBird},
month = {oct},
number = {10},
pages = {2282--2292},
pmid = {1960744},
publisher = {Elsevier},
title = {{eBird: A citizen-based bird observation network in the biological sciences}},
url = {https://www.sciencedirect.com/science/article/pii/S000632070900216X},
volume = {142},
year = {2009}
}
@article{Ahmed2017,
abstract = {The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.},
author = {Ahmed, Ejaz and Yaqoob, Ibrar and Hashem, Ibrahim Abaker Targio and Khan, Imran and Ahmed, Abdelmuttlib Ibrahim Abdalla and Imran, Muhammad and Vasilakos, Athanasios V.},
doi = {10.1016/j.comnet.2017.06.013},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ahmed et al. - 2017 - The role of big data analytics in Internet of Things.pdf:pdf},
issn = {13891286},
journal = {Computer Networks},
keywords = {Analytics,Big data,Distributed computing,Internet of Things,Smart city},
month = {dec},
pages = {459--471},
publisher = {Elsevier B.V.},
title = {{The role of big data analytics in Internet of Things}},
volume = {129},
year = {2017}
}
@article{Srivastava2000,
abstract = {Web usage mining is the application of data mining techniques to discover usage patterns from Web data, in order to understand and better serve the needs of Web-based applications. Web usage mining consists of three phases, namely preprocessing, pattern discovery, and pattern analysis. This paper describes each of these phases in detail. Given its application potential, Web usage mining has seen a rapid increase in interest, from both the research and practice communities. This paper provides a detailed taxonomy of the work in this area, including research efforts as well as commercial offerings. An up-to-date survey of the existing work is also provided. Finally, a brief overview of the WebSIFT system as an example of a prototypical Web usage mining system is given.},
author = {Srivastava, Jaideep and Cooley, Robert and Deshpande, Mukund and Tan, Pang-Ning},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2000 - Web usage mining Discovery and applications of usage patterns from web data.pdf:pdf},
journal = {SIGKDD Explorations},
keywords = {Big Data,data mining,web usage mining,world wide web},
mendeley-tags = {Big Data},
number = {12},
pages = {12--23},
title = {{Web usage mining: Discovery and applications of usage patterns from web data}},
url = {https://dl.acm.org/citation.cfm?id=846188},
volume = {1},
year = {2000}
}
@article{Hoare1975,
abstract = {This paper surveys the problems of achieving data reliability, and finds them more severe than those of program reliability. It then outlines some of the conceptual and methodological tools which are available for the solution of these problems, including the concept of type, direct product, union, sequence, recursion and mapping. It touches on the topdown design of data and programs, and argues that references or pointers are to be avoided. It concludes with an annotated bibliography for further reading. {\textcopyright} 1975, ACM. All rights reserved.},
annote = {Raised questions about the imporance of data quality.},
author = {Hoare, C. A.R.},
doi = {10.1145/390016.808476},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hoare - 1975 - Data reliability.pdf:pdf},
issn = {15581160},
journal = {ACM SIGPLAN Notices},
keywords = {Abstraction and representation,Avoidance of references,Data structuring,Software reliability},
month = {jun},
number = {6},
pages = {528--533},
title = {{Data reliability}},
url = {http://portal.acm.org/citation.cfm?doid=390016.808476},
volume = {10},
year = {1975}
}
@misc{NaturalHistoryMuseum2017a,
abstract = {You can search the Dictionary of UK species for Individual species found in the UK (by scientific or common name) Higher taxa and their members e.g. lepidoptera (by scientific name) Informal groups of animals, plants, amphibians and insects found in the UK e.g. butterflies (by common name).},
author = {{Natural History Museum}},
title = {{Search for a UK species - Natural History Museum}},
url = {http://www.nhm.ac.uk/our-science/data/uk-species/species/index.html},
urldate = {2017-08-16},
year = {2017}
}
@article{Hartmann2016,
abstract = {Purpose This paper aims to derive a taxonomy of business models used by start-up firms that rely on data as a key resource for business, namely data-driven business models (DDBMs). By providing a framework to systematically analyse DDBMs, the study provides an introduction to DDBM as a field of study. Design/methodology/approach To develop the taxonomy of DDBMs, business model descriptions of 100 randomly chosen start-up firms were coded using a DDBM framework derived from literature, comprising six dimensions with thirty-five features. Subsequent application of clustering algorithms produced six different types of DDBM, validated by case studies from the study?s sample. Findings The taxonomy derived from our research consists of six different types of DDBM among start-ups. These types are characterised by a subset of six of nine clustering variables from the DDBM framework. Practical implications A major contribution of the paper is the designed framework, which stimulates thinking about the nature and future of DDBMs. The proposed taxonomy will help organisations to position their activities in the current DDBM landscape. Moreover, framework and taxonomy may lead to a DDBM design toolbox. Originality/value This paper develops a basis for understanding how start-ups build business models to capture value from data as a key resource, adding a business perspective to the discussion of big data. By offering the scientific community a specific framework of business model features and a subsequent taxonomy, the paper provides reference points and serves as a foundation for future studies of DDBMs.},
archivePrefix = {arXiv},
arxivId = {A luxury brand management framework built from historical review and case study analysis},
author = {Hartmann, Philipp Max and Zaki, Mohamed and Feldmann, Niels and Neely, Andy},
doi = {10.1108/IJOPM-02-2014-0098},
eprint = {A luxury brand management framework built from historical review and case study analysis},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Hartmann et al. - 2016 - Capturing value from big data – a taxonomy of data-driven business models used by start-up firms.pdf:pdf},
isbn = {0320130029},
issn = {17586593},
journal = {International Journal of Operations and Production Management},
keywords = {Big data,Business model,Data-driven business model,Start-up business model},
number = {10},
pages = {1382--1406},
pmid = {42012058},
title = {{Capturing value from big data – a taxonomy of data-driven business models used by start-up firms}},
url = {https://doi.org/10.1108/IJOPM-02-2014-0098},
volume = {36},
year = {2016}
}
@misc{Bishop2013,
abstract = {Spreadsheet errors are costing businesses billions of pounds, according to a financial modeling company, which is calling for the introduction of industry-wide standards to reduce the risk of mistakes. F1F9 estimated that 88 percent of all spreadsheets have errors in them, while 50 percent of spreadsheets used by large companies have material defects. The company said the mistakes are not just costly in terms of time and money - but also lead to damaged reputations, lost jobs and disrupted careers.},
address = {Englewood Cliffs, New Jersey,USA},
author = {Bishop, Katrina},
booktitle = {CNBC Europe News},
month = {jul},
pages = {1},
title = {{Spreadsheet blunders costing business billions}},
url = {https://www.cnbc.com/id/100923538},
year = {2013}
}
@techreport{Manyika2011,
abstract = {this study examines the potential value that big data can create for organizations and sectors of the economy and seeks to illustrate and quantify that value. We also explore what leaders of organizations and policy makers need to do to capture it. aligning incentives to ensure access to data; addressing privacy and security concerns; establishing intellectual property frameworks; overcoming technological barriers to data; and promoting information and communication technology infrastructure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manyika, James and Chui, Michael and Brown, Brad and Bughin, Jacques and Dobbs, Richard and Roxburgh, Charles and Byers, Angela H},
booktitle = {McKinsey Global Institute},
doi = {10.1080/01443610903114527},
eprint = {arXiv:1011.1669v3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Manyika et al. - 2011 - Big data The next frontier for innovation, competition, and productivity.pdf:pdf},
institution = {McKinsey Global Institute},
isbn = {0983179697, 978-0983179696},
issn = {14712970},
keywords = {Big data,competition,innovation},
number = {June},
pages = {156},
pmid = {19757261},
title = {{Big data: The next frontier for innovation, competition, and productivity}},
url = {www.mckinsey.com/mgi.},
year = {2011}
}
@article{sarvar2007thermalelectronics,
author = {Sarvar, F and Whalley, D C and Hutt, D A and Palmer, P J and Teh, N J},
doi = {10.1108/13565360710818439},
issn = {1356-5362},
journal = {Microelectronics International},
number = {(3)},
pages = {66--75},
title = {{Thermal and Thermo-Mechanical Modelling of Polymer Overmoulded Electronics}},
url = {http://dx.doi.org/10.1108/13565360710818439},
volume = {24},
year = {2007}
}
@techreport{Zhao2015,
abstract = {Examples, documents and resources on Data Mining with R, incl. decision trees, clustering, outlier detection, time series analysis, association rules, text mining and social network analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0611061v2},
author = {Zhao, Yanchang},
booktitle = {R and Data Mining},
doi = {10.1016/B978-0-12-396963-7.00001-5},
eprint = {0611061v2},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zhao - 2015 - R and Data Mining Examples and Case Studies.pdf:pdf},
institution = {RDataMining},
isbn = {978-0-12-396963-7},
issn = {20734859},
keywords = {R data mining example case study data import and e,R!},
mendeley-tags = {R!},
pages = {166},
pmid = {24809974},
primaryClass = {arXiv:quant-ph},
title = {{R and Data Mining: Examples and Case Studies}},
url = {http://www.rdatamining.com http://www2.rdatamining.com http://www.rdatamining.com/books/rdm http://www2.rdatamining.com/ http://www.rdatamining.com/books/dmar http://www2.rdatamining.com/data-mining-applications-with-r.html},
year = {2015}
}
@inproceedings{Kain2002,
abstract = {Often, the main motivation for using PKI in business environments is to streamline workflow, by enabling humans to digitally sign electronic documents, instead of manually signing paper ones. However, this application fails if adversaries can construct electronic documents whose viewed contents can change in useful ways, without invalidating the digital signature. In this paper, we examine the space of such attacks, and describe how many popular electronic document formats and PKI packages permit them.},
address = {Portoro{\v{z}}, Slovenia},
author = {Kain, K and Smith, SW and Asokan, R},
booktitle = {Advanced communications and multimedia security: IFIP TC6/TC11 Sixth Joint Working Conference on Communications and Multimedia Security},
doi = {10.1007/978-0-387-35612-9_23},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kain, Smith, Asokan - 2002 - Digital signatures and electronic documents A cautionary tale.pdf:pdf},
keywords = {PKI,digital signatures,e-commerce,e-government},
number = {September},
pages = {293},
title = {{Digital signatures and electronic documents: A cautionary tale}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-0-387-35612-9{\_}22.pdf http://books.google.com/books?hl=en{\&}lr={\&}id=xL6SxupQXjUC{\&}oi=fnd{\&}pg=PA293{\&}dq=DIGITAL+SIGNATURES+AND+ELECTRONIC+DOCUMENTS:+A+CAUTIONARY+TALE{\&}ots=O-SJH8uMZn{\&}s},
year = {2002}
}
@article{Deng2017,
abstract = {In many organizations, it is often challenging for users to find rele-vant data for specific tasks, since the data is usually scattered across the enterprise and often inconsistent. In fact, data scientists routinely report that the majority of their effort is spent finding, cleaning, in-tegrating, and accessing data of interest to a task at hand. In order to decrease the " grunt work " needed to facilitate the analysis of data " in the wild " , we present DATA CIVILIZER, an end-to-end big data management system. DATA CIVILIZER has a linkage graph computation module to build a linkage graph for the data and a data discovery module which utilizes the linkage graph to help identify data that is relevant to user tasks. It also uses the linkage graph to discover possible join paths that can then be used in a query. For the actual query execution, we use a polystore DBMS, which federates query processing across disparate systems. In addition, DATA CIVILIZER integrates data cleaning operations into query pro-cessing. Because different users need to invoke the above tasks in different orders, DATA CIVILIZER embeds a workflow engine which enables the arbitrary composition of different modules, as well as the handling of data updates. We have deployed our preliminary DATA CIVILIZER system in two institutions, MIT and Merck and describe initial positive experiences that show the system shortens the time and effort required to find, prepare, and analyze data.},
author = {Deng, Dong and Castro, Raul and Ziawasch, Fernandez and Sibo, Abedjan and Elmagarmid, Ahmed and Ilyas, Ihab F and Madden, Samuel and Ouzzani, Mourad and Tang, Nan},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Deng et al. - 2017 - (jk-p)The Data Civilizer System.pdf:pdf},
isbn = {9781450321389},
journal = {Cidr},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{(jk-p)The Data Civilizer System}},
url = {http://cidrdb.org/cidr2017/papers/p44-deng-cidr17.pdf},
year = {2017}
}
@inproceedings{cottrill2002foresightoptions,
author = {Cottrill, M C and Jaggernauth, W A and Webb, D P and Palmer, P J and Conway, P P and West, A A},
booktitle = {Proceedings of the Society of Automotive Engineers World Congress and Exposition, SAE Paper No 2002-01-1127},
issn = {0148-7191},
month = {mar},
organization = {Detroit, USA},
pages = {1--9},
title = {{Foresight Vehicle: Large Area Flexible Circuits for Automotive Applications Manufacturing Technology - A Review of Process Options}},
year = {2002}
}
@article{Madin2007,
abstract = {Research in ecology increasingly relies on the integration of small, focused studies, to produce larger datasets that allow for more powerful, synthetic analyses. The results of these synthetic analyses are critical in guiding decisions about how to sustainably manage our natural environment, so it is important for researchers to effectively discover relevant data, and appropriately integrate these within their analyses. However, ecological data encompasses an extremely broad range of data types, structures, and semantic concepts. Moreover, ecological data is widely distributed, with few well-established repositories or standard protocols for their archiving and retrieval. These factors make the discovery and integration of ecological data sets a highly labor-intensive task. Metadata standards such as the Ecological Metadata Language and Darwin Core are important steps for improving our ability to discover and access ecological data, but are limited to describing only a few, relatively specific aspects of data content (e.g., data owner and contact information, variable "names", keyword descriptions, etc.). A more flexible and powerful way to capture the semantic subtleties of complex ecological data, its structure and contents, and the inter-relationships among data variables is needed. We present a formal ontology for capturing the semantics of generic scientific observation and measurement. The ontology provides a convenient basis for adding detailed semantic annotations to scientific data, which crystallize the inherent "meaning" of observational data. The ontology can be used to characterize the context of an observation (e.g., space and time), and clarify inter-observational relationships such as dependency hierarchies (e.g., nested experimental observations) and meaningful dimensions within the data (e.g., axes for cross-classified categorical summarization). It also enables the robust description of measurement units (e.g., grams of carbon per liter of seawater), and can facilitate automatic unit conversions (e.g., pounds to kilograms). The ontology can be easily extended with specialized domain vocabularies, making it both broadly applicable and highly customizable. Finally, we describe the utility of the ontology for enriching the capabilities of data discovery and integration processes.},
author = {Madin, Joshua and Bowers, Shawn and Schildhauer, Mark and Krivov, Sergeui and Pennington, Deana and Villa, Ferdinando},
doi = {10.1016/j.ecoinf.2007.05.004},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Madin et al. - 2007 - An ontology for describing and synthesizing ecological observation data.pdf:pdf},
isbn = {1574-9541},
issn = {15749541},
journal = {Ecological Informatics},
keywords = {Data discovery,Data integration,Ecology,Measurement,Observation,Ontology},
month = {oct},
number = {3 SPEC. ISS.},
pages = {279--296},
pmid = {21435355},
publisher = {Elsevier},
title = {{An ontology for describing and synthesizing ecological observation data}},
url = {https://www.sciencedirect.com/science/article/pii/S1574954107000362},
volume = {2},
year = {2007}
}
@inproceedings{Belhajjame2011,
abstract = {User feedback is gaining momentum as a means of addressing the difficulties underlying information integration tasks. It can be used to assist users in building information integration systems and to improve the quality of existing systems, e.g., in dataspaces. Existing proposals in the area are confined to specific integration sub-problems considering a specific kind of feedback sought, in most cases, from a single user. We argue in this paper that, in order to maximize the benefits that can be drawn from user feedback, it should be considered and managed as a first class citizen. Accordingly, we present generic operations that underpin the management of feedback within information integration systems, and that are applicable to feedback of different kinds, potentially supplied by multiple users with different expectations. We present preliminary solutions that can be adopted for realizing such operations, and sketch a research agenda for the information integration community.},
author = {Belhajjame, Khalid and Paton, Norman W and Fernandes, Alvaro A.A. and Hedeler, Cornelia and Embury, Suzanne M},
booktitle = {CIDR 2011 - 5th Biennial Conference on Innovative Data Systems Research, Conference Proceedings},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Belhajjame et al. - 2011 - User feedback as a first class citizen in information integration systems.pdf:pdf},
keywords = {Dataspaces,Feedback management,Information integration,User feedback},
pages = {175--183},
title = {{User feedback as a first class citizen in information integration systems}},
year = {2011}
}
@article{Blackler1995,
abstract = {There is current interest in the competitive advantage that knowledge may provide for organizations and in the significance of knowledge workers, organ- izational competencies and knowledge-intensive firms. Yet the concept of knowledge is complex and its relevance to organization theory has been insuf- ficiently developed. The paper offers a review and critique of current approaches, and outlines an alternative. First, common images of knowledge in the organizational literature as embodied, embedded, embrained, encultured and encoded are identified and, to summarize popular writings on knowledge work, a typology of organizations and knowledge types is constructed. How- ever, traditional assumptions about knowledge, upon which most current speculation about organizational knowledge is based, offer a compartmental- ized and static approach to the subject. Drawing from recent studies of the impact of new technologies and from debates in philosophy, linguistics, social theory and cognitive science, the second part of the paper introduces an altern- ative. Knowledge (or, more appropriately, knowing) is analyzed as an active process that is mediated, situated, provisional, pragmatic and contested. Rather than documenting the types of knowledge that capitalism currently demands the approach suggests that attention should be focused on the (culturally located) systems through which people achieve their knowing, on the changes that are occurring within such systems, and on the processes through which new knowledge may be generated.},
annote = {Defining knowledge:
embodied, embedded, embrained, encultured and encoded.
Knowing is analyzed as an active process that is mediated, situated, provisional, pragmatic and contested.},
author = {Blackler, Frank},
doi = {10.1177/017084069501600605},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Blackler - 1995 - Knowledge, Knowledge Work and Organizations An Overview and Interpretation(2).pdf:pdf},
isbn = {0170-8406},
issn = {17413044},
journal = {Organization Studies},
keywords = {Descriptors: activity theory,knowledge,knowledge work,knowledge-intensive firms,organizational competencies,organizational learning},
month = {nov},
number = {6},
pages = {1021--1046},
pmid = {39},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Knowledge, Knowledge Work and Organizations: An Overview and Interpretation}},
url = {http://journals.sagepub.com/doi/10.1177/017084069501600605},
volume = {16},
year = {1995}
}
@techreport{Lee2006,
abstract = {Threads are a seemingly straightforward adaptation of the dominant sequential model of computation to concurrent systems. Languages require little or no syntactic changes to support threads, and operating systems and architectures have evolved to efficiently support them. Many technologists are pushing for increased use of multithreading in software in order to take advantage of the predicted increases in parallelism in computer architectures. In this paper, I argue that this is not a good idea. Although threads seem to be a small step from sequential computation, in fact, they represent a huge step. They discard the most essential and appeal- ing properties of sequential computation: understandability, predictability, and determinism. Threads, as a model of computation, are wildly nondeterministic, and the job of the program- mer becomes one of pruning that nondeterminism. Although many research techniques improve the model by offering more effective pruning, I argue that this is approaching the problem backwards. Rather than pruning nondeterminism, we should build from essentially determinis- tic, composable components. Nondeterminism should be explicitly and judiciously introduced where needed, rather than removed where not needed. The consequences of this principle are profound. I argue for the development of concurrent coordination languages based on sound, composable formalisms. I believe that such languages will yield much more reliable, and more concurrent programs.},
address = {Berkeley, CA},
annote = {Paper in Road2CPS.

A bit of a polemic. Wants new languages that recognise time in multi-processing. Lists some candidates. Conclusion is worth reading.

Quote:
"Language wars are religious wars, and few of these religions are polytheistic."

Threads:
"In general-purpose software engineering practice, we have reached a point where one approach to concurrent programming dominates all others, namely, threads. Threads are sequential processes that share memory. They represent a key concurrency model supported by modern computers, programming languages, and operating systems. Many general-purpose parallel architectures in use today (such as symmetric multiprocessors, SMPs) are direct hardware realizations of the thread abstraction.
Some applications can very effectively use threads. So-called “embarrassingly parallel” ap- plications (for example, applications that essentially spawn multiple independent processes such as build tools, like PVM gmake, or web servers). Because of the independence of these applica- tions, programming is relatively easy, and the abstraction being used is more like processes than threads (where memory is not shared). Where such applications do share data, they do so through database abstractions, which manage concurrency through such mechanisms as transactions. How- ever, client-side applications are not so simple. Quoting Sutter and Larus again [47]:
“The world of client applications is not nearly as well structured and regular. A typical client application executes a relatively small computation on behalf of a single user, so concurrency is found by dividing a computation into finer pieces. These pieces, say the user interface and program's computation, interact and share data in a myriad of ways.
Non-homogeneous code; fine-grain, complicated interactions; and pointer-based data structures make this type of program difficult to execute concurrently.” "

Problem illustration:
"An anecdote from the Ptolemy Project6 is telling (and alarming). In the early part of the year 2000, my group began developing the kernel of Ptolemy II [20], a modeling environment supporting concurrent models of computation. An early objective was to permit modification of concurrent programs via a graphical user interface while those concurrent programs were executing. The challenge was to ensure that no thread could ever see an inconsistent view of the program structure. The strategy was to use Java threads with monitors.
A part of the Ptolemy Project experiment was to see whether effective software engineering practices could be developed for an academic research setting. We developed a process that included a code maturity rating system (with four levels, red, yellow, green, and blue), design reviews, code reviews, nightly builds, regression tests, and automated code coverage metrics [43]. The portion of the kernel that ensured a consistent view of the program structure was written in early 2000, design reviewed to yellow, and code reviewed to green. The reviewers included concurrency experts, not just inexperienced graduate students (Christopher Hylands (now Brooks), Bart Kienhuis, John Reekie, and myself were all reviewers). We wrote regression tests that achieved 100 percent code coverage. The nightly build and regression tests ran on a two processor SMP machine, which exhibited different thread behavior than the development machines, which all had a single processor. The Ptolemy II system itself began to be widely used, and every use of the system exercised this code. No problems were observed until the code deadlocked on April 26, 2004, four years later.It is certainly true that our relatively rigorous software engineering practice identified and fixed many concurrency bugs. But the fact that a problem as serious as a deadlock that locked up the system could go undetected for four years despite this practice is alarming. How many more such problems remain? How long do we need test before we can be sure to have discovered all such problems? Regrettably, I have to conclude that testing may never reveal all the problems in nontrivial multithreaded code."

CONCLUSION:
"
Concurrency in software is difficult. However, much of this difficulty is a consequence of the ab- stractions for concurrency that we have chosen to use. The dominant one in use today for general- purpose computing is threads. But non-trivial multi-threaded programs are incomprehensible to humans. It is true that the programming model can be improved through the use of design pat- terns, better granularity of atomicity (e.g. transactions), improved languages, and formal methods. However, these techniques merely chip away at the unnecessarily enormous nondeterminism of the threading model. The model remains intrinsically intractable.
If we expect concurrent programming to be mainstream, and if we demand reliability and pre- dictability from programs, then we must discard threads as a programming model. Concurrent programming models can be constructed that are much more predictable and understandable than threads. They are based on a very simple principle: deterministic ends should be accomplished with deterministic means. Nondeterminism should be judiciously and carefully introduced where needed, and should be explicit in programs. This principle seems obvious, yet it is not accom- plished by threads. Threads must be relegated to the engine room of computing, to be suffered only by expert technology providers."},
author = {Lee, E A},
keywords = {Cyber-physical systems,IoT,certification:,design process,evaluation,methods,quality,software},
number = {UCB/EECS-2006-1},
publisher = {EE{\&}CS, University of California},
title = {{The problem with threads}},
year = {2006}
}
@techreport{OSOrdnanceSurvey2008,
abstract = {This booklet is aimed at people whose expertise is in fields other than geodesy, who need to know the concepts of coordinate systems in order to deal with coordinate data, and who need information on using mapping coordinate systems in Great Britain. It explains: - the basic concepts of terrestrial coordinate systems; - the coordinate systems used with the Global Positioning System (GPS) and in Ordnance Survey mapping; and - how these two relate to each other},
author = {{OS (Ordnance Survey)}},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/OS (Ordnance Survey) - 2018 - A guide to coordinate systems in Great Britain An introduction to mapping coordinate systems and the use o.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
pages = {53},
title = {{A guide to coordinate systems in Great Britain: An introduction to mapping coordinate systems and the use of GPS datasets with Ordnance Survey mapping}},
url = {https://www.ordnancesurvey.co.uk/docs/support/guide-coordinate-systems-great-britain.pdf},
year = {2018}
}
@book{Wickham2017,
abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You'll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you've learned along the way.},
author = {Wickham, Hadley and Grolemund, Garrett},
doi = {10.1145/3138860.3138865},
isbn = {9781491910399},
issn = {01635700},
pages = {250},
publisher = {O'Reilly Media},
title = {{R for Data Science: Import, Tidy, Transform, Visualize, and Model Data}},
year = {2016}
}
@techreport{Zhu2017a,
abstract = {The goal of kableExtra is to help you build common complex tables and manipulate table styles. It imports the pipe {\%}{\textgreater}{\%} symbol from magrittr and verbalize all the functions, so basically you can add “layers” to a kable output in a way that is similar with ggplot2 and plotly .},
author = {Zhu, Hao},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Zhu - 2017 - Create Awesome LaTeX Table with knitrkable and kableExtra.pdf:pdf},
pages = {1--19},
title = {{Create Awesome LaTeX Table with knitr::kable and kableExtra}},
url = {https://haozhu233.github.io/kableExtra/awesome{\_}table{\_}in{\_}pdf.pdf},
year = {2017}
}
@book{Lumley2005,
abstract = {R is now the most widely used statistical package/language in university statistics departments and many research organisations. Its great advantages are that for many years it has been the leading-edge statistical package/language and that it can be freely downloaded from the R web site. Its cooperative development and open code also attracts many contributors meaning that the modelling and data analysis possibilities in R are much richer than in GLIM4, and so the R edition can be substantially more comprehensive than the GLIM4 edition. This text provides a comprehensive treatment of the theory of statistical modelling in R with an emphasis on applications to practical problems and an expanded discussion of statistical theory. A wide range of case studies is provided, using the normal, binomial, Poisson, multinomial, gamma, exponential and Weibull distributions, making this book ideal for graduates and research students in applied statistics and a wide range of quantitative disciplines.},
author = {Lumley, Thomas},
doi = {10.2307/2347820},
isbn = {0199219141},
pages = {1--38},
pmid = {15371062},
publisher = {Oxford University Press},
title = {{Statistical Modelling in R}},
year = {2005}
}
@misc{Cave2013,
abstract = {British firm ClusterSeven has created technology to clean up the world's spreadsheets and eliminate errors such as those that led to {\pounds}3.8bn losses at JP Morgan.},
address = {London},
author = {Cave, Andrew},
booktitle = {The Telegraph},
month = {mar},
pages = {1},
title = {{Tech City firm storms spreadsheets to prevent 'London Whale' style losses - Telegraph}},
url = {http://www.telegraph.co.uk/finance/yourbusiness/9962933/Tech-City-firm-storms-spreadsheets-to-prevent-London-Whale-style-losses.html},
year = {2013}
}
@article{VanEck2010a,
abstract = {We present VOSviewer, a freely available computer program that we have developed for constructing and viewing bibliometric maps. Unlike most computer programs that are used for bibliometric mapping, VOSviewer pays special attention to the graphical representation of bibliometric maps. The functionality of VOSviewer is especially useful for displaying large bibliometric maps in an easy-to-interpret way. The paper consists of three parts. In the first part, an overview of VOSviewer's functionality for displaying bibliometric maps is provided. In the second part, the technical implementation of specific parts of the program is discussed. Finally, in the third part, VOSviewer's ability to handle large maps is demonstrated by using the program to construct and display a co-citation map of 5,000 major scientific journals.},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {van Eck, Nees Jan and Waltman, Ludo},
doi = {10.1007/s11192-009-0146-3},
eprint = {0803.1716},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/van Eck, Waltman - 2010 - Software survey VOSviewer, a computer program for bibliometric mapping.pdf:pdf},
isbn = {2175-1935},
issn = {01389130},
journal = {Scientometrics},
keywords = {Bibliometric mapping,Journal co-citation analysis,Science mapping,VOS,VOSviewer,Visualization},
month = {aug},
number = {2},
pages = {523--538},
pmid = {20585380},
publisher = {Springer Netherlands},
title = {{Software survey: VOSviewer, a computer program for bibliometric mapping}},
url = {http://link.springer.com/10.1007/s11192-009-0146-3},
volume = {84},
year = {2010}
}
@misc{UKCivilService2015,
annote = {id: 1},
author = {{UK Civil Service}},
number = {9/14/2015},
title = {{Rapid Evidence Assessment Toolkit index - Civil Service}},
url = {http://www.civilservice.gov.uk/networks/gsr/resources-and-guidance/rapid-evidence-assessment},
volume = {2015},
year = {2015}
}
@article{Xie2015a,
abstract = {We developed a new approach combining statistical and graphical methods to build a hierarchically networked structure for understanding spatial characteristics of urban landscapes at multiple scales. Natural breaks optimization algorithm is applied to determine the optimal number of urban land hierarchies and assign discrete patches into ordered sub-groups according to a selected geometric or functional attribute. Patches contained in a sub-group are linked to the patches in the next sub-group according to the spatial relationships between the patch centroids and Voronoi cells. The conceptual foundations and technical details of this approach are elaborated in the case study of building a hierarchically networked structure of urban built-up patches in Beijing. This approach can be applied to quantify landscape patterns of other land uses to facilitate assessments of interconnection between various types of land uses at varied hierarchical levels (spatial scales) and to evaluate ecological service functions of urban built infrastructures.},
author = {Xie, Yichun and Ma, Ting},
doi = {10.1080/02723638.2015.1067410},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Xie, Ma - 2015 - A method for delineating a hierarchically networked structure of urban landscape.pdf:pdf},
issn = {02723638},
journal = {Urban Geography},
keywords = {additively weighted Voronoi diagram,hierarchical structure,patchy landscape,urban morphology},
month = {aug},
number = {6},
pages = {947--963},
publisher = {Routledge},
title = {{A method for delineating a hierarchically networked structure of urban landscape}},
url = {http://www.tandfonline.com/doi/full/10.1080/02723638.2015.1067410},
volume = {36},
year = {2015}
}
@article{voros2001primer,
abstract = {This article is a very brief 'primer' on futures studies and foresight. The intention is to provide some solid starting points and orientation for people who are new to this field of study. I also want to place the use of scenarios and scenario planning into context as one methodology within a much broader foresight framework.},
author = {Voros, Joseph},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Voros - 2001 - A primer on futures studies, foresight and the use of scenarios.pdf:pdf},
journal = {Prospect: The Foresight Bulletin},
number = {1},
pages = {7},
title = {{A primer on futures studies, foresight and the use of scenarios}},
volume = {6},
year = {2001}
}
@techreport{HouseofLordsSelectComittee2018,
abstract = {Our inquiry has concluded that the UK is in a strong position to be among the world leaders in the development of artificial intelligence during the twenty- first century. Britain contains leading AI companies, a dynamic academic research culture, a vigorous start-up ecosystem and a constellation of legal, ethical, financial and linguistic strengths located in close proximity to each other. Artificial intelligence, handled carefully, could be a great opportunity for the British economy. In addition, AI presents a significant opportunity to solve complex problems and potentially improve productivity, which the UK is right to embrace. Our recommendations are designed to support the Government and the UK in realising the potential of AI for our society and our economy, and to protect society from potential threats and risks.},
address = {London, UK},
archivePrefix = {arXiv},
arxivId = {1802.01029},
author = {{House of Lords Select Comittee}},
doi = {10.1145/3173574.3174014},
eprint = {1802.01029},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/House of Lords Select Comittee - 2018 - Artificial Intelligence AI in the UK ready , willing and able.pdf:pdf},
institution = {House of Lords},
isbn = {9781450356206},
number = {March},
pages = {180},
title = {{Artificial Intelligence AI in the UK : ready , willing and able?}},
url = {http://www.parliament.uk/mps-lords-and-offices/standards-and-interests/register-of-lords-},
year = {2018}
}
@misc{Microsoft2016,
abstract = {Excel specifications},
author = {Microsoft},
booktitle = {Microsoft Office Support},
pages = {1--10},
title = {{Excel specifications and limits}},
url = {https://support.office.com/en-US/article/Excel-specifications-and-limits-CA36E2DC-1F09-4620-B726-67C00B05040F},
urldate = {2020-09-11},
year = {2020}
}
@techreport{Anon2020,
author = {Anon},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Anon - 2020 - HD Digital LCD Biological Microscope.pdf:pdf},
title = {{HD Digital LCD Biological Microscope}},
year = {2020}
}
@book{Silverman2018,
abstract = {Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician.},
address = {London, UK},
annote = {eBook published in 2018.
Cracking guide.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Silverman, B. W.},
booktitle = {Density Estimation: For Statistics and Data Analysis},
doi = {10.1201/9781315140919},
edition = {1 st},
eprint = {arXiv:1011.1669v3},
isbn = {978-0412246203},
issn = {00359254},
pmid = {2860460},
publisher = {Chapman {\&} Hall/CRC Texts in Statistical Science},
title = {{Density estimation: For statistics and data analysis}},
year = {1985}
}
@incollection{Manovich2011,
abstract = {In this article I want to address some of the theoretical and practical issues raised by emerging big data-driven social science and humanities. My observations are based on my own experience over last three years with big data projects carried out in my lab at UCSD and Calit2 (softwarestudies.com). The issues which we will discuss include the differences between deep data about a few and surface data about the many; getting access to transactional data; and the new data analysis divide between data experts and the rest of us.},
address = {Minneapolis},
author = {Manovich, Lev},
booktitle = {Debates in the Digital Humanities},
editor = {Gold, Matthew K},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Manovich - 2011 - Trending The Promises and the Challenges of Big Social Data.pdf:pdf},
isbn = {9780816677948},
issn = {1476-4687},
pages = {1--10},
pmid = {20019279},
publisher = {Univ Of Minnesota Press},
title = {{Trending: The Promises and the Challenges of Big Social Data}},
year = {2011}
}
@article{Pawson2014,
abstract = {Recognition of the extent and magnitude of night-time light pollution impacts on natural ecosystems is increasing, with pervasive effects observed in both nocturnal and diurnal species. Municipal and industrial lighting is on the cusp of a step change where energy-efficient lighting technology is driving a shift from “yellow” high-pressure sodium vapor lamps (HPS) to new “white” light-emitting diodes (LEDs). We hypothesized that white LEDs would be more attractive and thus have greater ecological impacts than HPS due to the peak UV-green-blue visual sensitivity of nocturnal invertebrates. Our results support this hypothesis; on average LED light traps captured 48{\%} more insects than were captured with light traps fitted with HPS lamps, and this effect was dependent on air temperature (significant light × air temperature interaction). We found no evidence that manipulating the color temperature of white LEDs would minimize the ecological impacts of the adoption of white LED lights. As such, large-scale adoption of energy-efficient white LED lighting for municipal and industrial use may exacerbate ecological impacts and potentially amplify phytosanitary pest infestations. Our findings highlight the urgent need for collaborative research between ecologists and electrical engineers to ensure that future developments in LED technology minimize their potential ecological effects.},
annote = {From Duplicate 1 (LED lighting increases the ecological impact of light pollution irrespective of color temperature - Pawson, S M; Bader, M K -F)
And Duplicate 4 (LED lighting increases the ecological impact of light pollution irrespective of color temperature - Pawson, S M; Bader, M K -F)

id: 1; issn: print 1051-0761; publication{\_}type: full{\_}text},
author = {Pawson, S M and Bader, M K -F},
doi = {10.1890/14-0468.1},
issn = {1051-0761},
journal = {Ecological Applications},
keywords = {biodiversity,high-pressure sodium lamp,light pollution,spectra,street lighting,urbanization},
number = {7},
pages = {1561 {\textless}last{\_}page{\textgreater} 1568},
publisher = {Ecological Society of America},
title = {{LED lighting increases the ecological impact of light pollution irrespective of color temperature}},
url = {http://dx.doi.org/10.1890/14-0468.1},
volume = {24},
year = {2014}
}
@book{IEEE2017,
abstract = {Machine learning is a branch of artificial intelligence that allows computer systems to learn directly from examples, data, and experience. Through enabling computers to perform specific tasks intelligently, machine learning systems can carry out complex processes by learning from data, rather than following pre-programmed rules. Recent years have seen exciting advances in machine learning, which have raised its capabilities across a suite of applications. Increasing data availability has allowed machine learning systems to be trained on a large pool of examples, while increasing computer processing power has supported the analytical capabilities of these systems. Within the field itself there have also been algorithmic advances, which have given machine learning greater power. As a result of these advances, systems which only a few years ago performed at noticeably below-human levels can now outperform humans at some specific tasks. Many people now interact with systems based on machine learning every day, for example in image recognition systems, such as those used on social media; voice recognition systems, used by virtual personal assistants; and recommender systems, such as those used by online retailers. As the field develops further, machine learning shows promise of supporting potentially transformative advances in a range of areas, and the social and economic opportunities which follow are significant. In healthcare, machine learning is creating systems that can help doctors give more accurate or effective diagnoses for certain conditions. In transport, it is supporting the development of autonomous vehicles, and helping to make existing transport networks more efficient. For public services it has the potential to target support more effectively to those in need, or to tailor services to users. And in science, machine learning is helping to make sense of the vast amount of data available to researchers today, offering new insights into biology, physics, medicine, the social sciences, and more. The UK has a strong history of leadership in machine learning. From early thinkers in the field, through to recent commercial successes, the UK has supported excellence in research, which has contributed to the recent advances in machine learning that promise such potential. These strengths in research and development mean that the UK is well placed to take a leading role in the future development of machine learning. Ensuring the best possible environment for the safe and rapid deployment of machine learning will be essential for enhancing the UK's economic growth, wellbeing, and security, and for unlocking the value of ‘big data'. Action in key areas – shaping the data landscape, building skills, supporting business, and advancing research – can help create this environment},
author = {{Royal Society}},
booktitle = {Report by the Royal Society},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Royal Society - 2017 - MACHINE LEARNING THE POWER AND PROMISE OF COMPUTERS THAT LEARN BY EXAMPLE.pdf:pdf},
isbn = {9781782522591},
issn = {1946-6234},
number = {January},
pages = {125},
title = {{Machine learning : the power and promise of computers that learn by example}},
volume = {66},
year = {2017}
}
@article{Riechers2017,
abstract = {Urban cultural ecosystem services are understood differently by experts and laypersons. Yet, unaccounted differences can lead to management problems for urban green spaces, as experts may recommend practices that do not meet the laypersons' wishes. Qualitative research on the perception of cultural ecosystem services can be one tool to analyze these differences. We use expert and problem-centered interviews to assess differences in cultural ecosystem service perceptions for experts and laypersons in Berlin. Using an innovative approach, we combine inductive qualitative content analysis with a frequency analysis and multidimensional scaling. This explorative study innovatively merges qualitative and quantitative approaches to show new ways of analysis. Our results show that the experts' perceptions of nature appear to be more practical, management-centered, whereas laypersons appear to prioritize enjoyment of nature. Overall, multidimensional scaling indicates different perceptions and conceptualizations of cultural ecosystem service bundles, emphasizing the diverging understandings. If these different perceptions are not accounted for it could lead to social and political contrast. They should therefore be acknowledged in decision-making and goal formulation for the management of urban green.},
author = {Riechers, Maraja and Noack, Eva Maria and Tscharntke, Teja},
doi = {10.1007/s11252-016-0616-3},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Riechers, Noack, Tscharntke - 2017 - Experts' versus laypersons' perception of urban cultural ecosystem services.pdf:pdf},
issn = {15731642},
journal = {Urban Ecosystems},
keywords = {Bundles of services,Management of urban green,Millennium ecosystem assessment,Political conflict,Public participation,Qualitative research},
month = {jun},
number = {3},
pages = {715--727},
publisher = {Springer New York LLC},
title = {{Experts' versus laypersons' perception of urban cultural ecosystem services}},
volume = {20},
year = {2017}
}
@article{Fox1994,
annote = {Defining data is tricky and this paper manages a definition that fits in well with a big data definition.

Figure 2 fits in with the notion that the real world can be represented as a rectangular data.},
author = {Fox, Christopher and Levitin, Anany},
doi = {10.1016/0306-4573(94)90020-5},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Fox, Levitin, Redman ' - 1994 - THE NOTION OF DATA AND ITS QUALITY DIMENSIONS.pdf:pdf},
isbn = {0306-4573},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Big Data},
mendeley-tags = {Big Data},
number = {1},
pages = {9--19},
title = {{The notion of data and its quality dimensions}},
url = {https://ac.els-cdn.com/0306457394900205/1-s2.0-0306457394900205-main.pdf?{\_}tid=spdf-a7cd125c-c769-4e24-a0fa-bf9097451f7a{\&}acdnat=1519917332{\_}d2e043f872f77df308aa31a663abcacd https://www.sciencedirect.com/science/article/pii/0306457394900205},
volume = {30},
year = {1994}
}
@misc{RCoreTeam2017,
abstract = {R is a system for statistical computation and graphics. It consists of a language plus a run-time environment with graphics, a debugger, access to certain system functions, and the ability to run programs stored in script files. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. The correct citation format is provided in R by running the citation fucntion citation().},
address = {Vienna, Austria},
author = {{R Core Team}},
keywords = {R!},
mendeley-tags = {R!},
publisher = {R Foundation for Statistical Computing},
title = {{R: A language and environment for statistical computing. R Foundation for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2017}
}
@article{Broadus1987,
abstract = {The definitions of the term bibliometrics as used in the literature are examined and evaluated. Most such definitions are held to be too broad. A new definition is proposed; then its advantages and possible defects pointed out. A crucial question is whether Zipf's law of word occurrence should be considered a part of this particular sub-discipline.},
author = {Broadus, R. N.},
doi = {10.1007/BF02016680},
isbn = {0138-9130},
issn = {01389130},
journal = {Scientometrics},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {nov},
number = {5-6},
pages = {373--379},
title = {{Toward a definition of "bibliometrics"}},
url = {http://link.springer.com/10.1007/BF02016680},
volume = {12},
year = {1987}
}
@article{Durairajan2018,
abstract = {In this paper we consider the risks to Internet infrastructure in the US due to sea level rise. Our study is based on sea level incursion projections from the National Oceanic and Atmospheric Administration (NOAA) [12] and Internet infrastructure deployment data from Internet Atlas [24]. We align the data formats and assess risks in terms of the amount and type of infrastructure that will be under water in different time intervals over the next 100 years. We find that 4,067 miles of fiber conduit will be under water and 1,101 nodes (e.g., points of presence and colocation centers) will be surrounded by water in the next 15 years. We further quantify the risks of sea level rise by defining a metric that considers the combination of geographic scope and Internet infrastructure density. We use this metric to examine different regions and find that the New York, Miami, and Seattle metropolitan areas are at highest risk. We also quantify the risks to individual service provider infrastructures and find that CenturyLink, Inteliquent, and AT{\&}T are at highest risk. While it is difficult to project the impact of countermeasures such as sea walls, our results suggest the urgency of developing mitigation strategies and alternative infrastructure deployments.},
author = {Durairajan, Ramakrishnan and Barford, Carol and Barford, Paul},
doi = {10.1145/3232755.3232775},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Durairajan, Barford, Barford - 2018 - Lights Out Climate Change Risk to Internet Infrastructure.pdf:pdf},
journal = {ANRW},
keywords = {CCS CONCEPTS • Networks → Physical links,KEYWORDS Physical Internet infrastructure,Network measurement,climate change,critical infrastructure,risks,sea level rise},
pages = {7},
title = {{Lights Out: Climate Change Risk to Internet Infrastructure}},
url = {https://doi.org/10.1145/3232755.3232775},
volume = {18},
year = {2018}
}
@article{Meehan2017,
abstract = {In this paper, we argue that in many " Big Data " applica-tions, getting data into the system correctly and at scale via traditional ETL (Extract, Transform, and Load) processes is a fundamental roadblock to being able to perform timely an-alytics or make real-time decisions. The best way to address this problem is to build a new architecture for ETL which takes advantage of the push-based nature of a stream pro-cessing system. We discuss the requirements for a streaming ETL engine and describe a generic architecture which sat-isfies those requirements. We also describe our implemen-tation of streaming ETL using a scalable messaging system (Apache Kafka), a transactional stream processing system (S-Store), and a distributed polystore (Intel's BigDAWG), as well as propose a new time-series database optimized to handle ingestion internally.},
author = {Meehan, John and Zdonik, Stan},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Meehan, Zdonik - 2017 - (jk-idea)(jk-related)Data Ingestion for the Connected World.pdf:pdf},
journal = {Cidr},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{(jk-idea)(jk-related)Data Ingestion for the Connected World}},
url = {https://cs.brown.edu/courses/cs227/papers/brown-data-ingest.pdf},
year = {2017}
}
@inproceedings{hutt2004overmouldingpolymers,
author = {Hutt, D A and Whalley, D C and Sarvar, F and Palmer, P J and Teh, N J},
booktitle = {Lead Free Technologies Conference, SSTC Winter Meeting},
month = {dec},
organization = {NPL},
pages = {.----},
title = {{Overmoulding of electronics with recyclable polymers}},
year = {2004}
}
@article{Ranjan2016,
abstract = {Welcome to the special issue of Springer Computing on Software Tools and Technolo- gies for Delivering Smart Media-Optimized Applications in the Cloud. This special issue deals with the intersection of big media data and it needs for exploiting elastic cloud computing services for efficiently processing and analysing such big data to support emerging media-optimised applications.},
author = {Ranjan, Rajiv and Georgakopoulos, Dimitrios and Wang, Lizhe},
doi = {10.1007/s00607-015-0471-8},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Ranjan, Georgakopoulos, Wang - 2016 - A note on software tools and technologies for delivering smart media-optimized big data applicatio.pdf:pdf},
issn = {0010-485X},
journal = {Computing},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {jan},
number = {1-2},
pages = {1--5},
publisher = {Springer Vienna},
title = {{A note on software tools and technologies for delivering smart media-optimized big data applications in the cloud}},
url = {http://link.springer.com/10.1007/s00607-015-0471-8},
volume = {98},
year = {2016}
}
@book{Chang2013,
abstract = {"Practical recipes for visualizing data"--Cover. Includes index. This practical guide provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R's graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project, and includes a discussion of how and why the recipe works. Most of the recipes use the ggplot2 package, a powerful and flexible way to make graphs in R. If you have a basic understanding of the R language, you're ready to get started. R basics -- Quickly exploring data -- Bar graphs -- Line graphs -- Scatter plots -- Summarized data distributions -- Annotations -- Axes -- Controlling the overall appearance of graphs -- Legends -- Facets -- Using colors in plots -- Miscellaneous graphs -- Output for presentation -- Getting your data into shape.},
author = {Chang, Winston.},
isbn = {1449316956},
pages = {396},
publisher = {O'Reilly},
title = {{R graphics cookbook}},
year = {2013}
}
@article{Remington2016,
abstract = {It is widely believed that a graphical user interface (GUI) is superior to a command line interface (CLI) for novice users, but less efficient than the CLI after practice. However, there appears to be no detailed study of the crossover interaction that this implies. The rate of learning may shed light on the reluctance of experienced users to adopt keyboard shortcuts, even though, when mastered, shortcut use would reduce task completion times. We report 2 experiments examining changes in the efficiency of and preference for keyboard input versus GUI with practice. Experiment 1 had separate groups of subjects make speeded choice responses to words on a 20-item list either by clicking on a tab in a dropdown menu (GUI version) or by entering a preassigned keystroke combination (CLI version). The predicted crossover was observed after approximately 200 responses. Experiment 2 showed that following training all but 1 subject in the CLI-trained group chose to continue using shortcuts. These results suggest that frequency of shortcut use is a function of ease of retrieval, which develops over the course of multiple repetitions of the command. We discuss possible methods for promoting shortcut learning and the practical implications of our results. [ABSTRACT FROM AUTHOR]},
author = {Remington, Roger W and Yuen, Ho Wang Holman and Pashler, Harold},
doi = {10.1037/xap0000069},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Remington, Yuen, Pashler - 2016 - With practice, keyboard shortcuts become faster than menu selection A crossover interaction.pdf:pdf},
issn = {1076898X},
journal = {Journal of Experimental Psychology: Applied},
keywords = {Command line interface,Graphical user interface,Human-computer interface,Keyboard shortcuts,Learning curves,Practice effects},
month = {mar},
number = {1},
pages = {95--106},
pmid = {26651347},
title = {{With practice, keyboard shortcuts become faster than menu selection: A crossover interaction}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xap0000069 http://www.ncbi.nlm.nih.gov/pubmed/26651347},
volume = {22},
year = {2016}
}
@inproceedings{Sagiroglu2013,
abstract = {Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it. Keywords—big},
archivePrefix = {arXiv},
arxivId = {aimag.v17i3.1230},
author = {Sagiroglu, Seref and Sinanc, Duygu},
booktitle = {2013 International Conference on Collaboration Technologies and Systems (CTS)},
doi = {10.1109/CTS.2013.6567202},
eprint = {aimag.v17i3.1230},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sagiroglu, Sinanc - 2013 - Big data A review.pdf:pdf},
isbn = {978-1-4673-6404-1},
issn = {9781467364034},
month = {may},
pages = {42--47},
pmid = {1467720121},
publisher = {IEEE},
title = {{Big data: A review}},
url = {http://ieeexplore.ieee.org/document/6567202/},
year = {2013}
}
@inproceedings{sarvar2004thermo-mechanicalelectronics,
author = {Sarvar, F and Teh, N J and Whalley, D C and Hutt, D A and Palmer, P J},
booktitle = {Proceedings of the 9th IEEE I-THERM Conference},
isbn = {0-7803-8357-5},
month = {may},
organization = {Las Vegas, USA},
pages = {465--472},
title = {{Thermo-mechanical Modeling of Polymer Encapsulated Electronics}},
year = {2004}
}
@article{Bowden,
annote = {id: 1; issn: print 0007-4853; issn: electronic 1475-2670; publication{\_}type: full{\_}text},
author = {Bowden, John},
doi = {10.1017/S0007485300008579},
issn = {0007-4853},
journal = {Bulletin of entomological research},
number = {04},
pages = {535},
title = {{An Analysis of Factors Affecting Catches of Insects in Light-Traps }},
url = {http://dx.doi.org/10.1017/s0007485300008579},
volume = {72}
}
@article{Sarmah2013,
abstract = {Formalization of human thinking helps in fostering the process of learning by giving an explicit representation to human thoughts. Formal Concept Analysis (FCA) finds it's core here. It considers a “concept” as a formal unit of human thought. A concept is represented as a set of inter related objects called the extent and the set of the properties of these objects, called the intent. Making use of the mathematical principles of Lattice Theory and Map Theory of Abstract Algebra, a set of tools and algorithms have been developed in FCA. These helps us to analyze and represent any context as a relation between it's extent and intent. Concepts drawn from the subsets of the extent and intent can be organized in the form of a lattice giving a subsumption hierarchy. Such concept lattices could be maintained by different operations on the lattice like scaling, pruning, navigating etc. A host of applications and software have been developed over the years which serves the usage of FCA tools and processes for specific purposes in various fields. This paper reviews the theoretical foundation, research and applications of FCA in different areas. The paper projects current trends in FCA and concludes with a discussion on open issues and limitations of FCA.},
author = {Sarmah, Achyanta Kumar and Hazarika, Shyamanta M. and Sinha, Smriti Kumar},
doi = {10.1007/s10462-013-9404-0},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sarmah, Hazarika, Sinha - 2013 - Formal concept analysis current trends and directions.pdf:pdf},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Concept,FCA,Formal concept,Formal context},
month = {jun},
number = {1},
pages = {47--86},
publisher = {Springer Netherlands},
title = {{Formal concept analysis: current trends and directions}},
url = {http://link.springer.com/10.1007/s10462-013-9404-0 http://www.scopus.com/inward/record.url?eid=2-s2.0-84929521356{\&}partnerID=tZOtx3y1},
volume = {44},
year = {2013}
}
@misc{Germuska2018,
abstract = {csvkit is a suite of command-line tools for converting to and working with CSV, the king of tabular file formats. It is inspired by pdftk, gdal and the original csvcut tool by Joe Germuska and Aaron Bycoffe. If you need to do more complex data analysis than csvkit can handle, use agate. Important links: Repository: https://github.com/wireservice/csvkit Issues: https://github.com/wireservice/csvkit/issues Documentation: http://csvkit.rtfd.org/ Schemas: https://github.com/wireservice/ffs Buildbot: https://travis-ci.org/wireservice/csvkit},
author = {Germuska, Joe and Bycoffe, Aaron},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {csvkit 1.0.2 — csvkit 1.0.2 documentation},
url = {https://csvkit.readthedocs.io/en/1.0.2/index.html{\#}},
urldate = {2018-01-21},
year = {2018}
}
@article{Baker2016,
abstract = {A Nature survey lifts the lid on how researchers view the ‘crisis' rocking science and what they think will help.},
author = {Baker, Monya and Penny, Dan},
doi = {10.1038/533452A},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Baker, Penny - 2016 - Is there a reproducibility crisis.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7604},
pages = {452--454},
pmid = {27225100},
title = {{Is there a reproducibility crisis?}},
volume = {533},
year = {2016}
}
@inproceedings{Coallier2018,
abstract = {“IoT systems have the following characteristics: • They are ‘network centric' since we have interconnected entities. • They are essentially distributed in nature since we can have layered tiers of computing as illustrated in figure 1. These layers range from the Cloud, where the heavy analytics is done to the ‘edge' or the ‘fog' where more real-time processing and control is distributed, to finally the ‘extreme edge' or the ‘mist' where the ‘things' are. These ‘things' can range from passive sensors to semi- autonomous and autonomous devices. • They are data intensive. An IoT system includes sensors that generate large amount of data, data that must then be processed to add value. For instance, a small smart factory generates 0,3 TB of data per days [2]. In the case of a smart car, it is about 4 TB per day [3].},
address = {Paris, France},
author = {Coallier, F},
booktitle = {13th System of Systems Engineering Conference (SoSE)},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Coallier - 2018 - A system of systems engineering perspective on IoT trustworthiness.pdf:pdf},
publisher = {IEEE},
title = {{A system of systems engineering perspective on IoT trustworthiness}},
year = {2018}
}
@article{Milcu2013,
abstract = {Cultural ecosystem services constitute a growing field of research that is characterized by an increasing number of publications from various academic disciplines. We conducted a semiquantitative review of publications explicitly dealing with cultural ecosystem services. Our aims were: (1) to provide an overview of the current state of research, (2) to classify the diversity of research approaches by identifying clusters of publications that address cultural ecosystem services in similar ways, and (3) to highlight some important challenges for the future of cultural ecosystem services research. We reviewed 107 publications and extracted 20 attributes describing their type and content, including methods, scales, drivers of change, and trade-offs between services. Using a cluster analysis on a subset of attributes we identified five groups of publications: Group 1, conceptual focus, deals with theoretical issues; Group 2, descriptive reviews, consists mostly of desktop studies; Group 3, localized outcomes, deals with case studies coming from different disciplines; Group 4, social and participatory, deals mainly with assessing preferences and perceptions; and Group 5, economic assessments, provides economic valuations. Emerging themes in cultural ecosystem services research relate to improving methods for cultural ecosystem services valuation, studying cultural ecosystem services in the context of ecosystem service bundles, and more clearly articulating policy implications. Based on our findings, we conclude that: (1) cultural ecosystem services are well placed as a tool to bridge gaps between different academic disciplines and research communities, (2) capitalizing on the societal relevance of cultural ecosystem services could help address real-world problems, and (3) cultural ecosystem services have the potential to foster new conceptual links between alternative logics relating to a variety of social and ecological issues. {\textcopyright} 2013 by the author(s).},
author = {Milcu, Andra Ioana and Hanspach, Jan and Abson, David and Fischer, Joern},
doi = {10.5751/ES-05790-180344},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Milcu et al. - 2013 - Cultural ecosystem services A literature review and prospects for future research.pdf:pdf},
issn = {17083087},
journal = {Ecology and Society},
keywords = {Aesthetic values,Bundling,CES valuation,Cluster analysis,Cultural heritage,Cultural landscapes,Drivers of change,Intangible benefits,Landscape values,Nonuse values,Policy implications,Recreation and ecotourism},
month = {sep},
number = {3},
title = {{Cultural ecosystem services: A literature review and prospects for future research}},
volume = {18},
year = {2013}
}
@article{Rossell2014,
abstract = {Big Data brings unprecedented power to address scientific, economic and societal issues, but also amplifies the possibility of certain pitfalls. These include using purely data-driven approaches that disregard understanding the phenomenon under study, aiming at a dynamically moving target, ignoring critical data collection issues, summarizing or preprocessing the data inadequately and mistaking noise for signal. We review some success stories and illustrate how statistical principles can help obtain more reliable information from data. We also touch upon current challenges that require active methodological research, such as strategies for efficient computation, integration of heterogeneous data, extending the underlying theory to increasingly complex questions and, perhaps most importantly, training a new generation of scientists to develop and deploy these strategies.},
author = {Rossell, David},
doi = {10.7203/metode.83.3590},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Rossell - 2014 - Big Data and statisitics A statistician's perspective.pdf:pdf},
issn = {2174-9221},
journal = {M{\`{e}}tode Revista de difusi{\'{o}} de la investigaci{\'{o}}},
keywords = {Big Data,case studies,challenges,pitfalls,statistics},
pages = {143--149},
pmid = {27722040},
publisher = {NIH Public Access},
title = {{Big Data and statisitics: A statistician's perspective}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27722040 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5053772 https://ojs.uv.es/index.php/Metode/article/view/3590},
volume = {5},
year = {2014}
}
@article{Altwegg2019,
abstract = {Large-scale citizen-science projects, such as atlases of species distribution, are an important source of data for macroecological research, for understanding the effects of climate change and other drivers on biodiversity, and for more applied conservation tasks, such as early-warning systems for biodiversity loss. However, citizen-science data are challenging to analyse because the observation process has to be taken into account. Typically, the observation process leads to heterogeneous and non-random sampling, false absences, false detections, and spatial correlations in the data. Increasingly, occupancy models are being used to analyse atlas data. We advocate a dual approach to strengthen inference from citizen science data for the questions the programme is intended to address: (a) the survey design should be chosen with a particular set of questions and associated analysis strategy in mind and (b) the statistical methods should be tailored not only to those questions but also to the specific characteristics of the data. We review the consequences of particular survey design choices that typically need to be made in atlas-style citizen-science projects. These include spatial resolution of the sampling units, allocation of effort in space, and collection of information about the observation process. On the analysis side, we review extensions of the basic occupancy models that are frequently necessary with atlas data, including methods for dealing with heterogeneity, non-independent detections, false detections, and violation of the closure assumption. New technologies, such as cell-phone apps and fixed remote detection devices, are revolutionizing citizen-science projects. There is an opportunity to maximize the usefulness of the resulting datasets if the protocols are rooted in robust statistical designs and data analysis issues are being considered. Our review provides guidelines for designing new projects and an overview of the current methods that can be used to analyse data from such projects.},
author = {Altwegg, Res and Nichols, James D.},
doi = {10.1111/2041-210X.13090},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Altwegg, Nichols - 2019 - Occupancy models for citizen-science data.pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {bird atlas,citizen science project,occupancy model,survey design},
month = {jan},
number = {1},
pages = {8--21},
publisher = {British Ecological Society},
title = {{Occupancy models for citizen-science data}},
volume = {10},
year = {2019}
}
@inproceedings{palmer2001costcosts,
address = {Kauai, Hawaii, USA},
author = {Palmer, P J and Hyslop, S M and {'Palmer PJ'}, 'Hyslop SM'},
booktitle = {Proceedings of InterPACK'01, The PACIFIC RIM/International, Intersociety, Electronic Packaging Mechanical/Business Conference {\&} Exhibition},
month = {jul},
organization = {Kauai, Hawaii, USA},
pages = {IPACK2001----15655},
title = {{Cost Expert - A Demonstrator for Advanced Methodologies for the Monitoring and Assessment of Lifecycle Costs}},
volume = {IPACK2001-},
year = {2001}
}
@incollection{Wrongsortofsnow,
abstract = {The notorious excuse offered by British Rail's director of operations in February 1991 after heavy snowfalls had caused widespread delays},
author = {Ayto, John and Crofton, Ian},
booktitle = {Brewer's Dictionary of Modern Phrase {\&} Fable.},
edition = {2},
editor = {Ayto, John and Crofton, Ian},
isbn = {9780199916108},
publisher = {Chambers Harrap Publishers},
title = {{Wrong sort of snow}},
year = {2009}
}
@article{Dryden2018,
abstract = {The realm of big data is a very wide and varied one. We discuss old, new, small and big data, with some of the important challenges including dealing with highly-structured and object-oriented data. In many applications the objective is to discern patterns and learn from large datasets of historical data. We shall discuss such issues in some transportation network applications in non-academic settings, which are naturally applicable to other situations. Vital aspects include dealing with logistics, coding and choosing appropriate statistical methodology, and we provide a summary and checklist for wider implementation.},
author = {Dryden, Ian L. and Hodge, David J.},
doi = {10.1016/j.spl.2018.02.013},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Dryden, Hodge - 2018 - Journeys in big data statistics.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Big data,Networks,Object-oriented data,Transport},
month = {may},
pages = {121--125},
publisher = {North-Holland},
title = {{Journeys in big data statistics}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300580},
volume = {136},
year = {2018}
}
@article{Dumbill2013,
abstract = {The combination of cloud computing, internet, and mobile devices is moving ever greater portions of our lives and businesses inside the data center. As a result, we are generating, and must analyze, vast and varied flows of information. The techniques developed to do ... $\backslash$n},
author = {Dumbill, Edd},
doi = {10.1089/big.2012.1503},
isbn = {2167-6461$\backslash$n2167-647X},
issn = {2167-6461},
journal = {Big Data},
month = {mar},
number = {1},
pages = {1--2},
pmid = {27447028},
title = {{Making Sense of Big Data}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27447028 http://www.liebertpub.com/doi/10.1089/big.2012.1503 http://online.liebertpub.com/doi/abs/10.1089/big.2012.1503},
volume = {1},
year = {2013}
}
@article{Wong2018,
abstract = {Lay analysts often test hypotheses incorrectly. They also need help to find interesting hypotheses. They usually do not know what to do next after testing an initial hypothesis. We discuss their common mistakes, and also suggest practical tactics for their problems.},
author = {Wong, Limsoon},
doi = {10.1016/j.spl.2018.02.033},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wong - 2018 - Big data and a bewildered lay analyst.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Data analysis tactics,Exception,Hypothesis exploration,Hypothesis testing,Trend enhancement,Trend reversal},
month = {may},
pages = {73--77},
publisher = {North-Holland},
title = {{Big data and a bewildered lay analyst}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300786},
volume = {136},
year = {2018}
}
@misc{Leek2013,
abstract = {Jeffrey Leek, Assistant Professor of Biostatistics at John Hopkins Bloomberg School of Public Health, has identified six(6) archetypical analyses. As presented, they range from the least to most complex, in terms of knowledge, costs, and time.In summary, Descriptive Exploratory Inferential Predictive Causal Mechanistic},
author = {Leek, Jeffery},
booktitle = {Data Scientist Insights},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Leek - 2013 - Six Types Of Analyses Every Data Scientist Should Know - Data Scientist Insights.pdf:pdf},
title = {{Six Types Of Analyses Every Data Scientist Should Know - Data Scientist Insights}},
url = {https://datascientistinsights.com/2013/01/29/six-types-of-analyses-every-data-scientist-should-know/},
urldate = {2019-01-02},
year = {2013}
}
@inproceedings{jaggernauth2002foresightcircuits,
author = {Jaggernauth, W A and Webb, D P and Cottrill, M C and Palmer, P J and Conway, P P and West, A A},
booktitle = {Proceedings of the Society of Automotive Engineers World Congress and Exposition, SAE Paper No 2002-01-1129},
issn = {0148-7191},
month = {mar},
organization = {Detroit, USA},
pages = {1--5},
title = {{Foresight Vehicle: Specification and Acceptability Criteria for Copper-Clad Dielectric Materials Used in Large Automotive Flexible Printed Circuits}},
year = {2002}
}
@article{Wickham2013,
abstract = {In spatial statistics the ability to visualize data and models superimposed with their basic social landmarks and geographic context is invaluable. ggmap is a new tool which enables such visualization by combining the spatial information of static maps from Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps with the layered grammar of graphics implementation of ggplot2. In addition, several new utility functions are introduced which allow the user to access the Google Geocoding, Distance Matrix, and Directions APIs. The result is an easy, consistent and modular framework for spatial graphics with several convenient tools for spatial data analysis.},
author = {Wickham, Hadley and Kahle, David},
doi = {10.1023/A:1009843930701},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Wickham, Kahle - 2013 - ggmap Spatial Visualization with ggplot2.pdf:pdf},
isbn = {2073-4859},
issn = {ISSN 2073-4859},
journal = {The R Journal},
number = {1},
pages = {144--161},
pmid = {19960118},
title = {{ggmap: Spatial Visualization with ggplot2}},
url = {http://stat405.had.co.nz/ggmap.pdf},
volume = {5},
year = {2013}
}
@article{Stone2012,
abstract = {Artificial lighting is a key biodiversity threat and produces 1900 million tonnes of CO 2 emissions globally, more than three times that produced by aviation. The need to meet climate change targets has led to a global increase in energy-efficient light sources such as high-brightness light-emitting diodes (LEDs). Despite the energetic benefits of LEDs, their ecological impacts have not been tested. Using an experimental approach, we show that LED street lights caused a reduction in activity of slow-flying bats ( Rhinolophus hipposideros and Myotis spp.). Both R. hipposideros and Myotis spp. activities were significantly reduced even during low light levels of 3.6 lux. There was no effect of LED lighting on the relatively fast-flying Pipistrellus pipistrellus, Pipistrellus pygmaeus and Nyctalus/Eptesicus spp. We provide the first evidence of the effects of LED lights on bats. Despite having considerable energy-saving benefits, LED lights can potentially fragment commuting routes for bats with associated negative conservation consequences. Our results add to the growing evidence of negative impacts of lighting on a wide range of taxa. We highlight the complexities involved in simultaneously meeting targets for reduction of greenhouse gas emissions and biodiversity loss. New lighting strategies should integrate climate change targets with the cultural, social and ecological impacts of emerging lighting technologies.},
author = {Stone, Emma L and Jones, Gareth and Harris, Stephen},
doi = {10.1111/j.1365-2486.2012.02705.x},
issn = {1365-2486},
journal = {Global Change Biology},
keywords = {LED street lights,anthropogenic impacts,biodiversity conservation,climate change targets,light pollution,light-emitting diodes},
number = {8},
pages = {2458--2465},
title = {{Conserving energy at a cost to biodiversity? Impacts of LED lighting on bats}},
url = {http://dx.doi.org/10.1111/j.1365-2486.2012.02705.x},
volume = {18},
year = {2012}
}
@article{Olhede2018,
abstract = {The ubiquity of sensing devices, the low cost of data storage, and the commoditization of computing have together led to a big data revolution. We discuss the implication of this revolution for statistics, focusing on how our discipline can best contribute to the emerging field of data science.},
author = {Olhede, Sofia C. and Wolfe, Patrick J.},
doi = {10.1016/j.spl.2018.02.042},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Olhede, Wolfe - 2018 - The future of statistics and data science.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Algorithmic transparency,Data analysis,Data governance,Predictive analytics,Statistical inference,Structured and unstructured data},
month = {may},
pages = {46--50},
publisher = {North-Holland},
title = {{The future of statistics and data science}},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300877?via{\%}3Dihub},
volume = {136},
year = {2018}
}
@misc{Heilmeier1977,
abstract = {George H. Heilmeier, a former DARPA director (1975-1977), crafted a set of questions known as the "Heilmeier Catechism" to help Agency officials think through and evaluate proposed research programs. What are you trying to do? Articulate your objectives using absolutely no jargon. How is it done today, and what are the limits of current practice? What is new in your approach and why do you think it will be successful? Who cares? If you are successful, what difference will it make? What are the risks? How much will it cost? How long will it take? What are the mid-term and final “exams” to check for success?},
author = {Heilmeier, George},
booktitle = {DARPA website},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Heilmeier - 1977 - The Heilmeier Catechism.pdf:pdf},
keywords = {Big Data},
mendeley-tags = {Big Data},
title = {{The Heilmeier Catechism}},
url = {https://www.darpa.mil/work-with-us/heilmeier-catechism},
urldate = {2018-04-04},
year = {1977}
}
@article{Kulkarni2015,
abstract = {In today's world the amount of data has been exploding. Companies capture trillions of bytes of information every day about their customers, suppliers, and operations. Physical devices such as mobile phones, smart phones, and various other communicating devices generate loads of data similarly data on social networking sites will continue to grow. Large pools of data that can be captured, communicated, aggregated, stored, and analyzed—is now part of every sector and function of the global economy. Hence it's high time to preserve and secure this data and apply tools that can better visualize and analyse data. There are many data visualizing and analytical tools available in the market today and analyst are still working to improve these tools. In this paper we present a brief review of most of the data visualization and analysis tools. We have focused on two such tools i.e. “Data Wrangler” and “Tableau Public”. Wrangler combines direct manipulation of visualized data, enabling analysts to iteratively explore the space of applicable operations and preview their effects. Through the case study presented in the paper we have shown that Wrangler significantly reduces editing time as compared to manual editing and one can focus and spend more time on analysis instead of editing.” Tableau Public” turns data into any number of visualizations from simple to complex. It also provides drag and drop options into work area which we have applied on the data and presented in this paper.},
annote = {Refers to csvkit and ranks tools to use by expert level.},
author = {Kulkarni, Shibani and Takawale, Neeta},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Kulkarni, Takawale - 2015 - To study the application of Data Visualization and Analysis tools.pdf:pdf},
issn = {2454-8499},
journal = {International Research Journal of Multidisciplinary Studies},
keywords = {Big Data,Data Wrangler,Data cleaning,Tableau Public,transformation,visualization},
mendeley-tags = {Big Data},
month = {nov},
number = {5},
title = {{To study the application of Data Visualization and Analysis tools}},
url = {http://irjms.in/sites/irjms/index.php/files/article/view/61},
volume = {1},
year = {2015}
}
@inproceedings{DeMauro2015,
abstract = {Although Big Data is a trending buzzword in both academia and the industry, its meaning is still shrouded by much conceptual vagueness. The term is used to describe a wide range of concepts: from the technological ability to store, aggregate, and process data, to the cultural shift that is pervasively invading business and society, both drowning in information overload. The lack of a formal definition has led research to evolve into multiple and inconsistent paths. Furthermore, the existing ambiguity among researchers and practitioners undermines an efficient development of the subject. In this paper we have reviewed the existing literature on Big Data and analyzed its previous definitions in order to pursue two results: first, to provide a summary of the key research areas related to the phenomenon, identifying emerging trends and suggesting opportunities for future development; second, to provide a consensual definition for Big Data, by synthesizing common themes of existing works and patterns in previous definitions. ABSTRACT FROM AUTHOR]; Copyright of AIP Conference Proceedings is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.02998v1},
author = {{De Mauro}, Andrea and Greco, Marco and Grimaldi, Michele},
booktitle = {AIP Conference Proceedings},
doi = {10.1063/1.4907823},
eprint = {arXiv:1804.02998v1},
isbn = {9780735412835},
issn = {15517616},
month = {feb},
number = {1},
pages = {97--104},
publisher = {American Institute of Physics},
title = {{What is big data? A consensual definition and a review of key research topics}},
url = {http://aip.scitation.org/doi/abs/10.1063/1.4907823},
volume = {1644},
year = {2015}
}
@article{Oliver2010,
abstract = {Habitat heterogeneity is often suggested as being important for the stability of populations, and promoted as a means to aid the conservation of species, but the evidence for such an assumption is poor. Here we show that heterogeneous landscapes that contain a variety of suitable habitat types are associated with more stable population dynamics for 35 British butterfly species from 166 sites. In addition, topographic heterogeneity may also promote stability. Our results were robust to different measures of population variability, differences in mean abundance among sites, and to the spatial scale (radius 1-5 km around the centres of sites) at which landscapes were analysed. Responses to landscape heterogeneity differed among species; for more mobile 'wider-countryside' species, habitat heterogeneity at larger landscape scales had the strongest effect on population dynamics. We suggest that heterogeneous landscapes offer a greater range of resources and microclimates, which can buffer populations against climatic variation and generate more stable population dynamics. {\textcopyright} 2010 Blackwell Publishing Ltd/CNRS.},
author = {Oliver, Tom and Roy, David B. and Hill, Jane K. and Brereton, Tom and Thomas, Chris D.},
doi = {10.1111/j.1461-0248.2010.01441.x},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Oliver et al. - 2010 - Heterogeneous landscapes promote population stability.pdf:pdf},
issn = {1461023X},
journal = {Ecology Letters},
keywords = {Climatic extremes,Coefficient of variation,Environmental diversity,Habitat management,Landscape ecology,Microclimatic variability,Specialist species},
month = {apr},
number = {4},
pages = {473--484},
title = {{Heterogeneous landscapes promote population stability}},
volume = {13},
year = {2010}
}
@article{Castells1999,
abstract = {In the following paper, presented at the UNRISD conference on Information Technologies and Social Development (Geneva, June 1998), Manuel Castells examines the profile of this new world, centred around multinational corporations, global financial markets and a highly concentrated system of technological research and development. He stresses the extreme flexibility of the system, which allows it to link up everything that is valuable according to dominant values and interests, while disconnecting everything that is not valuable, or becomes devalued. This simultaneous capacity to include and exclude people, territories and activities is based upon a capacity to network.},
author = {Castells, Manuel and Nations, Palais},
doi = {ISSN 1012-6511},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Castells, Nations - 1999 - Information Technology , Globalization and Social Development f or Social Development.pdf:pdf},
isbn = {0780385233},
issn = {1012-6511},
journal = {Information Age},
number = {114},
pages = {1477--1483},
title = {{Information Technology , Globalization and Social Development f or Social Development}},
url = {www.unrisd.org http://www.unrisd.org/unrisd/website/document.nsf/462fc27bd1fce00880256b4a0060d2af/f270e0c066f3de7780256b67005b728c/{\$}FILE/dp114.pdf},
volume = {9},
year = {1999}
}
@article{Soga2016a,
abstract = {Children are becoming less likely to have direct contact with nature. This ongoing loss of human interactions with nature, the extinction of experience, is viewed as one of the most fundamental obstacles to addressing global environmental challenges. However, the consequences for biodiversity conservation have been examined very little. Here, we conducted a questionnaire survey of elementary schoolchildren and investigated effects of the frequency of direct (participating in nature-based activities) and vicarious experiences of nature (reading books or watching TV programs about nature and talking about nature with parents or friends) on their affective attitudes (individuals' emotional feelings) toward and willingness to conserve biodiversity. A total of 397 children participated in the surveys in Tokyo. Children's affective attitudes and willingness to conserve biodiversity were positively associated with the frequency of both direct and vicarious experiences of nature. Path analysis showed that effects of direct and vicarious experiences on children's willingness to conserve biodiversity were mediated by their affective attitudes. This study demonstrates that children who frequently experience nature are likely to develop greater emotional affinity to and support for protecting biodiversity. We suggest that children should be encouraged to experience nature and be provided with various types of these experiences.},
author = {Soga, Masashi and Gaston, Kevin J. and Yamaura, Yuichi and Kurisu, Kiyo and Hanaki, Keisuke},
doi = {10.3390/ijerph13060529},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Soga et al. - 2016 - Both direct and vicarious experiences of nature affect children's willingness to conserve biodiversity.pdf:pdf},
issn = {16604601},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Biodiversity conservation,Biophilia,Conservation psychology,Ecosystem services,Environmental education,Global change,Human-nature interactions,Pro-environmental behavior,Public health,Well-being},
month = {jun},
number = {6},
publisher = {MDPI AG},
title = {{Both direct and vicarious experiences of nature affect children's willingness to conserve biodiversity}},
volume = {13},
year = {2016}
}
@article{Butts2008,
abstract = {Effective memory structures for relational data within R must be capable of representing a wide range of data while keeping overhead to a minimum. The network package provides an class which may be used for encoding complex relational structures composed a vertex set together with any combination of undirected/directed, valued/unvalued, dyadic/hyper, and single/multiple edges; storage requirements are on the order of the number of edges involved. Some simple constructor, interface, and visualization functions are provided, as well as a set of operators to facilitate employment by end users. The package also supports a C-language API, which allows developers to work directly with network objects within backend code.},
author = {Butts, Carter T},
doi = {10.18637/jss.v024.i02},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Butts - 2008 - network A package for managing relational data in R.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Data structures,Graphs,Network,R,Relational data,Statnet},
number = {2},
pages = {39},
title = {{network: A package for managing relational data in R}},
url = {http://www.jstatsoft.org/v24/i02/paper},
volume = {24},
year = {2008}
}
@article{Siemieniuch2014,
abstract = {Socio-technical issues for Systems of Systems (SoS) differ in several ways from those for systems, mainly because the individual systems that are components of the SoS are usually owned by different organi-sations, each responsible for the optimisation and operation of its own system. Consequently, man-agement of the SoS is about negotiation and management of the interfaces. Because of issues of Intellectual Property Rights (IPRs), commercial conﬁdence, and the like, there is seldom sufﬁcient, timely information in circulation about the SoS. Surprises are endemic to SoS, and resilience is a fundamental requirement. This paper outlines the different characteristics of SoS compared to ordinary systems, discusses many of the socio-technical issues involved, and then outlines a generic approach to these issues, treating the SoS as a ‘wicked problem'. Endemic to this is the need for governance, which is discussed brieﬂy. This is followed by a description of the evident gaps in knowledge about the func-tioning of SoS, and a listing of tool classes, the development of which would enable progress to be made more effectively. Finally, the paper discusses how the SoS approach might be the best way to entrain ICT to address global drivers, thus pointing to the importance of the SoS approach.},
annote = {Paper in},
author = {Siemieniuch, C E and Sinclair, M A},
journal = {Applied Ergonomics},
keywords = {systems},
number = {1},
pages = {85--98},
title = {{Extending systems ergonomics thinking to accommodate the socio-technical issues of Systems of Systems}},
volume = {45},
year = {2014}
}
@article{Hahn2015,
abstract = {In agricultural landscapes, field margins are potential habitats for moths and butterflies (Lepidoptera). However, because of their proximity to agricultural sites, field margins can be affected by inputs of pesticides and fertilizers. In the present study, we assessed the use of field margins by caterpillars as habitat. Furthermore, the effects of realistic field margin input rates of various agrochemicals on moths, especially on their caterpillar stages, were studied in field, semi-field, and laboratory experiments. Our monitoring results indicate that, although caterpillars were found in field margins, their mean abundance was 35-60{\%} lower compared to meadows. In a field experiment, the insecticide treatment (pyrethroid, lambda-cyhalothrin) significantly reduced the number of caterpillars and only 15{\%} of the sampled caterpillars occurred in the insecticide-treated plots. Furthermore, the insecticide affected the community composition of the caterpillars, whereas the fertilizer treatment slightly increased the caterpillar abundance. In laboratory experiments, Mamestra brassicae caterpillars were shown to be very sensitive when exposed to insecticide-treated leaves (rate that kills 50{\%} of the test caterpillars (LR50) after 48. h: 0.78{\%} of the recommended field rate; this rate corresponds to the arable spray drift input in field margins at a distance of 3-4. m from the crop), and the caterpillars also appeared to avoid feeding on the treated leaves. In addition, in a semi-field study, 40{\%} fewer eggs of Hadena bicruris moths were found on Silene latifolia plants sprayed with the insecticide compared to control plants and the flowers of insecticide-treated plants were less likely to be pollinated by moths. Overall, these studies illustrate that moths use field margins as habitats and that they can be affected by realistic input rates of agrochemicals. As caterpillars are important prey organisms and adult moths can act as pollinators, inputs of agrochemicals in field margins should be reduced to maintain biodiversity in agricultural landscapes.},
author = {Hahn, Melanie and Schotth{\"{o}}fer, Annalena and Schmitz, Juliane and Franke, Lea A. and Br{\"{u}}hl, Carsten A.},
doi = {10.1016/j.agee.2015.04.002},
isbn = {01678809},
issn = {01678809},
journal = {Agriculture, Ecosystems and Environment},
keywords = {Agro-ecosystem,Caterpillars,Fertilizer,Herbicide,Insecticide,Moth pollination,Non-crop habitats},
pages = {153--162},
title = {{The effects of agrochemicals on Lepidoptera, with a focus on moths, and their pollination service in field margin habitats}},
volume = {207},
year = {2015}
}
@techreport{Beyer2012,
abstract = {"Big data" warrants innovative processing solutions for a variety of new and existing data to provide real business benefits. But processing large volumes or wide varieties of data remains merely a technological solution unless it is tied to business goals and objectives.},
address = {Stamford, CT.},
archivePrefix = {arXiv},
arxivId = {G00235055},
author = {Beyer, Mark A. and Laney, Douglas},
booktitle = {Gartner Publications},
doi = {G00235055},
eprint = {G00235055},
institution = {Gartner},
isbn = {9781612083957},
issn = {00010782},
number = {June},
pages = {1--9},
title = {{The Importance of 'Big Data': A Definition}},
url = {https://www.gartner.com/doc/2057415/importance-big-data-definition},
volume = {i},
year = {2012}
}
@misc{pandoc2018,
abstract = {Pandoc is a Haskell library for converting from one markup format to another, and a command-line tool that uses this library. Pandoc can read Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, MultiMarkdown, and (subsets of) Textile, reStructuredText, HTML, LaTeX, MediaWiki markup, TWiki markup, TikiWiki markup, Creole 1.0, Haddock markup, OPML, Emacs Org mode, DocBook, JATS, Muse, txt2tags, Vimwiki, EPUB, ODT, and Word docx. Pandoc can write plain text, Markdown, CommonMark, PHP Markdown Extra, GitHub-Flavored Markdown, MultiMarkdown, reStructuredText, XHTML, HTML5, LaTeX (including beamer slide shows), ConTeXt, RTF, OPML, DocBook, JATS, OpenDocument, ODT, Word docx, GNU Texinfo, MediaWiki markup, DokuWiki markup, ZimWiki markup, Haddock markup, EPUB (v2 or v3), FictionBook2, Textile, groff man, groff ms, Emacs Org mode, AsciiDoc, InDesign ICML, TEI Simple, Muse, PowerPoint slide shows and Slidy, Slideous, DZSlides, reveal.js or S5 HTML slide shows. It can also produce PDF output on systems where LaTeX, ConTeXt, pdfroff, wkhtmltopdf, prince, or weasyprint is installed.},
author = {Pandoc.org},
doi = {10.1038/171737a0},
title = {{Pandoc a universal document converter https://pandoc.org}},
url = {https://pandoc.org/MANUAL.html},
year = {2018}
}
@article{Tenopir2011,
abstract = {BACKGROUND: Scientific research in the 21st century is more data intensive and collaborative than in the past. It is important to study the data practices of researchers--data accessibility, discovery, re-use, preservation and, particularly, data sharing. Data sharing is a valuable part of the scientific method allowing for verification of results and extending research from prior results. METHODOLOGY/PRINCIPAL FINDINGS: A total of 1329 scientists participated in this survey exploring current data sharing practices and perceptions of the barriers and enablers of data sharing. Scientists do not make their data electronically available to others for various reasons, including insufficient time and lack of funding. Most respondents are satisfied with their current processes for the initial and short-term parts of the data or research lifecycle (collecting their research data; searching for, describing or cataloging, analyzing, and short-term storage of their data) but are not satisfied with long-term data preservation. Many organizations do not provide support to their researchers for data management both in the short- and long-term. If certain conditions are met (such as formal citation and sharing reprints) respondents agree they are willing to share their data. There are also significant differences and approaches in data management practices based on primary funding agency, subject discipline, age, work focus, and world region. CONCLUSIONS/SIGNIFICANCE: Barriers to effective data sharing and preservation are deeply rooted in the practices and culture of the research process as well as the researchers themselves. New mandates for data management plans from NSF and other federal agencies and world-wide attention to the need to share and preserve data could lead to changes. Large scale programs, such as the NSF-sponsored DataNET (including projects like DataONE) will both bring attention and resources to the issue and make it easier for scientists to apply sound data management principles.},
archivePrefix = {arXiv},
arxivId = {0803973233},
author = {Tenopir, Carol and Allard, Suzie and Douglass, Kimberly and Aydinoglu, Arsev Umur and Wu, Lei and Read, Eleanor and Manoff, Maribeth and Frame, Mike},
doi = {10.1371/journal.pone.0021101},
editor = {Neylon, Cameron},
eprint = {0803973233},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Tenopir et al. - 2011 - Data sharing by scientists Practices and perceptions.pdf:pdf},
isbn = {1932-6203 (Electronic) 1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
month = {jun},
number = {6},
pages = {e21101},
pmid = {21738610},
title = {{Data sharing by scientists: Practices and perceptions}},
url = {http://dx.plos.org/10.1371/journal.pone.0021101},
volume = {6},
year = {2011}
}
@techreport{Annon2018a,
author = {Annon},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Annon - 2018 - User guide for CNC 6040 Series Main features for this mini CNC router machine CNC 6040Z-S65J-V2 Water-cooled 800W 1500W S.pdf:pdf},
title = {{User guide for CNC 6040 Series Main features for this mini CNC router machine CNC 6040Z-S65J-V2 Water-cooled 800W /1500W Spindle 220V available 3 Axis Ball screw USB interface Limited swicth added}},
url = {https://www.china-cncrouter.com//downfile/2015120416433598141.pdf},
year = {2018}
}
@article{Lawrence2011,
abstract = {This paper discusses many of the issues associated with formally publishing data in academia, focusing primarily on the structures that need to be put in place for peer review and formal citation of datasets. Data publication is becoming increasingly important to the scientific community, as it will provide a mechanism for those who create data to receive academic credit for their work and will allow the conclusions arising from an analysis to be more readily verifiable, thus promoting transparency in the scientific process. Peer review of data will also provide a mechanism for ensuring the quality of datasets, and we provide suggestions on the types of activities one expects to see in the peer review of data. A simple taxonomy of data publication methodologies is presented and evaluated, and the paper concludes with a discussion of dataset granularity, transience and semantics, along with a recommended human-readable citation syntax.},
author = {Lawrence, Bryan and Jones, Catherine and Matthews, Brian and Pepler, Sam and Callaghan, Sarah},
doi = {10.2218/ijdc.v6i2.205},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Lawrence et al. - 2011 - Citation and peer review of data moving towards formal data publication.pdf:pdf},
isbn = {1746-8256},
issn = {1746-8256},
journal = {International Journal of Digital Curation},
month = {jul},
number = {2},
pages = {4--37},
title = {{Citation and peer review of data: moving towards formal data publication}},
url = {http://www.ijdc.net/article/view/181 http://www.ijdc.net/index.php/ijdc/article/view/181/265},
volume = {6},
year = {2011}
}
@article{VanDenBroeck2005,
abstract = {In this policy forum the authors argue that data cleaning is an essential part of the research process, and should be incorporated into study design.},
author = {{Van Den Broeck}, Jan and Cunningham, Solveig Argeseanu and Eeckels, Roger and Herbst, Kobus},
doi = {10.1371/journal.pmed.0020267},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Van Den Broeck et al. - 2005 - Data cleaning Detecting, diagnosing, and editing data abnormalities.pdf:pdf},
isbn = {1549-1676 (Electronic)},
issn = {15491277},
journal = {PLoS Medicine},
keywords = {Big Data},
mendeley-tags = {Big Data},
month = {oct},
number = {10},
pages = {0966--0970},
pmid = {16138788},
publisher = {Public Library of Science},
title = {{Data cleaning: Detecting, diagnosing, and editing data abnormalities}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16138788 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1198040},
volume = {2},
year = {2005}
}
@techreport{Sarukhan2005,
abstract = {The Millennium Ecosystem Assessment was called for by United Nations Secretary-General Kofi Annan in 2000 in his report to the UN General Assembly, We the Peoples: The Role of the United Nations in the 21st Century. Governments subsequently supported the establishment of the assessment through decisions taken by three international conventions, and the MA was initiated in 2001. The MA was conducted under the auspices of the United Nations, with the secretariat coordinated by the United Nations Environment Programme, and it was governed by a multistake- holder board that included representatives of international institutions, governments, business, NGOs, and indigenous peoples. The objective of the MA was to assess the consequences of ecosystem change for human well-being and to establish the scientific basis for actions needed to enhance the conservation and sustainable use of ecosystems and their contributions to human well-being.},
address = {Washington, DC},
author = {Sarukh{\'{a}}n, Jos{\'{e}} and Whyte, Anne},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Sarukh{\'{a}}n, Whyte - 2005 - Millenium Ecosystem Assessment.pdf:pdf},
institution = {World Resources Institute},
isbn = {1-59726-040-1},
title = {{Millenium Ecosystem Assessment}},
year = {2005}
}
@article{Williams2018,
abstract = {Research in information systems (IS) has been dominated by an epistemic script that leads to a variety of negative consequences related to the novelty, creativity, and nature of theory produced. While having matured as a field, theory and theorising in IS tend to lack diversity, struggle in balancing the role of reference theory and empirics, inadequately internalise the information technology (IT) artefact into theories developed, and lack clear relationships to what practitioners actually experience. Our premise is that critical realism (CR), based on specific ontological and epistemological foundations, affords the potential to establish alternative epistemic scripts that can enhance the theory product in IS research. We discuss the basic concepts of theory and theorising and relate these to the current state of theory developed in IS research. Next, we review the fundamental principles of CR and describe how a research script based on this philosophy of science addresses the negative consequences posed by the current dominant script. Lastly, we demonstrate the potential value of CR in terms of generating novel and substantive theory as manifest in two examples of published research that adopt the CR script.},
author = {Williams, Clay K. and Wynn, Donald E.},
doi = {10.1080/0960085X.2018.1435231},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Williams, Wynn - 2018 - A critical realist script for creative theorising in information systems.pdf:pdf},
issn = {14769344},
journal = {European Journal of Information Systems},
keywords = {Critical realism,IS theory,dominant epistemic scripts,theorising},
month = {may},
number = {3},
pages = {315--325},
publisher = {Taylor and Francis Ltd.},
title = {{A critical realist script for creative theorising in information systems}},
url = {https://orsociety.tandfonline.com/doi/abs/10.1080/0960085X.2018.1435231},
volume = {27},
year = {2018}
}
@article{Downie2016,
author = {Downie, Tim},
doi = {10.18637/jss.v075.b03},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Downie - 2016 - Using the R Commander A Point-and-Click Interface for R.pdf:pdf},
isbn = {9781498741903},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {Book Review 3},
pages = {1--4},
title = {{Using the R Commander: A Point-and-Click Interface for R}},
url = {http://www.jstatsoft.org/v75/b03/},
volume = {75},
year = {2016}
}
@book{Skinner1984,
abstract = {3rd rev. and updated ed. 2009},
author = {Skinner, Bernard.},
isbn = {9788788757903},
pages = {325},
publisher = {Apollo Books},
title = {{Colour identification guide to moths of the British Isles (Macrolepidoptera)}},
year = {2009}
}
@article{Boccaletti2014,
abstract = {In the past years, network theory has successfully characterized the interaction among the constituents of a variety of complex systems, ranging from biological to technological, and social systems. However, up until recently, attention was almost exclusively given to networks in which all components were treated on equivalent footing, while neglecting all the extra information about the temporal- or context-related properties of the interactions under study. Only in the last years, taking advantage of the enhanced resolution in real data sets, network scientists have directed their interest to the multiplex character of real-world systems, and explicitly considered the time-varying and multilayer nature of networks. We offer here a comprehensive review on both structural and dynamical organization of graphs made of diverse relationships (layers) between its constituents, and cover several relevant issues, from a full redefinition of the basic structural measures, to understanding how the multilayer nature of the network affects processes and dynamics.},
annote = {Over 500 references!},
archivePrefix = {arXiv},
arxivId = {1407.0742},
author = {Boccaletti, S. and Bianconi, G. and Criado, R. and del Genio, C. I. and G{\'{o}}mez-Garde{\~{n}}es, J. and Romance, M. and Sendi{\~{n}}a-Nadal, I. and Wang, Z. and Zanin, M.},
doi = {10.1016/j.physrep.2014.07.001},
eprint = {1407.0742},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Boccaletti et al. - 2014 - The structure and dynamics of multilayer networks.pdf:pdf},
isbn = {0370-1573},
issn = {03701573},
journal = {Physics Reports},
month = {nov},
number = {1},
pages = {1--122},
pmid = {20505119},
publisher = {North-Holland},
title = {{The structure and dynamics of multilayer networks}},
url = {https://www.sciencedirect.com/science/article/pii/S0370157314002105?{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}origin=gateway{\&}{\_}docanchor={\&}md5=b8429449ccfc9c30159a5f9aeaa92ffb{\&}dgcid=raven{\_}sd{\_}recommender{\_}email},
volume = {544},
year = {2014}
}
@misc{Skala2013,
abstract = {The pgfgantt package provides the ganttchart environment, which draws a Gantt chart within a TikZ picture. The user may add various elements to the chart, for example, titles, bars, groups, milestones and dif- ferent links between these elements. The appearance of the chart elements is highly customizable, and even new chart elements may be defined. Contents},
address = {Hei­del­berg, Germany},
author = {Skala, Wolfgang},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Skala - 2013 - Drawing Gantt Charts in LATEX with TikZ The pgfgantt Package.pdf:pdf},
pages = {1--93},
publisher = {Comprehensive TeX Archive Network},
title = {{Drawing Gantt Charts in LATEX with TikZ The pgfgantt Package}},
url = {http://ctan.org/tex-archive/graphics/pstricks/contrib/pst-gantt/},
year = {2013}
}
@article{Rodriguez-Mazahua2016,
abstract = {Big Data has become a very popular term. It refers to the enormous amount of structured, semi-structured and unstructured data that are exponentially generated by high-performance applications in many domains: biochemistry, genetics, molecular biology, physics, astronomy, business, to mention a few. Since the literature of Big Data has increased significantly in recent years, it becomes necessary to develop an overview of the state-of-the-art in Big Data. This paper aims to provide a comprehensive review of Big Data literature of the last 4 years, to identify the main challenges, areas of application, tools and emergent trends of Big Data. To meet this objective, we have analyzed and classified 457 papers concerning Big Data. This review gives relevant B Giner Alor-Hern{\'{a}}ndez 123 3074 L. Rodr{\'{i}}guez-Mazahua et al. information to practitioners and researchers about the main trends in research and application of Big Data in different technical domains, as well as a reference overview of Big Data tools.},
author = {Rodr{\'{i}}guez-Mazahua, Lisbeth and Rodr{\'{i}}guez-Enr{\'{i}}quez, Cristian Aar{\'{o}}n and S{\'{a}}nchez-Cervantes, Jos{\'{e}} Luis and Cervantes, Jair and Garc{\'{i}}a-Alcaraz, Jorge Luis and Alor-Hern{\'{a}}ndez, Giner},
doi = {10.1007/s11227-015-1501-1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Rodr{\'{i}}guez-Mazahua et al. - 2016 - A general perspective of Big Data applications, tools, challenges and trends.pdf:pdf},
isbn = {09208542 (ISSN)},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Application domains,Big Data,Classification,Literature review},
month = {aug},
number = {8},
pages = {3073--3113},
publisher = {Springer US},
title = {{A general perspective of Big Data: applications, tools, challenges and trends}},
url = {http://link.springer.com/10.1007/s11227-015-1501-1},
volume = {72},
year = {2016}
}
@article{palmer1999observationsindustry,
abstract = {The paper begins with a brief review of previous work in the field of technology forecasting and discusses the Fisher-Pry model in particular, before concluding with an attempt at producing a forecast of the rate of development of smart technology. The Fisher-Pry model is shown to fit well with an assumed ultimate microprocessor clock speed of 1-2 GHz. By combining data from multiple sources, support is found for the hypothesis that rates of development can be correlated with development activity, as measured by the number of US patents filed in specific technology areas. A forecast for the development of 'smart' technology is made to demonstrate how the models may be used to generate forecasts of future developments where limited data is available.},
author = {Palmer, P J and Williams, D J and Hughes, C},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Palmer, Williams, Hughes - 1999 - Observations and models of technology trends within the electronics industry.pdf:pdf},
issn = {0963-7346},
journal = {Engineering Science and Education Journal},
month = {oct},
number = {5},
pages = {233--240},
title = {{Observations and models of technology trends within the electronics industry}},
volume = {8},
year = {1999}
}
@techreport{BigDataPublicWorkingGroup2018,
abstract = {Big Data is a term used to describe the large amount of data in the networked, digitized, sensor-laden, information-driven world. The growth of data is outpacing scientific and technological advances in data analytics. Opportunities exist with Big Data to address the volume, velocity and variety of data through new scalable architectures. To advance progress in Big Data, the NIST Big Data Public Working Group (NBD-PWG) is working to develop consensus on important, fundamental concepts related to Big Data. The results are reported in the NIST Big Data Interoperability Framework (NBDIF) series of volumes. This volume, Volume 1, contains a definition of Big Data and related terms necessary to lay the groundwork for discussions surrounding Big Data.},
author = {{Big Data Public Working Group}},
booktitle = {NIST Special Publication 1500-1r1},
doi = {10.6028/NIST.SP.1500-1r1},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Big Data Public Working Group - 2018 - NIST Big Data Interoperability Framework Volume 1, Definitions NIST Big Data Public Working Group.pdf:pdf},
institution = {NIST},
keywords = {Big Data,Big Data Application Provider,Big Data Characteristics,Big Data Definitions,Big Data Framework Provider,Big Data Taxonomy,Data Consumer,Data Provider,Internet of Things (IoT),Management Fabric,Reference Architecture,Security and Privacy Fabric,System Orchestrator,cloud,data science,machine learning,use cases.},
pages = {43},
title = {{NIST Big Data Interoperability Framework: Volume 1, Definitions NIST Big Data Public Working Group Definitions and Taxonomies Subgroup}},
url = {https://doi.org/10.6028/NIST.SP.1500-1r1},
year = {2018}
}
@article{Pich2002,
abstract = {This article develops a model of a project as a payoff function that depends on the state of the world and the choice of a sequence of actions. A causal mapping, which may be incompletely known by the project team, represents the impact of possible actions on the states of the world. An underlying probability space represents available information about the state of the world. Interactions among actions and states of the world determine the complexity of the payoff function. Activities are endogenous, in that they are the result of a policy that maximizes the expected project payoff. A key concept is the adequacy of the available information about states of the world and action effects. We express uncertainty, ambiguity, and complexity in terms of information adequacy. We identify three fundamental project management strategies: instructionism, learning, and selectionism. We show that classic project management methods emphasize adequate information and instructionism, and demonstrate how modern methods fit into the three fundamental strategies. The appropriate strategy is contingent on the type of uncertainty present and the complexity of the project payoff function. Our model establishes a rigorous language that allows the project manager to judge the adequacy of the available project information at the outset, choose an appropriate combination of strategies, and set a supporting project infrastructure—that is, systems for planning, coordination and incentives, and monitoring. (Project Management; Uncertainty; Complexity; Instructionalism; Project Selection; Ambiguity)},
annote = {Paper in T-AREA-Sos.

Companion paper to deMeyed et al, also 2002. This is the probabilistic, set-theoretic, version.

Paper has a good summary of the project planning domain; very compact. Then they introduce a series of matrices that involve probability of success. Notion is that if you are unsurprised and know enough, you can plan an optimal project, and can also plan for all foreseesable contingencies. Latter includes ALL of them, in this pure version. This they call 'Instructionism', because all the requisite instructions are present for any activity plan.

Then they deal with departures from this ideal situation; you don't know all the contingencies, you don't know fully all the activities, and you don't know what the payoff is for these activities. Nor do you understand your 'wicked problem'. This is where the mantra, 'Fail early and fail often' comes in, as a manifest statement about SCRUM, for instance. They dignify this by calling it 'Learning and Selectionism'. See qoutes below.

The discussion and implications section formulates what is in the deMeyer paper, q.v. This paper is the deep thinking; the deMeyer paper is the marketing version. Best to quote both in future.p2002 Learning and selectionism:
"Learning
In our simple model of projects, learning comes from signals [XXX] that are incompatible with the project team's predictions. As project teams monitor their projects, they must recognize that observed signals are incompatible with their model of the world and be willing to change their representation of the world either by updating the partition [XXX] or the transition mapping [XXX]. We refer to this process as learning.

Of course, the project team does not know the true value of the improvement in the project payoff from learning. Learning, unlike contingent action, cannot be planned in advance. The project team may have a hypothetical model of how activities might yield signals suitable for learning. In the context of tools such as the design of experiments or Failure Mode and Effect Analysis (FMEA), actions are deliberately taken specifically to establish causal relationships between actions and outcomes. In this way a project team learns incrementally—that is, they calculate an estimate of the expected net gain from past learning and extrapolate this to the current situation. If the result is positive in terms of an improvement to the project payoff, then the team may refine further; if not, they stop. This approach assumes that benefits from further learning have predictable returns which may not in fact be true. Alternatively, learning can proceed opportunistically—that is, by paying attention to new information that may arrive from the environment and recognizing when this information implies a change of the project map.

Learning is time consuming, psychologically difficult, and often resisted (e.g., Staw and Ross 1987). The team must actively incorporate the new information, develop a new model, and then replan the project in terms of a new set of activities or new policy. This evidently requires that the team be flexible. Unlike in contingency planning, where “flexible” actions are predetermined and then either “triggered” by signals or “used up” as design slack (Thomke and Reinertsen 1998), here the exact changes required cannot, by definition, be anticipated. Thus, it involves a greater level of flexibility than that required by contingency planning.

Selectionism
Learning can be seen as an extension of the instructionist approach: The project team improves their project model in order to improve their policy. Thus, the team is still relying on their ability to identify an optimal policy, albeit modified over time as the project model evolves. Some projects, however, are not suitable for this approach.

For instance, the project payoff and transition model, while known in principle, may be so complex that they are intractable. Recent results from combinatorial optimization show that pure optimization does not work well for large, complex problems. Known sophisticated optimization algorithms, such as simulated annealing, tabu search, or general hill-climbing algorithms, are outperformed by “randomized local search”—that is, the repetitive or parallel execution of local searches around randomly chosen initial values (e.g., Ferreira and Zerovnik 1993; Fox 1993, 1994; Jacobson and Yücesan 1998)."



Alternatively, the nature of the project may render learning ineffective. If the environment is inaccessible, signals perceived by the project management team will be too costly or not sufficiently rich to learn from. Or, the nature of unk-unks may be such that radical changes to the project are required every time the project team learns, which may be too costly or may not “converge” to a global optimum within a reasonable number of iterations.

In either case, the project team is left with a project black—or at best dark gray—box: Neither an instructionist nor learning approach can be expected to yield an optimal policy reasonably effectively. As the team cannot effectively predict the project payoff, nor are they likely to learn to predict it without unreasonable effort, they are left with having to observe the project payoff. Thus, the team puts forward a reasonable policy [XXX], and then observes the outcome [XXXXX]. We refer to this approach as selectionism .

The team may choose to “hedge,” pursuing multiple approaches in the hope that one will work (as proposed by Abernathy and Rosenbloom 1968). Examples of selectionism—as opposed to optimization— abound in management. Multiple parallel product concepts are frequently pursued for consumer products (Srinivasan et al. 1997) and cars (Sobek et al. 1999). The process of introducing multiple new products into an unknown market and seeing which ones succeed has been referred to as “vicarious selection” in technology management (e.g., Leonard-Barton 1995, Veryzer 1998). Similarly, technological evolution at the level of an economy has never been successfully planned— history has often chosen one of several available candidates ex post (e.g., Mokyr 1990).

When projects are run sequentially, typically a target is set (e.g., derived from a minimum requirement or a known lower bound), and the first to surpass the target is taken as the global optimum (thus m is a stopping time). When m projects are run in parallel, the best of the actual project payoffs (observed only ex post) is retained and taken as the available approximation of the global optimum. "},
author = {Pich, M T and Loch, C H and DeMeyer, A},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pich, Loch, DeMeyer - 2002 - On uncertainty, ambiguity, and complexity in project management.pdf:pdf},
journal = {Management Science},
keywords = {Clever,SIMPLOFI,competencies,competitive challenges,complexity,control,core competences,corporate governance,culture,design process,empowerment,ethics,federated control,harvard business school,innovation,knowledge configuration,knowledge management,knowledge structures,learning organisation,measurement,methods,networks,org. trust,organisational change,product introduction process,projects,prototyping,readiness,strategic planning,strategy,supply chain,systems,teams,theory},
number = {8},
pages = {1008--1023},
title = {{On uncertainty, ambiguity, and complexity in project management}},
volume = {48},
year = {2002}
}
@article{Pereira2015,
abstract = {Tourism is a major social and cultural activity with relevant economic impact. In an effort to promote their attractions with tourists, some cities have adopted the open-data model, publishing touristic data for programmers to use in their own applications. Unfortunately, each city publishes touristic information in its own way. A common Application Programming Interface (API) for accessing this information would enable applications to seamlessly use data from several cities, increasing their potential market while reducing the development costs. This would help developers in making cross-city applications, lowering the overhead of supporting new cities and providing them with increased exposure. Finally, tourists will also benefit from better and cheaper applications due to the boosted competition. This paper provides an overview of the design, deployment and utilization of the CitySDK Tourism API, which aims to provide access to information about Points of Interest, Events and Itineraries. It was designed in order to be used by municipalities, regional or national governments as well as other public or private entities interested in publishing touristic information. The API comprehends a delegation model, allowing applications to access worldwide information by only knowing a single API endpoint. The API was created and validated in the context of the CitySDK project, through which a server reference implementation, client libraries and a set of demonstration applications have also been made available. The API is currently available for the cities of Amsterdam, Helsinki, Lamia, Lisbon and Rome. Several companies have developed mobile applications that use this API.},
author = {Pereira, Ricardo Lopes and Sousa, Pedro Cruz and Barata, Ricardo and Oliveira, Andr{\'{e}} and Monsieur, Geert},
doi = {10.1186/s13174-015-0039-z},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pereira et al. - 2015 - CitySDK Tourism API - building value around open data.pdf:pdf},
isbn = {1867-4828},
issn = {18690238},
journal = {Journal of Internet Services and Applications},
keywords = {API,Open data,Smart cities,Tourism},
month = {aug},
number = {1},
pages = {1--13},
publisher = {Springer London},
title = {{CitySDK Tourism API - building value around open data}},
url = {http://www.jisajournal.com/content/6/1/24},
volume = {6},
year = {2015}
}
@article{Pearl2019a,
abstract = {This paper provides empirical interpretation of the do(x) operator when applied to non-manipulable variables such as race, obesity, or cholesterol level. We view do(x) as an ideal intervention that provides valuable information on the effects of manipulable variables and is thus empirically testable. We draw parallels between this interpretation and ways of enabling machines to learn effects of untried actions from those tried. We end with the conclusion that researchers need not distinguish manipulable from non-manipulable variables; both types are equally eligible to receive the do(x) operator and to produce useful information for decision makers.},
author = {Pearl, Judea},
doi = {10.1515/jci-2019-2002},
file = {:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2019 - On the Interpretation of do(x).pdf:pdf;:Users/TechTrends/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2019 - On the Interpretation of do(x)(2).pdf:pdf},
journal = {Journal of Causal Inference},
keywords = {Manipulability,causal effects,interventions,reinforcement learning,testability},
number = {1},
pages = {6},
title = {{On the Interpretation of do(x)}},
volume = {7},
year = {2019}
}
